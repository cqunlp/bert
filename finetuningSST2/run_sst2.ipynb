{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import mindspore\n",
    "from mindspore import context\n",
    "from mindspore import nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import ops\n",
    "from src.bert import BertForPretraining\n",
    "from src.config import BertConfig\n",
    "from src.tokenizer import BertTokenizer\n",
    "from model import BertBinaryClassificationModel\n",
    "from mindnlp.mindnlp.mindtext.dataset.classification.sst import SST2Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST-2 dataset loadding: 100%|██████████| 872/872 [00:00<00:00, 13114.44it/s]\n",
      "SST-2 dataset loadding: 100%|██████████| 1821/1821 [00:00<00:00, 16529.73it/s]\n",
      "SST-2 dataset loadding: 100%|██████████| 67349/67349 [00:04<00:00, 15668.09it/s] \n",
      "SST-2 train dataset sentence preprocess bar(tokenize).: 100%|██████████| 67349/67349 [00:22<00:00, 3052.18it/s]\n",
      "SST-2 train dataset processing.: 100%|██████████| 67349/67349 [00:03<00:00, 17087.49it/s] \n",
      "[WARNING] ME(17740:140613899089664,MainProcess):2022-11-11-15:25:17.453.595 [mindspore/dataset/core/validator_helpers.py:806] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "Writing data to .mindrecord file: 100%|██████████| 67349/67349 [02:50<00:00, 394.19it/s]\n",
      "SST-2 dev dataset sentence preprocess bar(tokenize).: 100%|██████████| 872/872 [00:00<00:00, 1909.87it/s]\n",
      "SST-2 dev dataset processing.: 100%|██████████| 872/872 [00:00<00:00, 16450.17it/s]\n",
      "[WARNING] ME(17740:140613899089664,MainProcess):2022-11-11-15:28:09.600.508 [mindspore/dataset/core/validator_helpers.py:806] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "Writing data to .mindrecord file: 100%|██████████| 872/872 [00:02<00:00, 409.03it/s]\n",
      "SST-2 test dataset sentence preprocess bar(tokenize).: 100%|██████████| 1821/1821 [00:00<00:00, 1872.28it/s]\n",
      "SST-2 test dataset processing.: 100%|██████████| 1821/1821 [00:00<00:00, 16378.80it/s]\n",
      "[WARNING] ME(17740:140613899089664,MainProcess):2022-11-11-15:28:12.919.262 [mindspore/dataset/core/validator_helpers.py:806] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "Writing data to .mindrecord file: 100%|██████████| 1821/1821 [00:03<00:00, 456.70it/s]\n"
     ]
    }
   ],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE)\n",
    "#context.set_context(mode=context.PYNATIVE_MODE)\n",
    "dataset = SST2Dataset(paths='SST-2',\n",
    "                      tokenizer=\"bert-base-uncased\",\n",
    "                      max_length=128,\n",
    "                      truncation_strategy=True,\n",
    "                      columns_list=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
    "                      test_columns_list=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "                      batch_size=16)\n",
    "\n",
    "sst_2_ds = dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'dev', 'test'])\n",
      "['input_ids', 'token_type_ids', 'attention_mask', 'label']\n",
      "16 4210\n",
      "16 55\n"
     ]
    }
   ],
   "source": [
    "print(sst_2_ds.keys())\n",
    "# dev_dataset batch_size 8\n",
    "# dev_dataset = sst_2_ds1['dev']\n",
    "# train_dataset batch_size 32\n",
    "train_dataset = sst_2_ds['train']\n",
    "test_dataset = sst_2_ds['test']\n",
    "dev_dataset = sst_2_ds['dev']\n",
    "type(test_dataset)\n",
    "print(train_dataset.get_col_names())\n",
    "\n",
    "print(train_dataset.get_batch_size(), train_dataset.get_dataset_size())\n",
    "print(dev_dataset.get_batch_size(), dev_dataset.get_dataset_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask', 'label']\n",
      "(16, 128)\n",
      "(16, 128)\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "4\n",
      "[[  101  4760  2149 ...     0     0     0]\n",
      " [  101  6256   102 ...     0     0     0]\n",
      " [  101 11867  6340 ...     0     0     0]\n",
      " ...\n",
      " [  101  5636 24372 ...     0     0     0]\n",
      " [  101  2013  1996 ...     0     0     0]\n",
      " [  101  2003  1037 ...     0     0     0]] [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]] [[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# batch and tokenizer -> (input_ids ,token_type_ids, attention_mask, label)\n",
    "print(train_dataset.get_col_names())\n",
    "for data in train_dataset.create_dict_iterator():\n",
    "    print(data['input_ids'].shape)\n",
    "    print(data['attention_mask'].shape)\n",
    "    print(data['token_type_ids'].shape)\n",
    "    print(data['label'].shape)\n",
    "    print(len(data))\n",
    "    break\n",
    "for input_ids, token_type_ids, attention_mask, label in train_dataset.create_tuple_iterator():\n",
    "    print(input_ids, token_type_ids, attention_mask, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# test load ckpt\n",
    "from src.config import BertConfig\n",
    "ckpt_file = \"checkpoint/bert_L4_H128_step_308749_card_id_0.ckpt\"\n",
    "config_file = 'config/bert_L4_H128_config.json'\n",
    "config = BertConfig(config_file)\n",
    "print(config.vocab_size)\n",
    "model = BertForPretraining(config)\n",
    "\n",
    "before_model = BertForPretraining(config)\n",
    "dict_before = before_model.bert.parameters_dict()\n",
    "\n",
    "dict_after = mindspore.load_checkpoint(ckpt_file)\n",
    "mindspore.load_param_into_net(model, dict_after)\n",
    "model = model.bert\n",
    "dict_after = model.parameters_dict()\n",
    "import numpy as np\n",
    "list_before = list(dict_before.values())\n",
    "list_after = list(dict_after.values())\n",
    "\n",
    "print(np.allclose(list_before[2].asnumpy(), list_after[2].asnumpy(), rtol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Parameter (name=classifier.weight, shape=(2, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=classifier.bias, shape=(2,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.embeddings.word_embeddings.embedding_table, shape=(30522, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.embeddings.position_embeddings.embedding_table, shape=(512, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.embeddings.token_type_embeddings.embedding_table, shape=(2, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.embeddings.layer_norm.gamma, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.embeddings.layer_norm.beta, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.attention.self_attn.query.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.attention.self_attn.query.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.attention.self_attn.key.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.attention.self_attn.key.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.attention.self_attn.value.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.attention.self_attn.value.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.attention.output.dense.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.attention.output.dense.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.attention.output.layer_norm.gamma, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.attention.output.layer_norm.beta, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.intermediate.dense.weight, shape=(512, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.intermediate.dense.bias, shape=(512,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.output.dense.weight, shape=(128, 512), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.output.dense.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.output.layer_norm.gamma, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.0.output.layer_norm.beta, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.attention.self_attn.query.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.attention.self_attn.query.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.attention.self_attn.key.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.attention.self_attn.key.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.attention.self_attn.value.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.attention.self_attn.value.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.attention.output.dense.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.attention.output.dense.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.attention.output.layer_norm.gamma, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.attention.output.layer_norm.beta, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.intermediate.dense.weight, shape=(512, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.intermediate.dense.bias, shape=(512,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.output.dense.weight, shape=(128, 512), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.output.dense.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.output.layer_norm.gamma, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.1.output.layer_norm.beta, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.attention.self_attn.query.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.attention.self_attn.query.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.attention.self_attn.key.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.attention.self_attn.key.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.attention.self_attn.value.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.attention.self_attn.value.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.attention.output.dense.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.attention.output.dense.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.attention.output.layer_norm.gamma, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.attention.output.layer_norm.beta, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.intermediate.dense.weight, shape=(512, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.intermediate.dense.bias, shape=(512,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.output.dense.weight, shape=(128, 512), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.output.dense.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.output.layer_norm.gamma, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.2.output.layer_norm.beta, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.attention.self_attn.query.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.attention.self_attn.query.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.attention.self_attn.key.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.attention.self_attn.key.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.attention.self_attn.value.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.attention.self_attn.value.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.attention.output.dense.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.attention.output.dense.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.attention.output.layer_norm.gamma, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.attention.output.layer_norm.beta, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.intermediate.dense.weight, shape=(512, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.intermediate.dense.bias, shape=(512,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.output.dense.weight, shape=(128, 512), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.output.dense.bias, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.output.layer_norm.gamma, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.encoder.layer.3.output.layer_norm.beta, shape=(128,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.pooler.dense.weight, shape=(128, 128), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=bert.pooler.dense.bias, shape=(128,), dtype=Float32, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "bert_sst_2 = BertBinaryClassificationModel(config, ckpt_file)\n",
    "# test output \n",
    "\n",
    "for input_ids, token_type_ids, attention_mask, label in train_dataset.create_tuple_iterator():\n",
    "    print(len(input_ids))\n",
    "    loss, logits = bert_sst_2(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, label=label)\n",
    "    break\n",
    "\n",
    "for item in bert_sst_2.get_parameters():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def getpwd():\n",
    "    pwd = sys.path[0]\n",
    "    if os.path.isfile(pwd):\n",
    "        pwd = os.path.dirname(pwd)\n",
    "    return pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, dataset, optimizer):\n",
    "    def forward_fn(input_ids, attention_mask, token_type_ids, label):\n",
    "        loss, logits = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        label=label)\n",
    "        return loss, logits\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "    def train_step(input_ids, attention_mask, token_type_ids, label):\n",
    "        (loss, _), grads = grad_fn(input_ids, attention_mask, token_type_ids, label)\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "\n",
    "    for batch, (input_ids, attention_mask, token_type_ids, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(input_ids, attention_mask, token_type_ids, label)\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, dataset, output_file, current_step, current_epoch):\n",
    "#def test_loop(model, dataset):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for input_ids, attention_mask, token_type_ids, label in dataset.create_tuple_iterator():\n",
    "        loss, logits = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        label=label)\n",
    "        total += len(input_ids)\n",
    "        test_loss += loss.asnumpy()\n",
    "        correct_loop = (logits.argmax(1) == label.view(-1)).asnumpy().sum()\n",
    "        correct += correct_loop\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    if correct*100 >= 82.84:\n",
    "        output_ckpt_file = os.path.join(output_file, \"{epoch}_epoch_{step}_step_acc_{acc}%.ckpt\".format(\n",
    "            epoch=current_epoch, step=current_step, acc=(correct*100)\n",
    "        ))\n",
    "        mindspore.save_checkpoint(model, output_ckpt_file)\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, train_dataset, dev_dataset, optimizer, current_epoch):\n",
    "    output_file = os.path.join(getpwd(), \"outputs\")\n",
    "    if not os.path.exists(output_file):\n",
    "        try:\n",
    "            os.mkdir(output_file)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    def forward_fn(input_ids, attention_mask, token_type_ids, label):\n",
    "        loss, logits = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        label=label)\n",
    "        return loss, logits\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "    def train_step(input_ids, attention_mask, token_type_ids, label):\n",
    "        (loss, _), grads = grad_fn(input_ids, attention_mask, token_type_ids, label)\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "\n",
    "    size = train_dataset.get_dataset_size()\n",
    "    model.set_train(True)\n",
    "\n",
    "    for batch, (input_ids, attention_mask, token_type_ids, label) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "        loss = train_step(input_ids, attention_mask, token_type_ids, label)\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")\n",
    "        # add test\n",
    "        if batch % 100 == 0:\n",
    "            print(\"In epoch {} _ batch {} testing\".format(current_epoch + 1, current))\n",
    "            test_loop(model, dev_dataset, output_file, batch, current_epoch)\n",
    "            # test_loop(model, dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "epoch_num = 10\n",
    "config_file = 'config/bert_L4_H128_config.json'\n",
    "ckpt_file = \"checkpoint/bert_L4_H128_step_308749_card_id_0.ckpt\"\n",
    "config = BertConfig(config_file)\n",
    "model = BertForPretraining(config)\n",
    "bert_sst_2 = BertBinaryClassificationModel(config, ckpt_file)\n",
    "\n",
    "# freeze_layer for classify task\n",
    "# freeze_params = list(filter(lambda x: 'bert.encoder.layer.0' in x.name \\\n",
    "#                                     or 'bert.encoder.layer.1' in x.name \\\n",
    "#                                     or 'bert.encoder.layer.2' in x.name \\\n",
    "#                                     or 'bert.embeddings.token_type_embeddings' in x.name \\\n",
    "#                                     or 'bert.embeddings.position_embeddings' in x.name \\\n",
    "#                                     or 'bert.embeddings.word_embeddings' in x.name, \\\n",
    "#                             bert_sst_2.bert.trainable_params()))\n",
    "# freeze_params = list(filter(lambda x: 'bert' in x.name, bert_sst_2.bert.trainable_params()))\n",
    "# for item in freeze_params:\n",
    "#     item.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = bert_sst_2.trainable_params()\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamWeightDecay\n",
    "from src.optimization import BertLearningRate\n",
    "# lr_schedule_for_backbone = BertLearningRate(learning_rate=5e-5,\n",
    "#                                    end_learning_rate=0.0,\n",
    "#                                    warmup_steps=int(train_dataset.get_dataset_size() * epoch_num * 0.1),\n",
    "#                                    decay_steps=train_dataset.get_dataset_size() * epoch_num,\n",
    "#                                    power=1.0)\n",
    "\n",
    "lr_schedule = BertLearningRate(learning_rate=5e-5,\n",
    "                                   end_learning_rate=0.0,\n",
    "                                   warmup_steps=int(train_dataset.get_dataset_size() * epoch_num * 0.1),\n",
    "                                   decay_steps=train_dataset.get_dataset_size() * epoch_num,\n",
    "                                   power=1.0)\n",
    "# decay_params = list(filter(lambda x: 'gamma' not in x.name \\\n",
    "#                                         and 'beta' not in x.name \\\n",
    "#                                         and 'bias' not in x.name, params))\n",
    "# other_params = list(filter(lambda x: 'gamma' in x.name \\\n",
    "#                                         or 'beta' in x.name \\\n",
    "#                                         or 'bias' in x.name, params))\n",
    "# group_params = [{'params': decay_params, 'weight_decay': 0.01},\n",
    "#                     {'params': other_params},\n",
    "#                     {'order_params': params}]\n",
    "\n",
    "# classify_params = list(filter(lambda x: 'classifier' in x.name, \\\n",
    "#                             bert_sst_2.trainable_params()))\n",
    "\n",
    "# other_params = list(filter(lambda x: 'classifier' not in x.name, \\\n",
    "#                             bert_sst_2.trainable_params()))\n",
    "\n",
    "# group_params = [{'params': classify_params, 'weight_decay': 0.01, 'lr': lr_schedule_for_finetuing},\n",
    "#                 {'params': other_params, 'lr': lr_schedule_for_finetuing}]\n",
    "\n",
    "optimizer = nn.AdamWeightDecay(params, lr_schedule, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.692476  [  0/4210]\n",
      "In epoch 1 _ batch 0 testing\n",
      "Test: \n",
      " Accuracy: 51.6%, Avg loss: 0.691844 \n",
      "\n",
      "loss: 0.680482  [ 10/4210]\n",
      "loss: 0.664221  [ 20/4210]\n",
      "loss: 0.707480  [ 30/4210]\n",
      "loss: 0.693433  [ 40/4210]\n",
      "loss: 0.674515  [ 50/4210]\n",
      "loss: 0.681755  [ 60/4210]\n",
      "loss: 0.676224  [ 70/4210]\n",
      "loss: 0.681729  [ 80/4210]\n",
      "loss: 0.711949  [ 90/4210]\n",
      "loss: 0.687227  [100/4210]\n",
      "In epoch 1 _ batch 100 testing\n",
      "Test: \n",
      " Accuracy: 50.9%, Avg loss: 0.692012 \n",
      "\n",
      "loss: 0.675782  [110/4210]\n",
      "loss: 0.683089  [120/4210]\n",
      "loss: 0.688611  [130/4210]\n",
      "loss: 0.693463  [140/4210]\n",
      "loss: 0.673625  [150/4210]\n",
      "loss: 0.682184  [160/4210]\n",
      "loss: 0.689173  [170/4210]\n",
      "loss: 0.687371  [180/4210]\n",
      "loss: 0.671180  [190/4210]\n",
      "loss: 0.715777  [200/4210]\n",
      "In epoch 1 _ batch 200 testing\n",
      "Test: \n",
      " Accuracy: 50.9%, Avg loss: 0.693102 \n",
      "\n",
      "loss: 0.697979  [210/4210]\n",
      "loss: 0.695149  [220/4210]\n",
      "loss: 0.668432  [230/4210]\n",
      "loss: 0.675704  [240/4210]\n",
      "loss: 0.644326  [250/4210]\n",
      "loss: 0.701753  [260/4210]\n",
      "loss: 0.675120  [270/4210]\n",
      "loss: 0.673377  [280/4210]\n",
      "loss: 0.670918  [290/4210]\n",
      "loss: 0.683722  [300/4210]\n",
      "In epoch 1 _ batch 300 testing\n",
      "Test: \n",
      " Accuracy: 50.9%, Avg loss: 0.694967 \n",
      "\n",
      "loss: 0.698118  [310/4210]\n",
      "loss: 0.662826  [320/4210]\n",
      "loss: 0.694830  [330/4210]\n",
      "loss: 0.665778  [340/4210]\n",
      "loss: 0.678311  [350/4210]\n",
      "loss: 0.688944  [360/4210]\n",
      "loss: 0.675650  [370/4210]\n",
      "loss: 0.689042  [380/4210]\n",
      "loss: 0.646445  [390/4210]\n",
      "loss: 0.686169  [400/4210]\n",
      "In epoch 1 _ batch 400 testing\n",
      "Test: \n",
      " Accuracy: 50.9%, Avg loss: 0.693711 \n",
      "\n",
      "loss: 0.676767  [410/4210]\n",
      "loss: 0.723878  [420/4210]\n",
      "loss: 0.691005  [430/4210]\n",
      "loss: 0.688658  [440/4210]\n",
      "loss: 0.647200  [450/4210]\n",
      "loss: 0.650286  [460/4210]\n",
      "loss: 0.678645  [470/4210]\n",
      "loss: 0.699368  [480/4210]\n",
      "loss: 0.649172  [490/4210]\n",
      "loss: 0.716947  [500/4210]\n",
      "In epoch 1 _ batch 500 testing\n",
      "Test: \n",
      " Accuracy: 50.9%, Avg loss: 0.692838 \n",
      "\n",
      "loss: 0.699601  [510/4210]\n",
      "loss: 0.702647  [520/4210]\n",
      "loss: 0.713654  [530/4210]\n",
      "loss: 0.699517  [540/4210]\n",
      "loss: 0.715197  [550/4210]\n",
      "loss: 0.714932  [560/4210]\n",
      "loss: 0.651446  [570/4210]\n",
      "loss: 0.704819  [580/4210]\n",
      "loss: 0.710167  [590/4210]\n",
      "loss: 0.685288  [600/4210]\n",
      "In epoch 1 _ batch 600 testing\n",
      "Test: \n",
      " Accuracy: 51.0%, Avg loss: 0.690211 \n",
      "\n",
      "loss: 0.680481  [610/4210]\n",
      "loss: 0.673852  [620/4210]\n",
      "loss: 0.681386  [630/4210]\n",
      "loss: 0.719122  [640/4210]\n",
      "loss: 0.651246  [650/4210]\n",
      "loss: 0.665098  [660/4210]\n",
      "loss: 0.688592  [670/4210]\n",
      "loss: 0.687150  [680/4210]\n",
      "loss: 0.680824  [690/4210]\n",
      "loss: 0.662120  [700/4210]\n",
      "In epoch 1 _ batch 700 testing\n",
      "Test: \n",
      " Accuracy: 58.3%, Avg loss: 0.687528 \n",
      "\n",
      "loss: 0.681182  [710/4210]\n",
      "loss: 0.645325  [720/4210]\n",
      "loss: 0.648677  [730/4210]\n",
      "loss: 0.674028  [740/4210]\n",
      "loss: 0.625115  [750/4210]\n",
      "loss: 0.744181  [760/4210]\n",
      "loss: 0.704095  [770/4210]\n",
      "loss: 0.685829  [780/4210]\n",
      "loss: 0.678795  [790/4210]\n",
      "loss: 0.688959  [800/4210]\n",
      "In epoch 1 _ batch 800 testing\n",
      "Test: \n",
      " Accuracy: 60.1%, Avg loss: 0.684960 \n",
      "\n",
      "loss: 0.729145  [810/4210]\n",
      "loss: 0.702608  [820/4210]\n",
      "loss: 0.666567  [830/4210]\n",
      "loss: 0.661314  [840/4210]\n",
      "loss: 0.722578  [850/4210]\n",
      "loss: 0.690937  [860/4210]\n",
      "loss: 0.710701  [870/4210]\n",
      "loss: 0.716099  [880/4210]\n",
      "loss: 0.671760  [890/4210]\n",
      "loss: 0.643472  [900/4210]\n",
      "In epoch 1 _ batch 900 testing\n",
      "Test: \n",
      " Accuracy: 59.2%, Avg loss: 0.680815 \n",
      "\n",
      "loss: 0.761084  [910/4210]\n",
      "loss: 0.671008  [920/4210]\n",
      "loss: 0.703067  [930/4210]\n",
      "loss: 0.657401  [940/4210]\n",
      "loss: 0.698962  [950/4210]\n",
      "loss: 0.617338  [960/4210]\n",
      "loss: 0.689664  [970/4210]\n",
      "loss: 0.633877  [980/4210]\n",
      "loss: 0.683061  [990/4210]\n",
      "loss: 0.663132  [1000/4210]\n",
      "In epoch 1 _ batch 1000 testing\n",
      "Test: \n",
      " Accuracy: 55.6%, Avg loss: 0.681420 \n",
      "\n",
      "loss: 0.660143  [1010/4210]\n",
      "loss: 0.675659  [1020/4210]\n",
      "loss: 0.714490  [1030/4210]\n",
      "loss: 0.636892  [1040/4210]\n",
      "loss: 0.657937  [1050/4210]\n",
      "loss: 0.648501  [1060/4210]\n",
      "loss: 0.699184  [1070/4210]\n",
      "loss: 0.621642  [1080/4210]\n",
      "loss: 0.589683  [1090/4210]\n",
      "loss: 0.693367  [1100/4210]\n",
      "In epoch 1 _ batch 1100 testing\n",
      "Test: \n",
      " Accuracy: 60.0%, Avg loss: 0.669052 \n",
      "\n",
      "loss: 0.709808  [1110/4210]\n",
      "loss: 0.739809  [1120/4210]\n",
      "loss: 0.637045  [1130/4210]\n",
      "loss: 0.695669  [1140/4210]\n",
      "loss: 0.655319  [1150/4210]\n",
      "loss: 0.671143  [1160/4210]\n",
      "loss: 0.675003  [1170/4210]\n",
      "loss: 0.688517  [1180/4210]\n",
      "loss: 0.616327  [1190/4210]\n",
      "loss: 0.646500  [1200/4210]\n",
      "In epoch 1 _ batch 1200 testing\n",
      "Test: \n",
      " Accuracy: 61.6%, Avg loss: 0.660077 \n",
      "\n",
      "loss: 0.703223  [1210/4210]\n",
      "loss: 0.673017  [1220/4210]\n",
      "loss: 0.627388  [1230/4210]\n",
      "loss: 0.630863  [1240/4210]\n",
      "loss: 0.620960  [1250/4210]\n",
      "loss: 0.608458  [1260/4210]\n",
      "loss: 0.618783  [1270/4210]\n",
      "loss: 0.666665  [1280/4210]\n",
      "loss: 0.726641  [1290/4210]\n",
      "loss: 0.558742  [1300/4210]\n",
      "In epoch 1 _ batch 1300 testing\n",
      "Test: \n",
      " Accuracy: 62.8%, Avg loss: 0.652082 \n",
      "\n",
      "loss: 0.708570  [1310/4210]\n",
      "loss: 0.620615  [1320/4210]\n",
      "loss: 0.565008  [1330/4210]\n",
      "loss: 0.597695  [1340/4210]\n",
      "loss: 0.635296  [1350/4210]\n",
      "loss: 0.654975  [1360/4210]\n",
      "loss: 0.475119  [1370/4210]\n",
      "loss: 0.662811  [1380/4210]\n",
      "loss: 0.639571  [1390/4210]\n",
      "loss: 0.637791  [1400/4210]\n",
      "In epoch 1 _ batch 1400 testing\n",
      "Test: \n",
      " Accuracy: 61.6%, Avg loss: 0.654543 \n",
      "\n",
      "loss: 0.632200  [1410/4210]\n",
      "loss: 0.637579  [1420/4210]\n",
      "loss: 0.652014  [1430/4210]\n",
      "loss: 0.535490  [1440/4210]\n",
      "loss: 0.631665  [1450/4210]\n",
      "loss: 0.698215  [1460/4210]\n",
      "loss: 0.680966  [1470/4210]\n",
      "loss: 0.749941  [1480/4210]\n",
      "loss: 0.679932  [1490/4210]\n",
      "loss: 0.665140  [1500/4210]\n",
      "In epoch 1 _ batch 1500 testing\n",
      "Test: \n",
      " Accuracy: 64.3%, Avg loss: 0.639809 \n",
      "\n",
      "loss: 0.651590  [1510/4210]\n",
      "loss: 0.687105  [1520/4210]\n",
      "loss: 0.639277  [1530/4210]\n",
      "loss: 0.644186  [1540/4210]\n",
      "loss: 0.623422  [1550/4210]\n",
      "loss: 0.589491  [1560/4210]\n",
      "loss: 0.617461  [1570/4210]\n",
      "loss: 0.615277  [1580/4210]\n",
      "loss: 0.606045  [1590/4210]\n",
      "loss: 0.545399  [1600/4210]\n",
      "In epoch 1 _ batch 1600 testing\n",
      "Test: \n",
      " Accuracy: 63.5%, Avg loss: 0.641069 \n",
      "\n",
      "loss: 0.592757  [1610/4210]\n",
      "loss: 0.769810  [1620/4210]\n",
      "loss: 0.632539  [1630/4210]\n",
      "loss: 0.657351  [1640/4210]\n",
      "loss: 0.600756  [1650/4210]\n",
      "loss: 0.651310  [1660/4210]\n",
      "loss: 0.779113  [1670/4210]\n",
      "loss: 0.515826  [1680/4210]\n",
      "loss: 0.560902  [1690/4210]\n",
      "loss: 0.856997  [1700/4210]\n",
      "In epoch 1 _ batch 1700 testing\n",
      "Test: \n",
      " Accuracy: 65.5%, Avg loss: 0.631945 \n",
      "\n",
      "loss: 0.604900  [1710/4210]\n",
      "loss: 0.641894  [1720/4210]\n",
      "loss: 0.622455  [1730/4210]\n",
      "loss: 0.651961  [1740/4210]\n",
      "loss: 0.730840  [1750/4210]\n",
      "loss: 0.631355  [1760/4210]\n",
      "loss: 0.515294  [1770/4210]\n",
      "loss: 0.577233  [1780/4210]\n",
      "loss: 0.721879  [1790/4210]\n",
      "loss: 0.702023  [1800/4210]\n",
      "In epoch 1 _ batch 1800 testing\n",
      "Test: \n",
      " Accuracy: 66.3%, Avg loss: 0.626051 \n",
      "\n",
      "loss: 0.638270  [1810/4210]\n",
      "loss: 0.636046  [1820/4210]\n",
      "loss: 0.596960  [1830/4210]\n",
      "loss: 0.622193  [1840/4210]\n",
      "loss: 0.590104  [1850/4210]\n",
      "loss: 0.798717  [1860/4210]\n",
      "loss: 0.703868  [1870/4210]\n",
      "loss: 0.622894  [1880/4210]\n",
      "loss: 0.542580  [1890/4210]\n",
      "loss: 0.621896  [1900/4210]\n",
      "In epoch 1 _ batch 1900 testing\n",
      "Test: \n",
      " Accuracy: 64.1%, Avg loss: 0.630291 \n",
      "\n",
      "loss: 0.556650  [1910/4210]\n",
      "loss: 0.598509  [1920/4210]\n",
      "loss: 0.613565  [1930/4210]\n",
      "loss: 0.678768  [1940/4210]\n",
      "loss: 0.599529  [1950/4210]\n",
      "loss: 0.495226  [1960/4210]\n",
      "loss: 0.712332  [1970/4210]\n",
      "loss: 0.738915  [1980/4210]\n",
      "loss: 0.498852  [1990/4210]\n",
      "loss: 0.594397  [2000/4210]\n",
      "In epoch 1 _ batch 2000 testing\n",
      "Test: \n",
      " Accuracy: 68.5%, Avg loss: 0.609966 \n",
      "\n",
      "loss: 0.656999  [2010/4210]\n",
      "loss: 0.611366  [2020/4210]\n",
      "loss: 0.549127  [2030/4210]\n",
      "loss: 0.606286  [2040/4210]\n",
      "loss: 0.657261  [2050/4210]\n",
      "loss: 0.590362  [2060/4210]\n",
      "loss: 0.576921  [2070/4210]\n",
      "loss: 0.574401  [2080/4210]\n",
      "loss: 0.651766  [2090/4210]\n",
      "loss: 0.537576  [2100/4210]\n",
      "In epoch 1 _ batch 2100 testing\n",
      "Test: \n",
      " Accuracy: 63.3%, Avg loss: 0.629969 \n",
      "\n",
      "loss: 0.534064  [2110/4210]\n",
      "loss: 0.717930  [2120/4210]\n",
      "loss: 0.490301  [2130/4210]\n",
      "loss: 0.621457  [2140/4210]\n",
      "loss: 0.744936  [2150/4210]\n",
      "loss: 0.927544  [2160/4210]\n",
      "loss: 0.665204  [2170/4210]\n",
      "loss: 0.741951  [2180/4210]\n",
      "loss: 0.500683  [2190/4210]\n",
      "loss: 0.509513  [2200/4210]\n",
      "In epoch 1 _ batch 2200 testing\n",
      "Test: \n",
      " Accuracy: 67.0%, Avg loss: 0.608453 \n",
      "\n",
      "loss: 0.591996  [2210/4210]\n",
      "loss: 0.672837  [2220/4210]\n",
      "loss: 0.634709  [2230/4210]\n",
      "loss: 0.561656  [2240/4210]\n",
      "loss: 0.627280  [2250/4210]\n",
      "loss: 0.536267  [2260/4210]\n",
      "loss: 0.856604  [2270/4210]\n",
      "loss: 0.710656  [2280/4210]\n",
      "loss: 0.651855  [2290/4210]\n",
      "loss: 0.603267  [2300/4210]\n",
      "In epoch 1 _ batch 2300 testing\n",
      "Test: \n",
      " Accuracy: 70.3%, Avg loss: 0.587549 \n",
      "\n",
      "loss: 0.607217  [2310/4210]\n",
      "loss: 0.655331  [2320/4210]\n",
      "loss: 0.539787  [2330/4210]\n",
      "loss: 0.529523  [2340/4210]\n",
      "loss: 0.476604  [2350/4210]\n",
      "loss: 0.664358  [2360/4210]\n",
      "loss: 0.639490  [2370/4210]\n",
      "loss: 0.499831  [2380/4210]\n",
      "loss: 0.536138  [2390/4210]\n",
      "loss: 0.628855  [2400/4210]\n",
      "In epoch 1 _ batch 2400 testing\n",
      "Test: \n",
      " Accuracy: 68.2%, Avg loss: 0.594788 \n",
      "\n",
      "loss: 0.729591  [2410/4210]\n",
      "loss: 0.555125  [2420/4210]\n",
      "loss: 0.643219  [2430/4210]\n",
      "loss: 0.394314  [2440/4210]\n",
      "loss: 0.563492  [2450/4210]\n",
      "loss: 0.494500  [2460/4210]\n",
      "loss: 0.624933  [2470/4210]\n",
      "loss: 0.503586  [2480/4210]\n",
      "loss: 0.866135  [2490/4210]\n",
      "loss: 0.573763  [2500/4210]\n",
      "In epoch 1 _ batch 2500 testing\n",
      "Test: \n",
      " Accuracy: 72.0%, Avg loss: 0.576409 \n",
      "\n",
      "loss: 0.607445  [2510/4210]\n",
      "loss: 0.737941  [2520/4210]\n",
      "loss: 0.520916  [2530/4210]\n",
      "loss: 0.691631  [2540/4210]\n",
      "loss: 0.634271  [2550/4210]\n",
      "loss: 0.664181  [2560/4210]\n",
      "loss: 0.689942  [2570/4210]\n",
      "loss: 0.808841  [2580/4210]\n",
      "loss: 0.552492  [2590/4210]\n",
      "loss: 0.533616  [2600/4210]\n",
      "In epoch 1 _ batch 2600 testing\n",
      "Test: \n",
      " Accuracy: 71.2%, Avg loss: 0.583161 \n",
      "\n",
      "loss: 0.478793  [2610/4210]\n",
      "loss: 0.682417  [2620/4210]\n",
      "loss: 0.617870  [2630/4210]\n",
      "loss: 0.537050  [2640/4210]\n",
      "loss: 0.656716  [2650/4210]\n",
      "loss: 0.509803  [2660/4210]\n",
      "loss: 0.507828  [2670/4210]\n",
      "loss: 0.488409  [2680/4210]\n",
      "loss: 0.599460  [2690/4210]\n",
      "loss: 0.471244  [2700/4210]\n",
      "In epoch 1 _ batch 2700 testing\n",
      "Test: \n",
      " Accuracy: 70.2%, Avg loss: 0.579048 \n",
      "\n",
      "loss: 0.606389  [2710/4210]\n",
      "loss: 0.663842  [2720/4210]\n",
      "loss: 0.464818  [2730/4210]\n",
      "loss: 0.385402  [2740/4210]\n",
      "loss: 0.519974  [2750/4210]\n",
      "loss: 0.629687  [2760/4210]\n",
      "loss: 0.568873  [2770/4210]\n",
      "loss: 0.577001  [2780/4210]\n",
      "loss: 0.521289  [2790/4210]\n",
      "loss: 0.613889  [2800/4210]\n",
      "In epoch 1 _ batch 2800 testing\n",
      "Test: \n",
      " Accuracy: 73.1%, Avg loss: 0.561813 \n",
      "\n",
      "loss: 0.851479  [2810/4210]\n",
      "loss: 0.469622  [2820/4210]\n",
      "loss: 0.683366  [2830/4210]\n",
      "loss: 0.722306  [2840/4210]\n",
      "loss: 0.585069  [2850/4210]\n",
      "loss: 0.428166  [2860/4210]\n",
      "loss: 0.576077  [2870/4210]\n",
      "loss: 0.597288  [2880/4210]\n",
      "loss: 0.381024  [2890/4210]\n",
      "loss: 0.571811  [2900/4210]\n",
      "In epoch 1 _ batch 2900 testing\n",
      "Test: \n",
      " Accuracy: 71.8%, Avg loss: 0.553510 \n",
      "\n",
      "loss: 0.483089  [2910/4210]\n",
      "loss: 0.567313  [2920/4210]\n",
      "loss: 0.594642  [2930/4210]\n",
      "loss: 0.590870  [2940/4210]\n",
      "loss: 0.631981  [2950/4210]\n",
      "loss: 0.617109  [2960/4210]\n",
      "loss: 0.629758  [2970/4210]\n",
      "loss: 0.418496  [2980/4210]\n",
      "loss: 0.576903  [2990/4210]\n",
      "loss: 0.477144  [3000/4210]\n",
      "In epoch 1 _ batch 3000 testing\n",
      "Test: \n",
      " Accuracy: 72.0%, Avg loss: 0.548387 \n",
      "\n",
      "loss: 0.681044  [3010/4210]\n",
      "loss: 0.599804  [3020/4210]\n",
      "loss: 0.571993  [3030/4210]\n",
      "loss: 0.859146  [3040/4210]\n",
      "loss: 0.560860  [3050/4210]\n",
      "loss: 0.598380  [3060/4210]\n",
      "loss: 0.653058  [3070/4210]\n",
      "loss: 0.620144  [3080/4210]\n",
      "loss: 0.521958  [3090/4210]\n",
      "loss: 0.492909  [3100/4210]\n",
      "In epoch 1 _ batch 3100 testing\n",
      "Test: \n",
      " Accuracy: 71.9%, Avg loss: 0.550677 \n",
      "\n",
      "loss: 0.550922  [3110/4210]\n",
      "loss: 0.738240  [3120/4210]\n",
      "loss: 0.455105  [3130/4210]\n",
      "loss: 0.429593  [3140/4210]\n",
      "loss: 0.440217  [3150/4210]\n",
      "loss: 0.519332  [3160/4210]\n",
      "loss: 0.439140  [3170/4210]\n",
      "loss: 0.568954  [3180/4210]\n",
      "loss: 0.563475  [3190/4210]\n",
      "loss: 0.508347  [3200/4210]\n",
      "In epoch 1 _ batch 3200 testing\n",
      "Test: \n",
      " Accuracy: 74.5%, Avg loss: 0.527619 \n",
      "\n",
      "loss: 0.491139  [3210/4210]\n",
      "loss: 0.752362  [3220/4210]\n",
      "loss: 0.340847  [3230/4210]\n",
      "loss: 0.525352  [3240/4210]\n",
      "loss: 0.507858  [3250/4210]\n",
      "loss: 0.472960  [3260/4210]\n",
      "loss: 0.553829  [3270/4210]\n",
      "loss: 0.821185  [3280/4210]\n",
      "loss: 0.566005  [3290/4210]\n",
      "loss: 0.349188  [3300/4210]\n",
      "In epoch 1 _ batch 3300 testing\n",
      "Test: \n",
      " Accuracy: 73.1%, Avg loss: 0.538119 \n",
      "\n",
      "loss: 0.549167  [3310/4210]\n",
      "loss: 0.461071  [3320/4210]\n",
      "loss: 0.448826  [3330/4210]\n",
      "loss: 0.569431  [3340/4210]\n",
      "loss: 0.444763  [3350/4210]\n",
      "loss: 0.665381  [3360/4210]\n",
      "loss: 0.624455  [3370/4210]\n",
      "loss: 0.418105  [3380/4210]\n",
      "loss: 0.642988  [3390/4210]\n",
      "loss: 0.406022  [3400/4210]\n",
      "In epoch 1 _ batch 3400 testing\n",
      "Test: \n",
      " Accuracy: 74.9%, Avg loss: 0.513392 \n",
      "\n",
      "loss: 0.528177  [3410/4210]\n",
      "loss: 0.586474  [3420/4210]\n",
      "loss: 0.663233  [3430/4210]\n",
      "loss: 0.458275  [3440/4210]\n",
      "loss: 0.564859  [3450/4210]\n",
      "loss: 0.484821  [3460/4210]\n",
      "loss: 0.586312  [3470/4210]\n",
      "loss: 0.745698  [3480/4210]\n",
      "loss: 0.707691  [3490/4210]\n",
      "loss: 0.571024  [3500/4210]\n",
      "In epoch 1 _ batch 3500 testing\n",
      "Test: \n",
      " Accuracy: 74.7%, Avg loss: 0.521847 \n",
      "\n",
      "loss: 0.627472  [3510/4210]\n",
      "loss: 0.440735  [3520/4210]\n",
      "loss: 0.511699  [3530/4210]\n",
      "loss: 0.874005  [3540/4210]\n",
      "loss: 0.501428  [3550/4210]\n",
      "loss: 0.921762  [3560/4210]\n",
      "loss: 0.418137  [3570/4210]\n",
      "loss: 0.814837  [3580/4210]\n",
      "loss: 0.522165  [3590/4210]\n",
      "loss: 0.391664  [3600/4210]\n",
      "In epoch 1 _ batch 3600 testing\n",
      "Test: \n",
      " Accuracy: 75.0%, Avg loss: 0.512019 \n",
      "\n",
      "loss: 0.598499  [3610/4210]\n",
      "loss: 0.445433  [3620/4210]\n",
      "loss: 0.713648  [3630/4210]\n",
      "loss: 0.495462  [3640/4210]\n",
      "loss: 0.531615  [3650/4210]\n",
      "loss: 0.444108  [3660/4210]\n",
      "loss: 0.621847  [3670/4210]\n",
      "loss: 0.676022  [3680/4210]\n",
      "loss: 0.290385  [3690/4210]\n",
      "loss: 0.607371  [3700/4210]\n",
      "In epoch 1 _ batch 3700 testing\n",
      "Test: \n",
      " Accuracy: 73.6%, Avg loss: 0.523286 \n",
      "\n",
      "loss: 0.614654  [3710/4210]\n",
      "loss: 0.568988  [3720/4210]\n",
      "loss: 0.410055  [3730/4210]\n",
      "loss: 0.492869  [3740/4210]\n",
      "loss: 0.533702  [3750/4210]\n",
      "loss: 0.465220  [3760/4210]\n",
      "loss: 0.496458  [3770/4210]\n",
      "loss: 0.707312  [3780/4210]\n",
      "loss: 0.628502  [3790/4210]\n",
      "loss: 0.488291  [3800/4210]\n",
      "In epoch 1 _ batch 3800 testing\n",
      "Test: \n",
      " Accuracy: 77.3%, Avg loss: 0.480644 \n",
      "\n",
      "loss: 0.711086  [3810/4210]\n",
      "loss: 0.565779  [3820/4210]\n",
      "loss: 0.362093  [3830/4210]\n",
      "loss: 0.426080  [3840/4210]\n",
      "loss: 0.399342  [3850/4210]\n",
      "loss: 0.505488  [3860/4210]\n",
      "loss: 0.454890  [3870/4210]\n",
      "loss: 0.442858  [3880/4210]\n",
      "loss: 0.523551  [3890/4210]\n",
      "loss: 0.375499  [3900/4210]\n",
      "In epoch 1 _ batch 3900 testing\n",
      "Test: \n",
      " Accuracy: 75.7%, Avg loss: 0.497004 \n",
      "\n",
      "loss: 0.729267  [3910/4210]\n",
      "loss: 0.528266  [3920/4210]\n",
      "loss: 0.455152  [3930/4210]\n",
      "loss: 0.502139  [3940/4210]\n",
      "loss: 0.468424  [3950/4210]\n",
      "loss: 0.305982  [3960/4210]\n",
      "loss: 0.481641  [3970/4210]\n",
      "loss: 0.434847  [3980/4210]\n",
      "loss: 0.541022  [3990/4210]\n",
      "loss: 0.840567  [4000/4210]\n",
      "In epoch 1 _ batch 4000 testing\n",
      "Test: \n",
      " Accuracy: 74.7%, Avg loss: 0.523556 \n",
      "\n",
      "loss: 0.517343  [4010/4210]\n",
      "loss: 0.471474  [4020/4210]\n",
      "loss: 0.569037  [4030/4210]\n",
      "loss: 0.492548  [4040/4210]\n",
      "loss: 0.416083  [4050/4210]\n",
      "loss: 0.406566  [4060/4210]\n",
      "loss: 0.476855  [4070/4210]\n",
      "loss: 0.546707  [4080/4210]\n",
      "loss: 0.850698  [4090/4210]\n",
      "loss: 0.543221  [4100/4210]\n",
      "In epoch 1 _ batch 4100 testing\n",
      "Test: \n",
      " Accuracy: 76.4%, Avg loss: 0.487449 \n",
      "\n",
      "loss: 0.439122  [4110/4210]\n",
      "loss: 0.507527  [4120/4210]\n",
      "loss: 0.320459  [4130/4210]\n",
      "loss: 0.657670  [4140/4210]\n",
      "loss: 0.569220  [4150/4210]\n",
      "loss: 0.610747  [4160/4210]\n",
      "loss: 0.564128  [4170/4210]\n",
      "loss: 0.256947  [4180/4210]\n",
      "loss: 0.413641  [4190/4210]\n",
      "loss: 0.478763  [4200/4210]\n",
      "In epoch 1 _ batch 4200 testing\n",
      "Test: \n",
      " Accuracy: 75.8%, Avg loss: 0.488406 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.292609  [  0/4210]\n",
      "In epoch 2 _ batch 0 testing\n",
      "Test: \n",
      " Accuracy: 75.9%, Avg loss: 0.481035 \n",
      "\n",
      "loss: 0.459924  [ 10/4210]\n",
      "loss: 0.351799  [ 20/4210]\n",
      "loss: 0.502220  [ 30/4210]\n",
      "loss: 0.460758  [ 40/4210]\n",
      "loss: 0.531797  [ 50/4210]\n",
      "loss: 0.541919  [ 60/4210]\n",
      "loss: 0.592808  [ 70/4210]\n",
      "loss: 0.263345  [ 80/4210]\n",
      "loss: 0.451212  [ 90/4210]\n",
      "loss: 0.431337  [100/4210]\n",
      "In epoch 2 _ batch 100 testing\n",
      "Test: \n",
      " Accuracy: 76.8%, Avg loss: 0.484841 \n",
      "\n",
      "loss: 0.443609  [110/4210]\n",
      "loss: 0.204786  [120/4210]\n",
      "loss: 0.682279  [130/4210]\n",
      "loss: 0.317836  [140/4210]\n",
      "loss: 0.432928  [150/4210]\n",
      "loss: 0.237370  [160/4210]\n",
      "loss: 0.396391  [170/4210]\n",
      "loss: 0.353264  [180/4210]\n",
      "loss: 0.476608  [190/4210]\n",
      "loss: 0.595364  [200/4210]\n",
      "In epoch 2 _ batch 200 testing\n",
      "Test: \n",
      " Accuracy: 70.3%, Avg loss: 0.593186 \n",
      "\n",
      "loss: 0.329039  [210/4210]\n",
      "loss: 0.360736  [220/4210]\n",
      "loss: 0.701886  [230/4210]\n",
      "loss: 0.529153  [240/4210]\n",
      "loss: 0.477325  [250/4210]\n",
      "loss: 0.668135  [260/4210]\n",
      "loss: 0.331858  [270/4210]\n",
      "loss: 0.306403  [280/4210]\n",
      "loss: 0.317388  [290/4210]\n",
      "loss: 0.482796  [300/4210]\n",
      "In epoch 2 _ batch 300 testing\n",
      "Test: \n",
      " Accuracy: 76.6%, Avg loss: 0.491385 \n",
      "\n",
      "loss: 0.530330  [310/4210]\n",
      "loss: 0.400801  [320/4210]\n",
      "loss: 0.374995  [330/4210]\n",
      "loss: 0.362646  [340/4210]\n",
      "loss: 0.406241  [350/4210]\n",
      "loss: 0.323612  [360/4210]\n",
      "loss: 0.457856  [370/4210]\n",
      "loss: 0.425432  [380/4210]\n",
      "loss: 0.270332  [390/4210]\n",
      "loss: 0.312434  [400/4210]\n",
      "In epoch 2 _ batch 400 testing\n",
      "Test: \n",
      " Accuracy: 77.5%, Avg loss: 0.473989 \n",
      "\n",
      "loss: 0.456564  [410/4210]\n",
      "loss: 0.559025  [420/4210]\n",
      "loss: 0.245608  [430/4210]\n",
      "loss: 0.278774  [440/4210]\n",
      "loss: 0.366532  [450/4210]\n",
      "loss: 0.481235  [460/4210]\n",
      "loss: 0.475377  [470/4210]\n",
      "loss: 0.463229  [480/4210]\n",
      "loss: 0.306574  [490/4210]\n",
      "loss: 0.228842  [500/4210]\n",
      "In epoch 2 _ batch 500 testing\n",
      "Test: \n",
      " Accuracy: 74.0%, Avg loss: 0.508278 \n",
      "\n",
      "loss: 0.501839  [510/4210]\n",
      "loss: 0.407544  [520/4210]\n",
      "loss: 0.448796  [530/4210]\n",
      "loss: 0.385494  [540/4210]\n",
      "loss: 0.304272  [550/4210]\n",
      "loss: 0.651204  [560/4210]\n",
      "loss: 0.659995  [570/4210]\n",
      "loss: 0.350140  [580/4210]\n",
      "loss: 0.298588  [590/4210]\n",
      "loss: 0.476124  [600/4210]\n",
      "In epoch 2 _ batch 600 testing\n",
      "Test: \n",
      " Accuracy: 78.0%, Avg loss: 0.463626 \n",
      "\n",
      "loss: 0.558194  [610/4210]\n",
      "loss: 0.460742  [620/4210]\n",
      "loss: 0.344762  [630/4210]\n",
      "loss: 0.278221  [640/4210]\n",
      "loss: 0.334389  [650/4210]\n",
      "loss: 0.438377  [660/4210]\n",
      "loss: 0.453420  [670/4210]\n",
      "loss: 0.384229  [680/4210]\n",
      "loss: 0.620716  [690/4210]\n",
      "loss: 0.264610  [700/4210]\n",
      "In epoch 2 _ batch 700 testing\n",
      "Test: \n",
      " Accuracy: 74.7%, Avg loss: 0.505950 \n",
      "\n",
      "loss: 0.547343  [710/4210]\n",
      "loss: 0.385307  [720/4210]\n",
      "loss: 0.405161  [730/4210]\n",
      "loss: 0.369231  [740/4210]\n",
      "loss: 0.388556  [750/4210]\n",
      "loss: 0.763637  [760/4210]\n",
      "loss: 0.313293  [770/4210]\n",
      "loss: 0.260051  [780/4210]\n",
      "loss: 0.425146  [790/4210]\n",
      "loss: 0.530995  [800/4210]\n",
      "In epoch 2 _ batch 800 testing\n",
      "Test: \n",
      " Accuracy: 78.0%, Avg loss: 0.472357 \n",
      "\n",
      "loss: 0.448138  [810/4210]\n",
      "loss: 0.473517  [820/4210]\n",
      "loss: 0.752297  [830/4210]\n",
      "loss: 0.336352  [840/4210]\n",
      "loss: 0.301198  [850/4210]\n",
      "loss: 0.545946  [860/4210]\n",
      "loss: 0.294497  [870/4210]\n",
      "loss: 0.534388  [880/4210]\n",
      "loss: 0.320654  [890/4210]\n",
      "loss: 0.410255  [900/4210]\n",
      "In epoch 2 _ batch 900 testing\n",
      "Test: \n",
      " Accuracy: 79.1%, Avg loss: 0.456518 \n",
      "\n",
      "loss: 0.316474  [910/4210]\n",
      "loss: 0.470270  [920/4210]\n",
      "loss: 0.491998  [930/4210]\n",
      "loss: 0.191183  [940/4210]\n",
      "loss: 0.298740  [950/4210]\n",
      "loss: 0.625555  [960/4210]\n",
      "loss: 0.484907  [970/4210]\n",
      "loss: 0.393901  [980/4210]\n",
      "loss: 0.465863  [990/4210]\n",
      "loss: 0.241233  [1000/4210]\n",
      "In epoch 2 _ batch 1000 testing\n",
      "Test: \n",
      " Accuracy: 79.1%, Avg loss: 0.460079 \n",
      "\n",
      "loss: 0.394120  [1010/4210]\n",
      "loss: 0.345574  [1020/4210]\n",
      "loss: 0.281652  [1030/4210]\n",
      "loss: 0.427932  [1040/4210]\n",
      "loss: 0.555161  [1050/4210]\n",
      "loss: 0.574339  [1060/4210]\n",
      "loss: 0.327437  [1070/4210]\n",
      "loss: 0.492634  [1080/4210]\n",
      "loss: 0.786678  [1090/4210]\n",
      "loss: 0.359618  [1100/4210]\n",
      "In epoch 2 _ batch 1100 testing\n",
      "Test: \n",
      " Accuracy: 76.8%, Avg loss: 0.470411 \n",
      "\n",
      "loss: 0.234466  [1110/4210]\n",
      "loss: 0.305023  [1120/4210]\n",
      "loss: 0.882979  [1130/4210]\n",
      "loss: 0.454693  [1140/4210]\n",
      "loss: 0.279221  [1150/4210]\n",
      "loss: 0.542199  [1160/4210]\n",
      "loss: 0.375948  [1170/4210]\n",
      "loss: 0.347303  [1180/4210]\n",
      "loss: 0.436527  [1190/4210]\n",
      "loss: 0.418204  [1200/4210]\n",
      "In epoch 2 _ batch 1200 testing\n",
      "Test: \n",
      " Accuracy: 79.7%, Avg loss: 0.446030 \n",
      "\n",
      "loss: 0.359565  [1210/4210]\n",
      "loss: 0.247414  [1220/4210]\n",
      "loss: 0.376630  [1230/4210]\n",
      "loss: 0.268552  [1240/4210]\n",
      "loss: 0.297984  [1250/4210]\n",
      "loss: 0.255985  [1260/4210]\n",
      "loss: 0.478237  [1270/4210]\n",
      "loss: 0.299767  [1280/4210]\n",
      "loss: 0.277491  [1290/4210]\n",
      "loss: 0.267735  [1300/4210]\n",
      "In epoch 2 _ batch 1300 testing\n",
      "Test: \n",
      " Accuracy: 79.2%, Avg loss: 0.435673 \n",
      "\n",
      "loss: 0.378528  [1310/4210]\n",
      "loss: 0.329159  [1320/4210]\n",
      "loss: 0.601792  [1330/4210]\n",
      "loss: 0.815721  [1340/4210]\n",
      "loss: 0.314353  [1350/4210]\n",
      "loss: 0.237842  [1360/4210]\n",
      "loss: 0.362295  [1370/4210]\n",
      "loss: 0.448717  [1380/4210]\n",
      "loss: 0.340993  [1390/4210]\n",
      "loss: 0.277733  [1400/4210]\n",
      "In epoch 2 _ batch 1400 testing\n",
      "Test: \n",
      " Accuracy: 78.9%, Avg loss: 0.448927 \n",
      "\n",
      "loss: 0.251232  [1410/4210]\n",
      "loss: 0.528805  [1420/4210]\n",
      "loss: 1.009893  [1430/4210]\n",
      "loss: 0.701331  [1440/4210]\n",
      "loss: 0.318247  [1450/4210]\n",
      "loss: 0.569057  [1460/4210]\n",
      "loss: 0.334040  [1470/4210]\n",
      "loss: 0.459284  [1480/4210]\n",
      "loss: 0.346602  [1490/4210]\n",
      "loss: 0.503117  [1500/4210]\n",
      "In epoch 2 _ batch 1500 testing\n",
      "Test: \n",
      " Accuracy: 77.2%, Avg loss: 0.490421 \n",
      "\n",
      "loss: 0.358970  [1510/4210]\n",
      "loss: 0.376416  [1520/4210]\n",
      "loss: 0.265749  [1530/4210]\n",
      "loss: 0.240139  [1540/4210]\n",
      "loss: 0.333738  [1550/4210]\n",
      "loss: 0.149559  [1560/4210]\n",
      "loss: 0.388249  [1570/4210]\n",
      "loss: 0.293494  [1580/4210]\n",
      "loss: 0.494329  [1590/4210]\n",
      "loss: 0.297374  [1600/4210]\n",
      "In epoch 2 _ batch 1600 testing\n",
      "Test: \n",
      " Accuracy: 78.1%, Avg loss: 0.478641 \n",
      "\n",
      "loss: 0.525697  [1610/4210]\n",
      "loss: 0.505894  [1620/4210]\n",
      "loss: 0.379297  [1630/4210]\n",
      "loss: 0.292509  [1640/4210]\n",
      "loss: 0.300963  [1650/4210]\n",
      "loss: 0.607134  [1660/4210]\n",
      "loss: 0.395592  [1670/4210]\n",
      "loss: 0.406623  [1680/4210]\n",
      "loss: 0.105032  [1690/4210]\n",
      "loss: 0.433169  [1700/4210]\n",
      "In epoch 2 _ batch 1700 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.441070 \n",
      "\n",
      "loss: 0.497051  [1710/4210]\n",
      "loss: 0.396060  [1720/4210]\n",
      "loss: 0.137055  [1730/4210]\n",
      "loss: 0.295229  [1740/4210]\n",
      "loss: 0.152021  [1750/4210]\n",
      "loss: 0.388007  [1760/4210]\n",
      "loss: 0.313724  [1770/4210]\n",
      "loss: 0.521205  [1780/4210]\n",
      "loss: 0.322949  [1790/4210]\n",
      "loss: 0.295091  [1800/4210]\n",
      "In epoch 2 _ batch 1800 testing\n",
      "Test: \n",
      " Accuracy: 79.1%, Avg loss: 0.460588 \n",
      "\n",
      "loss: 0.367075  [1810/4210]\n",
      "loss: 0.267961  [1820/4210]\n",
      "loss: 0.312035  [1830/4210]\n",
      "loss: 0.286381  [1840/4210]\n",
      "loss: 0.436575  [1850/4210]\n",
      "loss: 0.272023  [1860/4210]\n",
      "loss: 0.389103  [1870/4210]\n",
      "loss: 0.095082  [1880/4210]\n",
      "loss: 0.740163  [1890/4210]\n",
      "loss: 0.172977  [1900/4210]\n",
      "In epoch 2 _ batch 1900 testing\n",
      "Test: \n",
      " Accuracy: 76.5%, Avg loss: 0.492255 \n",
      "\n",
      "loss: 0.524744  [1910/4210]\n",
      "loss: 0.392038  [1920/4210]\n",
      "loss: 0.478769  [1930/4210]\n",
      "loss: 0.163548  [1940/4210]\n",
      "loss: 0.392254  [1950/4210]\n",
      "loss: 0.543424  [1960/4210]\n",
      "loss: 0.230583  [1970/4210]\n",
      "loss: 0.304134  [1980/4210]\n",
      "loss: 0.619083  [1990/4210]\n",
      "loss: 0.367661  [2000/4210]\n",
      "In epoch 2 _ batch 2000 testing\n",
      "Test: \n",
      " Accuracy: 79.9%, Avg loss: 0.445348 \n",
      "\n",
      "loss: 0.268082  [2010/4210]\n",
      "loss: 0.584804  [2020/4210]\n",
      "loss: 0.349066  [2030/4210]\n",
      "loss: 0.404672  [2040/4210]\n",
      "loss: 0.315632  [2050/4210]\n",
      "loss: 0.277036  [2060/4210]\n",
      "loss: 0.361654  [2070/4210]\n",
      "loss: 0.118192  [2080/4210]\n",
      "loss: 0.273054  [2090/4210]\n",
      "loss: 0.516267  [2100/4210]\n",
      "In epoch 2 _ batch 2100 testing\n",
      "Test: \n",
      " Accuracy: 76.1%, Avg loss: 0.492907 \n",
      "\n",
      "loss: 0.171935  [2110/4210]\n",
      "loss: 0.459185  [2120/4210]\n",
      "loss: 0.403215  [2130/4210]\n",
      "loss: 0.394355  [2140/4210]\n",
      "loss: 0.473379  [2150/4210]\n",
      "loss: 0.425629  [2160/4210]\n",
      "loss: 0.498452  [2170/4210]\n",
      "loss: 0.315142  [2180/4210]\n",
      "loss: 0.177942  [2190/4210]\n",
      "loss: 0.379762  [2200/4210]\n",
      "In epoch 2 _ batch 2200 testing\n",
      "Test: \n",
      " Accuracy: 79.4%, Avg loss: 0.457252 \n",
      "\n",
      "loss: 0.289699  [2210/4210]\n",
      "loss: 0.322183  [2220/4210]\n",
      "loss: 0.288266  [2230/4210]\n",
      "loss: 0.166784  [2240/4210]\n",
      "loss: 0.347632  [2250/4210]\n",
      "loss: 0.189930  [2260/4210]\n",
      "loss: 0.893527  [2270/4210]\n",
      "loss: 0.301593  [2280/4210]\n",
      "loss: 0.467857  [2290/4210]\n",
      "loss: 0.636837  [2300/4210]\n",
      "In epoch 2 _ batch 2300 testing\n",
      "Test: \n",
      " Accuracy: 77.3%, Avg loss: 0.480076 \n",
      "\n",
      "loss: 0.497990  [2310/4210]\n",
      "loss: 0.377130  [2320/4210]\n",
      "loss: 0.367486  [2330/4210]\n",
      "loss: 0.275079  [2340/4210]\n",
      "loss: 0.383235  [2350/4210]\n",
      "loss: 0.555917  [2360/4210]\n",
      "loss: 0.314240  [2370/4210]\n",
      "loss: 0.402677  [2380/4210]\n",
      "loss: 0.495189  [2390/4210]\n",
      "loss: 0.262735  [2400/4210]\n",
      "In epoch 2 _ batch 2400 testing\n",
      "Test: \n",
      " Accuracy: 79.9%, Avg loss: 0.424250 \n",
      "\n",
      "loss: 0.496184  [2410/4210]\n",
      "loss: 0.242819  [2420/4210]\n",
      "loss: 0.229188  [2430/4210]\n",
      "loss: 0.197490  [2440/4210]\n",
      "loss: 0.297715  [2450/4210]\n",
      "loss: 0.723224  [2460/4210]\n",
      "loss: 0.163225  [2470/4210]\n",
      "loss: 0.575240  [2480/4210]\n",
      "loss: 0.693728  [2490/4210]\n",
      "loss: 0.334826  [2500/4210]\n",
      "In epoch 2 _ batch 2500 testing\n",
      "Test: \n",
      " Accuracy: 79.5%, Avg loss: 0.421639 \n",
      "\n",
      "loss: 0.320730  [2510/4210]\n",
      "loss: 0.178863  [2520/4210]\n",
      "loss: 0.407725  [2530/4210]\n",
      "loss: 0.163898  [2540/4210]\n",
      "loss: 0.214653  [2550/4210]\n",
      "loss: 0.202353  [2560/4210]\n",
      "loss: 0.343092  [2570/4210]\n",
      "loss: 0.579907  [2580/4210]\n",
      "loss: 0.456753  [2590/4210]\n",
      "loss: 0.150145  [2600/4210]\n",
      "In epoch 2 _ batch 2600 testing\n",
      "Test: \n",
      " Accuracy: 79.5%, Avg loss: 0.459556 \n",
      "\n",
      "loss: 0.640990  [2610/4210]\n",
      "loss: 0.407520  [2620/4210]\n",
      "loss: 0.218054  [2630/4210]\n",
      "loss: 0.206955  [2640/4210]\n",
      "loss: 0.451047  [2650/4210]\n",
      "loss: 0.260470  [2660/4210]\n",
      "loss: 0.336509  [2670/4210]\n",
      "loss: 0.301284  [2680/4210]\n",
      "loss: 0.489956  [2690/4210]\n",
      "loss: 0.469838  [2700/4210]\n",
      "In epoch 2 _ batch 2700 testing\n",
      "Test: \n",
      " Accuracy: 79.0%, Avg loss: 0.442397 \n",
      "\n",
      "loss: 0.268307  [2710/4210]\n",
      "loss: 0.364654  [2720/4210]\n",
      "loss: 0.316163  [2730/4210]\n",
      "loss: 0.583606  [2740/4210]\n",
      "loss: 0.213367  [2750/4210]\n",
      "loss: 0.253918  [2760/4210]\n",
      "loss: 0.219323  [2770/4210]\n",
      "loss: 0.260247  [2780/4210]\n",
      "loss: 0.381354  [2790/4210]\n",
      "loss: 0.354627  [2800/4210]\n",
      "In epoch 2 _ batch 2800 testing\n",
      "Test: \n",
      " Accuracy: 79.6%, Avg loss: 0.449575 \n",
      "\n",
      "loss: 0.409819  [2810/4210]\n",
      "loss: 0.194619  [2820/4210]\n",
      "loss: 0.312718  [2830/4210]\n",
      "loss: 0.508039  [2840/4210]\n",
      "loss: 0.250381  [2850/4210]\n",
      "loss: 0.533334  [2860/4210]\n",
      "loss: 0.609744  [2870/4210]\n",
      "loss: 0.487667  [2880/4210]\n",
      "loss: 0.494905  [2890/4210]\n",
      "loss: 0.279028  [2900/4210]\n",
      "In epoch 2 _ batch 2900 testing\n",
      "Test: \n",
      " Accuracy: 77.2%, Avg loss: 0.493343 \n",
      "\n",
      "loss: 0.330557  [2910/4210]\n",
      "loss: 0.500687  [2920/4210]\n",
      "loss: 0.312817  [2930/4210]\n",
      "loss: 0.358349  [2940/4210]\n",
      "loss: 0.398587  [2950/4210]\n",
      "loss: 0.540967  [2960/4210]\n",
      "loss: 0.803062  [2970/4210]\n",
      "loss: 0.279162  [2980/4210]\n",
      "loss: 0.258304  [2990/4210]\n",
      "loss: 0.470407  [3000/4210]\n",
      "In epoch 2 _ batch 3000 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.430951 \n",
      "\n",
      "loss: 0.237627  [3010/4210]\n",
      "loss: 0.262951  [3020/4210]\n",
      "loss: 0.325876  [3030/4210]\n",
      "loss: 0.468146  [3040/4210]\n",
      "loss: 0.373404  [3050/4210]\n",
      "loss: 0.288500  [3060/4210]\n",
      "loss: 0.221785  [3070/4210]\n",
      "loss: 0.355645  [3080/4210]\n",
      "loss: 0.295958  [3090/4210]\n",
      "loss: 0.471961  [3100/4210]\n",
      "In epoch 2 _ batch 3100 testing\n",
      "Test: \n",
      " Accuracy: 78.8%, Avg loss: 0.438778 \n",
      "\n",
      "loss: 0.340192  [3110/4210]\n",
      "loss: 0.397515  [3120/4210]\n",
      "loss: 0.312023  [3130/4210]\n",
      "loss: 0.387173  [3140/4210]\n",
      "loss: 0.139473  [3150/4210]\n",
      "loss: 0.113013  [3160/4210]\n",
      "loss: 0.249469  [3170/4210]\n",
      "loss: 0.239866  [3180/4210]\n",
      "loss: 0.333602  [3190/4210]\n",
      "loss: 0.266168  [3200/4210]\n",
      "In epoch 2 _ batch 3200 testing\n",
      "Test: \n",
      " Accuracy: 79.5%, Avg loss: 0.449409 \n",
      "\n",
      "loss: 0.344562  [3210/4210]\n",
      "loss: 0.192454  [3220/4210]\n",
      "loss: 0.689570  [3230/4210]\n",
      "loss: 0.359038  [3240/4210]\n",
      "loss: 0.223773  [3250/4210]\n",
      "loss: 0.383568  [3260/4210]\n",
      "loss: 0.328168  [3270/4210]\n",
      "loss: 0.366207  [3280/4210]\n",
      "loss: 0.529299  [3290/4210]\n",
      "loss: 0.141412  [3300/4210]\n",
      "In epoch 2 _ batch 3300 testing\n",
      "Test: \n",
      " Accuracy: 79.9%, Avg loss: 0.440544 \n",
      "\n",
      "loss: 0.325368  [3310/4210]\n",
      "loss: 0.465351  [3320/4210]\n",
      "loss: 0.394620  [3330/4210]\n",
      "loss: 0.318601  [3340/4210]\n",
      "loss: 0.369621  [3350/4210]\n",
      "loss: 0.226174  [3360/4210]\n",
      "loss: 0.369709  [3370/4210]\n",
      "loss: 0.360159  [3380/4210]\n",
      "loss: 0.263557  [3390/4210]\n",
      "loss: 0.136761  [3400/4210]\n",
      "In epoch 2 _ batch 3400 testing\n",
      "Test: \n",
      " Accuracy: 78.6%, Avg loss: 0.458953 \n",
      "\n",
      "loss: 0.250263  [3410/4210]\n",
      "loss: 0.523197  [3420/4210]\n",
      "loss: 0.404588  [3430/4210]\n",
      "loss: 0.292236  [3440/4210]\n",
      "loss: 0.196769  [3450/4210]\n",
      "loss: 0.224678  [3460/4210]\n",
      "loss: 0.395979  [3470/4210]\n",
      "loss: 0.153203  [3480/4210]\n",
      "loss: 0.273365  [3490/4210]\n",
      "loss: 0.388468  [3500/4210]\n",
      "In epoch 2 _ batch 3500 testing\n",
      "Test: \n",
      " Accuracy: 80.5%, Avg loss: 0.431890 \n",
      "\n",
      "loss: 0.525468  [3510/4210]\n",
      "loss: 0.452792  [3520/4210]\n",
      "loss: 0.207288  [3530/4210]\n",
      "loss: 1.196772  [3540/4210]\n",
      "loss: 0.151639  [3550/4210]\n",
      "loss: 0.318217  [3560/4210]\n",
      "loss: 0.244277  [3570/4210]\n",
      "loss: 0.396333  [3580/4210]\n",
      "loss: 0.441420  [3590/4210]\n",
      "loss: 0.113489  [3600/4210]\n",
      "In epoch 2 _ batch 3600 testing\n",
      "Test: \n",
      " Accuracy: 79.8%, Avg loss: 0.437357 \n",
      "\n",
      "loss: 0.424173  [3610/4210]\n",
      "loss: 0.306220  [3620/4210]\n",
      "loss: 0.259386  [3630/4210]\n",
      "loss: 0.365367  [3640/4210]\n",
      "loss: 0.086379  [3650/4210]\n",
      "loss: 0.426213  [3660/4210]\n",
      "loss: 0.256617  [3670/4210]\n",
      "loss: 0.423404  [3680/4210]\n",
      "loss: 0.172700  [3690/4210]\n",
      "loss: 0.359599  [3700/4210]\n",
      "In epoch 2 _ batch 3700 testing\n",
      "Test: \n",
      " Accuracy: 80.2%, Avg loss: 0.451587 \n",
      "\n",
      "loss: 0.228724  [3710/4210]\n",
      "loss: 0.293928  [3720/4210]\n",
      "loss: 0.489645  [3730/4210]\n",
      "loss: 0.194267  [3740/4210]\n",
      "loss: 0.311260  [3750/4210]\n",
      "loss: 0.246521  [3760/4210]\n",
      "loss: 0.602274  [3770/4210]\n",
      "loss: 0.400295  [3780/4210]\n",
      "loss: 0.304892  [3790/4210]\n",
      "loss: 0.354631  [3800/4210]\n",
      "In epoch 2 _ batch 3800 testing\n",
      "Test: \n",
      " Accuracy: 79.4%, Avg loss: 0.449728 \n",
      "\n",
      "loss: 0.722037  [3810/4210]\n",
      "loss: 0.337818  [3820/4210]\n",
      "loss: 0.232470  [3830/4210]\n",
      "loss: 0.644202  [3840/4210]\n",
      "loss: 0.436113  [3850/4210]\n",
      "loss: 0.470788  [3860/4210]\n",
      "loss: 0.365126  [3870/4210]\n",
      "loss: 0.228260  [3880/4210]\n",
      "loss: 0.484491  [3890/4210]\n",
      "loss: 0.221905  [3900/4210]\n",
      "In epoch 2 _ batch 3900 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.414840 \n",
      "\n",
      "loss: 0.325805  [3910/4210]\n",
      "loss: 0.269892  [3920/4210]\n",
      "loss: 0.208526  [3930/4210]\n",
      "loss: 0.520634  [3940/4210]\n",
      "loss: 0.286869  [3950/4210]\n",
      "loss: 0.217209  [3960/4210]\n",
      "loss: 0.197097  [3970/4210]\n",
      "loss: 0.231590  [3980/4210]\n",
      "loss: 0.256264  [3990/4210]\n",
      "loss: 0.192342  [4000/4210]\n",
      "In epoch 2 _ batch 4000 testing\n",
      "Test: \n",
      " Accuracy: 79.9%, Avg loss: 0.470983 \n",
      "\n",
      "loss: 0.279733  [4010/4210]\n",
      "loss: 0.219949  [4020/4210]\n",
      "loss: 0.282694  [4030/4210]\n",
      "loss: 0.310149  [4040/4210]\n",
      "loss: 0.278545  [4050/4210]\n",
      "loss: 0.234694  [4060/4210]\n",
      "loss: 0.447353  [4070/4210]\n",
      "loss: 0.206887  [4080/4210]\n",
      "loss: 0.204729  [4090/4210]\n",
      "loss: 0.358831  [4100/4210]\n",
      "In epoch 2 _ batch 4100 testing\n",
      "Test: \n",
      " Accuracy: 80.6%, Avg loss: 0.437668 \n",
      "\n",
      "loss: 0.209578  [4110/4210]\n",
      "loss: 0.313599  [4120/4210]\n",
      "loss: 0.217240  [4130/4210]\n",
      "loss: 0.404023  [4140/4210]\n",
      "loss: 0.236258  [4150/4210]\n",
      "loss: 0.374636  [4160/4210]\n",
      "loss: 0.362414  [4170/4210]\n",
      "loss: 0.185944  [4180/4210]\n",
      "loss: 0.289022  [4190/4210]\n",
      "loss: 0.284528  [4200/4210]\n",
      "In epoch 2 _ batch 4200 testing\n",
      "Test: \n",
      " Accuracy: 79.5%, Avg loss: 0.452117 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.384225  [  0/4210]\n",
      "In epoch 3 _ batch 0 testing\n",
      "Test: \n",
      " Accuracy: 80.2%, Avg loss: 0.460947 \n",
      "\n",
      "loss: 0.176161  [ 10/4210]\n",
      "loss: 0.389092  [ 20/4210]\n",
      "loss: 0.236460  [ 30/4210]\n",
      "loss: 0.248381  [ 40/4210]\n",
      "loss: 0.206373  [ 50/4210]\n",
      "loss: 0.219366  [ 60/4210]\n",
      "loss: 0.186829  [ 70/4210]\n",
      "loss: 0.154143  [ 80/4210]\n",
      "loss: 0.466133  [ 90/4210]\n",
      "loss: 0.213942  [100/4210]\n",
      "In epoch 3 _ batch 100 testing\n",
      "Test: \n",
      " Accuracy: 79.4%, Avg loss: 0.495689 \n",
      "\n",
      "loss: 0.059599  [110/4210]\n",
      "loss: 0.306907  [120/4210]\n",
      "loss: 0.454018  [130/4210]\n",
      "loss: 0.094854  [140/4210]\n",
      "loss: 0.166347  [150/4210]\n",
      "loss: 0.240281  [160/4210]\n",
      "loss: 0.553445  [170/4210]\n",
      "loss: 0.448454  [180/4210]\n",
      "loss: 0.416693  [190/4210]\n",
      "loss: 0.304323  [200/4210]\n",
      "In epoch 3 _ batch 200 testing\n",
      "Test: \n",
      " Accuracy: 78.8%, Avg loss: 0.485185 \n",
      "\n",
      "loss: 0.192621  [210/4210]\n",
      "loss: 0.231741  [220/4210]\n",
      "loss: 0.602678  [230/4210]\n",
      "loss: 0.360865  [240/4210]\n",
      "loss: 0.395082  [250/4210]\n",
      "loss: 0.251794  [260/4210]\n",
      "loss: 0.454442  [270/4210]\n",
      "loss: 0.284797  [280/4210]\n",
      "loss: 0.300152  [290/4210]\n",
      "loss: 0.408495  [300/4210]\n",
      "In epoch 3 _ batch 300 testing\n",
      "Test: \n",
      " Accuracy: 76.1%, Avg loss: 0.501957 \n",
      "\n",
      "loss: 0.528321  [310/4210]\n",
      "loss: 0.517409  [320/4210]\n",
      "loss: 0.330792  [330/4210]\n",
      "loss: 0.236206  [340/4210]\n",
      "loss: 0.441830  [350/4210]\n",
      "loss: 0.386071  [360/4210]\n",
      "loss: 0.533152  [370/4210]\n",
      "loss: 0.757235  [380/4210]\n",
      "loss: 0.195156  [390/4210]\n",
      "loss: 0.262247  [400/4210]\n",
      "In epoch 3 _ batch 400 testing\n",
      "Test: \n",
      " Accuracy: 79.8%, Avg loss: 0.436249 \n",
      "\n",
      "loss: 0.190292  [410/4210]\n",
      "loss: 0.482441  [420/4210]\n",
      "loss: 0.463741  [430/4210]\n",
      "loss: 0.434968  [440/4210]\n",
      "loss: 0.387699  [450/4210]\n",
      "loss: 0.356750  [460/4210]\n",
      "loss: 0.190118  [470/4210]\n",
      "loss: 0.545418  [480/4210]\n",
      "loss: 0.134439  [490/4210]\n",
      "loss: 0.373495  [500/4210]\n",
      "In epoch 3 _ batch 500 testing\n",
      "Test: \n",
      " Accuracy: 78.8%, Avg loss: 0.454671 \n",
      "\n",
      "loss: 0.254460  [510/4210]\n",
      "loss: 0.521467  [520/4210]\n",
      "loss: 0.195287  [530/4210]\n",
      "loss: 0.261619  [540/4210]\n",
      "loss: 0.312278  [550/4210]\n",
      "loss: 0.295052  [560/4210]\n",
      "loss: 0.401974  [570/4210]\n",
      "loss: 0.259325  [580/4210]\n",
      "loss: 0.149017  [590/4210]\n",
      "loss: 0.149279  [600/4210]\n",
      "In epoch 3 _ batch 600 testing\n",
      "Test: \n",
      " Accuracy: 80.0%, Avg loss: 0.453077 \n",
      "\n",
      "loss: 0.191887  [610/4210]\n",
      "loss: 0.502144  [620/4210]\n",
      "loss: 0.147430  [630/4210]\n",
      "loss: 0.268073  [640/4210]\n",
      "loss: 0.933115  [650/4210]\n",
      "loss: 0.343472  [660/4210]\n",
      "loss: 0.145243  [670/4210]\n",
      "loss: 0.136100  [680/4210]\n",
      "loss: 0.416602  [690/4210]\n",
      "loss: 0.238606  [700/4210]\n",
      "In epoch 3 _ batch 700 testing\n",
      "Test: \n",
      " Accuracy: 80.6%, Avg loss: 0.448536 \n",
      "\n",
      "loss: 0.442589  [710/4210]\n",
      "loss: 0.405673  [720/4210]\n",
      "loss: 0.277220  [730/4210]\n",
      "loss: 0.452619  [740/4210]\n",
      "loss: 0.134580  [750/4210]\n",
      "loss: 0.493333  [760/4210]\n",
      "loss: 0.491155  [770/4210]\n",
      "loss: 0.253990  [780/4210]\n",
      "loss: 0.456539  [790/4210]\n",
      "loss: 0.191128  [800/4210]\n",
      "In epoch 3 _ batch 800 testing\n",
      "Test: \n",
      " Accuracy: 79.1%, Avg loss: 0.441839 \n",
      "\n",
      "loss: 0.284278  [810/4210]\n",
      "loss: 0.256343  [820/4210]\n",
      "loss: 0.150450  [830/4210]\n",
      "loss: 0.216759  [840/4210]\n",
      "loss: 0.260001  [850/4210]\n",
      "loss: 0.398792  [860/4210]\n",
      "loss: 0.193088  [870/4210]\n",
      "loss: 0.313212  [880/4210]\n",
      "loss: 0.117076  [890/4210]\n",
      "loss: 0.233280  [900/4210]\n",
      "In epoch 3 _ batch 900 testing\n",
      "Test: \n",
      " Accuracy: 77.1%, Avg loss: 0.478202 \n",
      "\n",
      "loss: 0.227712  [910/4210]\n",
      "loss: 0.273419  [920/4210]\n",
      "loss: 0.215555  [930/4210]\n",
      "loss: 0.245731  [940/4210]\n",
      "loss: 0.145582  [950/4210]\n",
      "loss: 0.236503  [960/4210]\n",
      "loss: 0.292147  [970/4210]\n",
      "loss: 0.469861  [980/4210]\n",
      "loss: 0.613234  [990/4210]\n",
      "loss: 0.486954  [1000/4210]\n",
      "In epoch 3 _ batch 1000 testing\n",
      "Test: \n",
      " Accuracy: 80.6%, Avg loss: 0.443300 \n",
      "\n",
      "loss: 0.284241  [1010/4210]\n",
      "loss: 0.227006  [1020/4210]\n",
      "loss: 0.368447  [1030/4210]\n",
      "loss: 0.322104  [1040/4210]\n",
      "loss: 0.605956  [1050/4210]\n",
      "loss: 0.275862  [1060/4210]\n",
      "loss: 0.139223  [1070/4210]\n",
      "loss: 0.154843  [1080/4210]\n",
      "loss: 0.185627  [1090/4210]\n",
      "loss: 0.469865  [1100/4210]\n",
      "In epoch 3 _ batch 1100 testing\n",
      "Test: \n",
      " Accuracy: 79.0%, Avg loss: 0.451562 \n",
      "\n",
      "loss: 0.428537  [1110/4210]\n",
      "loss: 0.458405  [1120/4210]\n",
      "loss: 0.159577  [1130/4210]\n",
      "loss: 0.403198  [1140/4210]\n",
      "loss: 0.354979  [1150/4210]\n",
      "loss: 0.249294  [1160/4210]\n",
      "loss: 0.468151  [1170/4210]\n",
      "loss: 0.308063  [1180/4210]\n",
      "loss: 0.138705  [1190/4210]\n",
      "loss: 0.267298  [1200/4210]\n",
      "In epoch 3 _ batch 1200 testing\n",
      "Test: \n",
      " Accuracy: 79.4%, Avg loss: 0.429123 \n",
      "\n",
      "loss: 0.165259  [1210/4210]\n",
      "loss: 0.482546  [1220/4210]\n",
      "loss: 0.242750  [1230/4210]\n",
      "loss: 0.164167  [1240/4210]\n",
      "loss: 0.548304  [1250/4210]\n",
      "loss: 0.330408  [1260/4210]\n",
      "loss: 0.413383  [1270/4210]\n",
      "loss: 0.274866  [1280/4210]\n",
      "loss: 0.091944  [1290/4210]\n",
      "loss: 0.193643  [1300/4210]\n",
      "In epoch 3 _ batch 1300 testing\n",
      "Test: \n",
      " Accuracy: 80.2%, Avg loss: 0.448437 \n",
      "\n",
      "loss: 0.318739  [1310/4210]\n",
      "loss: 0.359299  [1320/4210]\n",
      "loss: 0.424794  [1330/4210]\n",
      "loss: 0.526445  [1340/4210]\n",
      "loss: 0.254128  [1350/4210]\n",
      "loss: 0.198201  [1360/4210]\n",
      "loss: 0.335435  [1370/4210]\n",
      "loss: 0.108170  [1380/4210]\n",
      "loss: 0.272736  [1390/4210]\n",
      "loss: 0.425283  [1400/4210]\n",
      "In epoch 3 _ batch 1400 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.423941 \n",
      "\n",
      "loss: 0.422960  [1410/4210]\n",
      "loss: 0.212300  [1420/4210]\n",
      "loss: 0.251057  [1430/4210]\n",
      "loss: 0.548532  [1440/4210]\n",
      "loss: 0.308042  [1450/4210]\n",
      "loss: 0.102921  [1460/4210]\n",
      "loss: 0.438413  [1470/4210]\n",
      "loss: 0.202089  [1480/4210]\n",
      "loss: 0.217335  [1490/4210]\n",
      "loss: 0.243873  [1500/4210]\n",
      "In epoch 3 _ batch 1500 testing\n",
      "Test: \n",
      " Accuracy: 79.6%, Avg loss: 0.444074 \n",
      "\n",
      "loss: 0.248166  [1510/4210]\n",
      "loss: 0.188817  [1520/4210]\n",
      "loss: 0.533327  [1530/4210]\n",
      "loss: 0.349833  [1540/4210]\n",
      "loss: 0.348513  [1550/4210]\n",
      "loss: 0.290398  [1560/4210]\n",
      "loss: 0.188225  [1570/4210]\n",
      "loss: 0.198647  [1580/4210]\n",
      "loss: 0.274368  [1590/4210]\n",
      "loss: 0.180253  [1600/4210]\n",
      "In epoch 3 _ batch 1600 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.418617 \n",
      "\n",
      "loss: 0.241338  [1610/4210]\n",
      "loss: 0.245145  [1620/4210]\n",
      "loss: 0.223920  [1630/4210]\n",
      "loss: 0.340617  [1640/4210]\n",
      "loss: 0.146134  [1650/4210]\n",
      "loss: 0.171887  [1660/4210]\n",
      "loss: 0.280706  [1670/4210]\n",
      "loss: 0.345936  [1680/4210]\n",
      "loss: 0.350475  [1690/4210]\n",
      "loss: 0.198978  [1700/4210]\n",
      "In epoch 3 _ batch 1700 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.425094 \n",
      "\n",
      "loss: 0.453046  [1710/4210]\n",
      "loss: 0.252651  [1720/4210]\n",
      "loss: 0.244311  [1730/4210]\n",
      "loss: 0.181905  [1740/4210]\n",
      "loss: 0.182626  [1750/4210]\n",
      "loss: 0.279765  [1760/4210]\n",
      "loss: 0.193240  [1770/4210]\n",
      "loss: 0.697268  [1780/4210]\n",
      "loss: 0.278367  [1790/4210]\n",
      "loss: 0.183133  [1800/4210]\n",
      "In epoch 3 _ batch 1800 testing\n",
      "Test: \n",
      " Accuracy: 79.5%, Avg loss: 0.443107 \n",
      "\n",
      "loss: 0.410882  [1810/4210]\n",
      "loss: 0.520421  [1820/4210]\n",
      "loss: 0.200560  [1830/4210]\n",
      "loss: 0.130139  [1840/4210]\n",
      "loss: 0.598763  [1850/4210]\n",
      "loss: 0.254998  [1860/4210]\n",
      "loss: 0.066172  [1870/4210]\n",
      "loss: 0.401433  [1880/4210]\n",
      "loss: 0.378776  [1890/4210]\n",
      "loss: 0.243489  [1900/4210]\n",
      "In epoch 3 _ batch 1900 testing\n",
      "Test: \n",
      " Accuracy: 80.3%, Avg loss: 0.420005 \n",
      "\n",
      "loss: 0.487864  [1910/4210]\n",
      "loss: 0.130470  [1920/4210]\n",
      "loss: 0.286715  [1930/4210]\n",
      "loss: 0.238114  [1940/4210]\n",
      "loss: 0.457852  [1950/4210]\n",
      "loss: 0.311516  [1960/4210]\n",
      "loss: 0.368954  [1970/4210]\n",
      "loss: 0.294161  [1980/4210]\n",
      "loss: 0.282766  [1990/4210]\n",
      "loss: 0.068301  [2000/4210]\n",
      "In epoch 3 _ batch 2000 testing\n",
      "Test: \n",
      " Accuracy: 80.3%, Avg loss: 0.438768 \n",
      "\n",
      "loss: 0.233520  [2010/4210]\n",
      "loss: 0.407694  [2020/4210]\n",
      "loss: 0.494639  [2030/4210]\n",
      "loss: 0.431787  [2040/4210]\n",
      "loss: 0.299824  [2050/4210]\n",
      "loss: 0.590277  [2060/4210]\n",
      "loss: 0.222623  [2070/4210]\n",
      "loss: 0.193794  [2080/4210]\n",
      "loss: 0.168227  [2090/4210]\n",
      "loss: 0.177359  [2100/4210]\n",
      "In epoch 3 _ batch 2100 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.435362 \n",
      "\n",
      "loss: 0.367718  [2110/4210]\n",
      "loss: 0.588626  [2120/4210]\n",
      "loss: 0.387399  [2130/4210]\n",
      "loss: 0.181862  [2140/4210]\n",
      "loss: 0.266234  [2150/4210]\n",
      "loss: 0.257510  [2160/4210]\n",
      "loss: 0.123367  [2170/4210]\n",
      "loss: 0.436122  [2180/4210]\n",
      "loss: 0.414385  [2190/4210]\n",
      "loss: 0.278746  [2200/4210]\n",
      "In epoch 3 _ batch 2200 testing\n",
      "Test: \n",
      " Accuracy: 79.8%, Avg loss: 0.483044 \n",
      "\n",
      "loss: 0.417426  [2210/4210]\n",
      "loss: 0.203754  [2220/4210]\n",
      "loss: 0.213137  [2230/4210]\n",
      "loss: 0.322349  [2240/4210]\n",
      "loss: 0.203252  [2250/4210]\n",
      "loss: 0.654433  [2260/4210]\n",
      "loss: 0.588557  [2270/4210]\n",
      "loss: 0.408261  [2280/4210]\n",
      "loss: 0.163948  [2290/4210]\n",
      "loss: 0.200406  [2300/4210]\n",
      "In epoch 3 _ batch 2300 testing\n",
      "Test: \n",
      " Accuracy: 79.9%, Avg loss: 0.478214 \n",
      "\n",
      "loss: 0.163150  [2310/4210]\n",
      "loss: 0.140522  [2320/4210]\n",
      "loss: 0.450275  [2330/4210]\n",
      "loss: 0.236895  [2340/4210]\n",
      "loss: 0.204890  [2350/4210]\n",
      "loss: 0.145606  [2360/4210]\n",
      "loss: 0.241703  [2370/4210]\n",
      "loss: 0.186334  [2380/4210]\n",
      "loss: 0.087567  [2390/4210]\n",
      "loss: 0.242783  [2400/4210]\n",
      "In epoch 3 _ batch 2400 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.429939 \n",
      "\n",
      "loss: 0.099182  [2410/4210]\n",
      "loss: 0.585645  [2420/4210]\n",
      "loss: 0.211408  [2430/4210]\n",
      "loss: 0.354092  [2440/4210]\n",
      "loss: 0.313411  [2450/4210]\n",
      "loss: 0.106627  [2460/4210]\n",
      "loss: 0.299457  [2470/4210]\n",
      "loss: 0.191652  [2480/4210]\n",
      "loss: 0.165258  [2490/4210]\n",
      "loss: 0.353446  [2500/4210]\n",
      "In epoch 3 _ batch 2500 testing\n",
      "Test: \n",
      " Accuracy: 80.2%, Avg loss: 0.432068 \n",
      "\n",
      "loss: 0.242044  [2510/4210]\n",
      "loss: 0.199370  [2520/4210]\n",
      "loss: 0.685539  [2530/4210]\n",
      "loss: 0.178939  [2540/4210]\n",
      "loss: 0.372209  [2550/4210]\n",
      "loss: 0.266270  [2560/4210]\n",
      "loss: 0.469505  [2570/4210]\n",
      "loss: 0.137498  [2580/4210]\n",
      "loss: 0.394605  [2590/4210]\n",
      "loss: 0.126498  [2600/4210]\n",
      "In epoch 3 _ batch 2600 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.415907 \n",
      "\n",
      "loss: 0.127534  [2610/4210]\n",
      "loss: 0.458812  [2620/4210]\n",
      "loss: 0.384120  [2630/4210]\n",
      "loss: 0.215989  [2640/4210]\n",
      "loss: 0.413813  [2650/4210]\n",
      "loss: 0.479862  [2660/4210]\n",
      "loss: 0.300316  [2670/4210]\n",
      "loss: 0.590463  [2680/4210]\n",
      "loss: 0.509334  [2690/4210]\n",
      "loss: 0.352508  [2700/4210]\n",
      "In epoch 3 _ batch 2700 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.404842 \n",
      "\n",
      "loss: 0.212318  [2710/4210]\n",
      "loss: 0.237351  [2720/4210]\n",
      "loss: 0.436036  [2730/4210]\n",
      "loss: 0.253960  [2740/4210]\n",
      "loss: 0.178867  [2750/4210]\n",
      "loss: 0.262493  [2760/4210]\n",
      "loss: 0.142568  [2770/4210]\n",
      "loss: 0.530249  [2780/4210]\n",
      "loss: 0.219607  [2790/4210]\n",
      "loss: 0.240170  [2800/4210]\n",
      "In epoch 3 _ batch 2800 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.437599 \n",
      "\n",
      "loss: 0.351139  [2810/4210]\n",
      "loss: 0.156003  [2820/4210]\n",
      "loss: 0.228672  [2830/4210]\n",
      "loss: 0.380506  [2840/4210]\n",
      "loss: 0.134196  [2850/4210]\n",
      "loss: 0.455314  [2860/4210]\n",
      "loss: 0.132959  [2870/4210]\n",
      "loss: 0.157535  [2880/4210]\n",
      "loss: 0.241202  [2890/4210]\n",
      "loss: 0.804397  [2900/4210]\n",
      "In epoch 3 _ batch 2900 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.437846 \n",
      "\n",
      "loss: 0.191400  [2910/4210]\n",
      "loss: 0.634580  [2920/4210]\n",
      "loss: 0.145139  [2930/4210]\n",
      "loss: 0.313007  [2940/4210]\n",
      "loss: 0.310460  [2950/4210]\n",
      "loss: 0.209283  [2960/4210]\n",
      "loss: 0.465658  [2970/4210]\n",
      "loss: 0.381686  [2980/4210]\n",
      "loss: 0.141444  [2990/4210]\n",
      "loss: 0.201858  [3000/4210]\n",
      "In epoch 3 _ batch 3000 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.445447 \n",
      "\n",
      "loss: 0.456574  [3010/4210]\n",
      "loss: 0.079987  [3020/4210]\n",
      "loss: 0.513256  [3030/4210]\n",
      "loss: 0.972212  [3040/4210]\n",
      "loss: 0.268653  [3050/4210]\n",
      "loss: 0.266372  [3060/4210]\n",
      "loss: 0.270910  [3070/4210]\n",
      "loss: 0.343490  [3080/4210]\n",
      "loss: 0.731316  [3090/4210]\n",
      "loss: 0.271389  [3100/4210]\n",
      "In epoch 3 _ batch 3100 testing\n",
      "Test: \n",
      " Accuracy: 80.2%, Avg loss: 0.440870 \n",
      "\n",
      "loss: 0.321530  [3110/4210]\n",
      "loss: 0.297496  [3120/4210]\n",
      "loss: 0.335300  [3130/4210]\n",
      "loss: 0.265970  [3140/4210]\n",
      "loss: 0.507694  [3150/4210]\n",
      "loss: 0.380022  [3160/4210]\n",
      "loss: 0.432776  [3170/4210]\n",
      "loss: 0.342774  [3180/4210]\n",
      "loss: 0.395534  [3190/4210]\n",
      "loss: 0.189329  [3200/4210]\n",
      "In epoch 3 _ batch 3200 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.422482 \n",
      "\n",
      "loss: 0.216601  [3210/4210]\n",
      "loss: 0.316433  [3220/4210]\n",
      "loss: 0.137082  [3230/4210]\n",
      "loss: 0.267565  [3240/4210]\n",
      "loss: 0.205586  [3250/4210]\n",
      "loss: 0.155866  [3260/4210]\n",
      "loss: 0.274201  [3270/4210]\n",
      "loss: 0.202403  [3280/4210]\n",
      "loss: 0.298299  [3290/4210]\n",
      "loss: 0.127022  [3300/4210]\n",
      "In epoch 3 _ batch 3300 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.432586 \n",
      "\n",
      "loss: 0.266253  [3310/4210]\n",
      "loss: 0.307973  [3320/4210]\n",
      "loss: 0.254553  [3330/4210]\n",
      "loss: 0.691122  [3340/4210]\n",
      "loss: 0.378865  [3350/4210]\n",
      "loss: 0.255509  [3360/4210]\n",
      "loss: 0.092749  [3370/4210]\n",
      "loss: 0.316829  [3380/4210]\n",
      "loss: 0.425515  [3390/4210]\n",
      "loss: 0.237324  [3400/4210]\n",
      "In epoch 3 _ batch 3400 testing\n",
      "Test: \n",
      " Accuracy: 82.6%, Avg loss: 0.432984 \n",
      "\n",
      "loss: 0.198780  [3410/4210]\n",
      "loss: 0.266657  [3420/4210]\n",
      "loss: 0.579891  [3430/4210]\n",
      "loss: 0.363251  [3440/4210]\n",
      "loss: 0.410070  [3450/4210]\n",
      "loss: 0.248418  [3460/4210]\n",
      "loss: 0.343288  [3470/4210]\n",
      "loss: 0.223974  [3480/4210]\n",
      "loss: 0.301231  [3490/4210]\n",
      "loss: 0.236647  [3500/4210]\n",
      "In epoch 3 _ batch 3500 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.415528 \n",
      "\n",
      "loss: 0.471255  [3510/4210]\n",
      "loss: 0.582017  [3520/4210]\n",
      "loss: 0.481817  [3530/4210]\n",
      "loss: 0.100976  [3540/4210]\n",
      "loss: 0.460374  [3550/4210]\n",
      "loss: 0.366169  [3560/4210]\n",
      "loss: 0.149340  [3570/4210]\n",
      "loss: 0.268847  [3580/4210]\n",
      "loss: 0.120222  [3590/4210]\n",
      "loss: 0.198672  [3600/4210]\n",
      "In epoch 3 _ batch 3600 testing\n",
      "Test: \n",
      " Accuracy: 76.3%, Avg loss: 0.520226 \n",
      "\n",
      "loss: 0.261351  [3610/4210]\n",
      "loss: 0.297553  [3620/4210]\n",
      "loss: 0.145527  [3630/4210]\n",
      "loss: 0.422068  [3640/4210]\n",
      "loss: 0.247187  [3650/4210]\n",
      "loss: 0.214398  [3660/4210]\n",
      "loss: 0.239401  [3670/4210]\n",
      "loss: 0.367380  [3680/4210]\n",
      "loss: 0.182795  [3690/4210]\n",
      "loss: 0.294320  [3700/4210]\n",
      "In epoch 3 _ batch 3700 testing\n",
      "Test: \n",
      " Accuracy: 80.6%, Avg loss: 0.436872 \n",
      "\n",
      "loss: 0.223169  [3710/4210]\n",
      "loss: 0.224091  [3720/4210]\n",
      "loss: 0.663691  [3730/4210]\n",
      "loss: 0.152926  [3740/4210]\n",
      "loss: 0.238142  [3750/4210]\n",
      "loss: 0.197262  [3760/4210]\n",
      "loss: 0.117255  [3770/4210]\n",
      "loss: 0.577971  [3780/4210]\n",
      "loss: 0.246772  [3790/4210]\n",
      "loss: 0.201115  [3800/4210]\n",
      "In epoch 3 _ batch 3800 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.424895 \n",
      "\n",
      "loss: 0.349115  [3810/4210]\n",
      "loss: 0.299437  [3820/4210]\n",
      "loss: 0.210475  [3830/4210]\n",
      "loss: 0.299679  [3840/4210]\n",
      "loss: 0.439670  [3850/4210]\n",
      "loss: 0.493102  [3860/4210]\n",
      "loss: 0.211672  [3870/4210]\n",
      "loss: 0.298714  [3880/4210]\n",
      "loss: 0.120954  [3890/4210]\n",
      "loss: 0.383718  [3900/4210]\n",
      "In epoch 3 _ batch 3900 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.465846 \n",
      "\n",
      "loss: 0.623366  [3910/4210]\n",
      "loss: 0.458634  [3920/4210]\n",
      "loss: 0.331620  [3930/4210]\n",
      "loss: 0.607987  [3940/4210]\n",
      "loss: 0.220199  [3950/4210]\n",
      "loss: 0.118266  [3960/4210]\n",
      "loss: 0.149111  [3970/4210]\n",
      "loss: 0.182232  [3980/4210]\n",
      "loss: 0.282187  [3990/4210]\n",
      "loss: 0.154604  [4000/4210]\n",
      "In epoch 3 _ batch 4000 testing\n",
      "Test: \n",
      " Accuracy: 80.7%, Avg loss: 0.462111 \n",
      "\n",
      "loss: 0.223801  [4010/4210]\n",
      "loss: 0.284071  [4020/4210]\n",
      "loss: 0.252555  [4030/4210]\n",
      "loss: 0.124213  [4040/4210]\n",
      "loss: 0.426363  [4050/4210]\n",
      "loss: 0.246372  [4060/4210]\n",
      "loss: 0.175020  [4070/4210]\n",
      "loss: 0.510980  [4080/4210]\n",
      "loss: 0.197012  [4090/4210]\n",
      "loss: 0.217113  [4100/4210]\n",
      "In epoch 3 _ batch 4100 testing\n",
      "Test: \n",
      " Accuracy: 82.3%, Avg loss: 0.418954 \n",
      "\n",
      "loss: 0.261418  [4110/4210]\n",
      "loss: 0.498892  [4120/4210]\n",
      "loss: 0.274588  [4130/4210]\n",
      "loss: 0.136092  [4140/4210]\n",
      "loss: 0.175244  [4150/4210]\n",
      "loss: 0.246451  [4160/4210]\n",
      "loss: 0.462132  [4170/4210]\n",
      "loss: 0.765686  [4180/4210]\n",
      "loss: 0.186744  [4190/4210]\n",
      "loss: 0.237623  [4200/4210]\n",
      "In epoch 3 _ batch 4200 testing\n",
      "Test: \n",
      " Accuracy: 80.2%, Avg loss: 0.432665 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.366553  [  0/4210]\n",
      "In epoch 4 _ batch 0 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.420808 \n",
      "\n",
      "loss: 0.340331  [ 10/4210]\n",
      "loss: 0.121922  [ 20/4210]\n",
      "loss: 0.351488  [ 30/4210]\n",
      "loss: 0.244280  [ 40/4210]\n",
      "loss: 0.112934  [ 50/4210]\n",
      "loss: 0.299429  [ 60/4210]\n",
      "loss: 0.171662  [ 70/4210]\n",
      "loss: 0.083982  [ 80/4210]\n",
      "loss: 0.279855  [ 90/4210]\n",
      "loss: 0.407142  [100/4210]\n",
      "In epoch 4 _ batch 100 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.464893 \n",
      "\n",
      "loss: 0.117059  [110/4210]\n",
      "loss: 0.260750  [120/4210]\n",
      "loss: 0.231341  [130/4210]\n",
      "loss: 0.186187  [140/4210]\n",
      "loss: 0.147346  [150/4210]\n",
      "loss: 0.218004  [160/4210]\n",
      "loss: 0.314798  [170/4210]\n",
      "loss: 0.284837  [180/4210]\n",
      "loss: 0.169952  [190/4210]\n",
      "loss: 0.241560  [200/4210]\n",
      "In epoch 4 _ batch 200 testing\n",
      "Test: \n",
      " Accuracy: 82.5%, Avg loss: 0.422005 \n",
      "\n",
      "loss: 0.178924  [210/4210]\n",
      "loss: 0.517474  [220/4210]\n",
      "loss: 0.245212  [230/4210]\n",
      "loss: 0.241141  [240/4210]\n",
      "loss: 0.058653  [250/4210]\n",
      "loss: 0.218867  [260/4210]\n",
      "loss: 0.208934  [270/4210]\n",
      "loss: 0.771767  [280/4210]\n",
      "loss: 0.186396  [290/4210]\n",
      "loss: 0.191007  [300/4210]\n",
      "In epoch 4 _ batch 300 testing\n",
      "Test: \n",
      " Accuracy: 82.8%, Avg loss: 0.441417 \n",
      "\n",
      "loss: 0.503144  [310/4210]\n",
      "loss: 0.120636  [320/4210]\n",
      "loss: 0.074379  [330/4210]\n",
      "loss: 0.395539  [340/4210]\n",
      "loss: 0.283356  [350/4210]\n",
      "loss: 0.533227  [360/4210]\n",
      "loss: 0.360511  [370/4210]\n",
      "loss: 0.302452  [380/4210]\n",
      "loss: 0.274322  [390/4210]\n",
      "loss: 0.201285  [400/4210]\n",
      "In epoch 4 _ batch 400 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.452667 \n",
      "\n",
      "loss: 0.127040  [410/4210]\n",
      "loss: 0.376813  [420/4210]\n",
      "loss: 0.250798  [430/4210]\n",
      "loss: 0.287129  [440/4210]\n",
      "loss: 0.414762  [450/4210]\n",
      "loss: 0.656290  [460/4210]\n",
      "loss: 0.564937  [470/4210]\n",
      "loss: 0.175711  [480/4210]\n",
      "loss: 0.118745  [490/4210]\n",
      "loss: 0.344419  [500/4210]\n",
      "In epoch 4 _ batch 500 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.466064 \n",
      "\n",
      "loss: 0.382786  [510/4210]\n",
      "loss: 0.309905  [520/4210]\n",
      "loss: 0.194841  [530/4210]\n",
      "loss: 0.332664  [540/4210]\n",
      "loss: 0.484242  [550/4210]\n",
      "loss: 0.270770  [560/4210]\n",
      "loss: 0.229263  [570/4210]\n",
      "loss: 0.115087  [580/4210]\n",
      "loss: 0.323687  [590/4210]\n",
      "loss: 0.256811  [600/4210]\n",
      "In epoch 4 _ batch 600 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.467554 \n",
      "\n",
      "loss: 0.272056  [610/4210]\n",
      "loss: 0.440944  [620/4210]\n",
      "loss: 0.438332  [630/4210]\n",
      "loss: 0.130617  [640/4210]\n",
      "loss: 0.272197  [650/4210]\n",
      "loss: 0.160832  [660/4210]\n",
      "loss: 0.344410  [670/4210]\n",
      "loss: 0.395823  [680/4210]\n",
      "loss: 0.218741  [690/4210]\n",
      "loss: 0.306793  [700/4210]\n",
      "In epoch 4 _ batch 700 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.439983 \n",
      "\n",
      "loss: 0.498846  [710/4210]\n",
      "loss: 0.211916  [720/4210]\n",
      "loss: 0.607227  [730/4210]\n",
      "loss: 0.449360  [740/4210]\n",
      "loss: 0.362099  [750/4210]\n",
      "loss: 0.145904  [760/4210]\n",
      "loss: 0.136655  [770/4210]\n",
      "loss: 0.136654  [780/4210]\n",
      "loss: 0.290873  [790/4210]\n",
      "loss: 0.150980  [800/4210]\n",
      "In epoch 4 _ batch 800 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.434971 \n",
      "\n",
      "loss: 0.203995  [810/4210]\n",
      "loss: 0.213349  [820/4210]\n",
      "loss: 0.297695  [830/4210]\n",
      "loss: 0.246992  [840/4210]\n",
      "loss: 0.163616  [850/4210]\n",
      "loss: 0.406872  [860/4210]\n",
      "loss: 0.261363  [870/4210]\n",
      "loss: 0.140553  [880/4210]\n",
      "loss: 0.286215  [890/4210]\n",
      "loss: 0.180136  [900/4210]\n",
      "In epoch 4 _ batch 900 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.462457 \n",
      "\n",
      "loss: 0.120070  [910/4210]\n",
      "loss: 0.453030  [920/4210]\n",
      "loss: 0.359892  [930/4210]\n",
      "loss: 0.218475  [940/4210]\n",
      "loss: 0.627274  [950/4210]\n",
      "loss: 0.223854  [960/4210]\n",
      "loss: 0.395558  [970/4210]\n",
      "loss: 0.431256  [980/4210]\n",
      "loss: 0.226792  [990/4210]\n",
      "loss: 0.341230  [1000/4210]\n",
      "In epoch 4 _ batch 1000 testing\n",
      "Test: \n",
      " Accuracy: 80.3%, Avg loss: 0.446666 \n",
      "\n",
      "loss: 0.518424  [1010/4210]\n",
      "loss: 0.184853  [1020/4210]\n",
      "loss: 0.157586  [1030/4210]\n",
      "loss: 0.280675  [1040/4210]\n",
      "loss: 0.175258  [1050/4210]\n",
      "loss: 0.126213  [1060/4210]\n",
      "loss: 0.217521  [1070/4210]\n",
      "loss: 0.194162  [1080/4210]\n",
      "loss: 0.216354  [1090/4210]\n",
      "loss: 0.332705  [1100/4210]\n",
      "In epoch 4 _ batch 1100 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.429495 \n",
      "\n",
      "loss: 0.335404  [1110/4210]\n",
      "loss: 0.228229  [1120/4210]\n",
      "loss: 0.133325  [1130/4210]\n",
      "loss: 0.085932  [1140/4210]\n",
      "loss: 0.339273  [1150/4210]\n",
      "loss: 0.225332  [1160/4210]\n",
      "loss: 0.615766  [1170/4210]\n",
      "loss: 0.224993  [1180/4210]\n",
      "loss: 0.173242  [1190/4210]\n",
      "loss: 0.169983  [1200/4210]\n",
      "In epoch 4 _ batch 1200 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.410486 \n",
      "\n",
      "loss: 0.240357  [1210/4210]\n",
      "loss: 0.290180  [1220/4210]\n",
      "loss: 0.063605  [1230/4210]\n",
      "loss: 0.349840  [1240/4210]\n",
      "loss: 0.213458  [1250/4210]\n",
      "loss: 0.241141  [1260/4210]\n",
      "loss: 0.329088  [1270/4210]\n",
      "loss: 0.408647  [1280/4210]\n",
      "loss: 0.148894  [1290/4210]\n",
      "loss: 0.439259  [1300/4210]\n",
      "In epoch 4 _ batch 1300 testing\n",
      "Test: \n",
      " Accuracy: 82.5%, Avg loss: 0.407568 \n",
      "\n",
      "loss: 0.543553  [1310/4210]\n",
      "loss: 0.084940  [1320/4210]\n",
      "loss: 0.243930  [1330/4210]\n",
      "loss: 0.078381  [1340/4210]\n",
      "loss: 0.077426  [1350/4210]\n",
      "loss: 0.565513  [1360/4210]\n",
      "loss: 0.429030  [1370/4210]\n",
      "loss: 0.171488  [1380/4210]\n",
      "loss: 0.131044  [1390/4210]\n",
      "loss: 0.136872  [1400/4210]\n",
      "In epoch 4 _ batch 1400 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.416398 \n",
      "\n",
      "loss: 0.134533  [1410/4210]\n",
      "loss: 0.236210  [1420/4210]\n",
      "loss: 0.241484  [1430/4210]\n",
      "loss: 0.427539  [1440/4210]\n",
      "loss: 0.289337  [1450/4210]\n",
      "loss: 0.284539  [1460/4210]\n",
      "loss: 0.101364  [1470/4210]\n",
      "loss: 0.233168  [1480/4210]\n",
      "loss: 0.655933  [1490/4210]\n",
      "loss: 0.253506  [1500/4210]\n",
      "In epoch 4 _ batch 1500 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.450404 \n",
      "\n",
      "loss: 0.082318  [1510/4210]\n",
      "loss: 0.966387  [1520/4210]\n",
      "loss: 0.369764  [1530/4210]\n",
      "loss: 0.231496  [1540/4210]\n",
      "loss: 0.190807  [1550/4210]\n",
      "loss: 0.243353  [1560/4210]\n",
      "loss: 0.068239  [1570/4210]\n",
      "loss: 0.294027  [1580/4210]\n",
      "loss: 0.128868  [1590/4210]\n",
      "loss: 0.415712  [1600/4210]\n",
      "In epoch 4 _ batch 1600 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.479438 \n",
      "\n",
      "loss: 0.159016  [1610/4210]\n",
      "loss: 0.229403  [1620/4210]\n",
      "loss: 0.460992  [1630/4210]\n",
      "loss: 0.781678  [1640/4210]\n",
      "loss: 0.322363  [1650/4210]\n",
      "loss: 0.304989  [1660/4210]\n",
      "loss: 0.172049  [1670/4210]\n",
      "loss: 0.101347  [1680/4210]\n",
      "loss: 0.084621  [1690/4210]\n",
      "loss: 0.560426  [1700/4210]\n",
      "In epoch 4 _ batch 1700 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.483714 \n",
      "\n",
      "loss: 0.338093  [1710/4210]\n",
      "loss: 0.199148  [1720/4210]\n",
      "loss: 0.327709  [1730/4210]\n",
      "loss: 0.336440  [1740/4210]\n",
      "loss: 0.073714  [1750/4210]\n",
      "loss: 0.184162  [1760/4210]\n",
      "loss: 0.422390  [1770/4210]\n",
      "loss: 0.246916  [1780/4210]\n",
      "loss: 0.081766  [1790/4210]\n",
      "loss: 0.367872  [1800/4210]\n",
      "In epoch 4 _ batch 1800 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.467649 \n",
      "\n",
      "loss: 0.164062  [1810/4210]\n",
      "loss: 0.455132  [1820/4210]\n",
      "loss: 0.390005  [1830/4210]\n",
      "loss: 0.153002  [1840/4210]\n",
      "loss: 0.088704  [1850/4210]\n",
      "loss: 0.674313  [1860/4210]\n",
      "loss: 0.092532  [1870/4210]\n",
      "loss: 0.347235  [1880/4210]\n",
      "loss: 0.154212  [1890/4210]\n",
      "loss: 0.189352  [1900/4210]\n",
      "In epoch 4 _ batch 1900 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.496096 \n",
      "\n",
      "loss: 0.347243  [1910/4210]\n",
      "loss: 0.516147  [1920/4210]\n",
      "loss: 0.120622  [1930/4210]\n",
      "loss: 0.386424  [1940/4210]\n",
      "loss: 0.109248  [1950/4210]\n",
      "loss: 0.072183  [1960/4210]\n",
      "loss: 0.264921  [1970/4210]\n",
      "loss: 0.737980  [1980/4210]\n",
      "loss: 0.227630  [1990/4210]\n",
      "loss: 0.212207  [2000/4210]\n",
      "In epoch 4 _ batch 2000 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.467373 \n",
      "\n",
      "loss: 0.277695  [2010/4210]\n",
      "loss: 0.220291  [2020/4210]\n",
      "loss: 0.063635  [2030/4210]\n",
      "loss: 0.272661  [2040/4210]\n",
      "loss: 0.111172  [2050/4210]\n",
      "loss: 0.088132  [2060/4210]\n",
      "loss: 0.171522  [2070/4210]\n",
      "loss: 0.233664  [2080/4210]\n",
      "loss: 0.119972  [2090/4210]\n",
      "loss: 0.227212  [2100/4210]\n",
      "In epoch 4 _ batch 2100 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.488787 \n",
      "\n",
      "loss: 0.190815  [2110/4210]\n",
      "loss: 0.265233  [2120/4210]\n",
      "loss: 0.565574  [2130/4210]\n",
      "loss: 0.205470  [2140/4210]\n",
      "loss: 0.164985  [2150/4210]\n",
      "loss: 0.898655  [2160/4210]\n",
      "loss: 0.282911  [2170/4210]\n",
      "loss: 0.247153  [2180/4210]\n",
      "loss: 0.087051  [2190/4210]\n",
      "loss: 0.098213  [2200/4210]\n",
      "In epoch 4 _ batch 2200 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.451934 \n",
      "\n",
      "loss: 0.478955  [2210/4210]\n",
      "loss: 0.273688  [2220/4210]\n",
      "loss: 0.332737  [2230/4210]\n",
      "loss: 0.037041  [2240/4210]\n",
      "loss: 0.233155  [2250/4210]\n",
      "loss: 0.042214  [2260/4210]\n",
      "loss: 0.148944  [2270/4210]\n",
      "loss: 0.253825  [2280/4210]\n",
      "loss: 0.234372  [2290/4210]\n",
      "loss: 0.111241  [2300/4210]\n",
      "In epoch 4 _ batch 2300 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.468511 \n",
      "\n",
      "loss: 0.096296  [2310/4210]\n",
      "loss: 0.156500  [2320/4210]\n",
      "loss: 0.238124  [2330/4210]\n",
      "loss: 0.443485  [2340/4210]\n",
      "loss: 0.367165  [2350/4210]\n",
      "loss: 0.143020  [2360/4210]\n",
      "loss: 0.180719  [2370/4210]\n",
      "loss: 0.162228  [2380/4210]\n",
      "loss: 0.139674  [2390/4210]\n",
      "loss: 0.255505  [2400/4210]\n",
      "In epoch 4 _ batch 2400 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.446462 \n",
      "\n",
      "loss: 0.105414  [2410/4210]\n",
      "loss: 0.211328  [2420/4210]\n",
      "loss: 0.659267  [2430/4210]\n",
      "loss: 0.346220  [2440/4210]\n",
      "loss: 0.190216  [2450/4210]\n",
      "loss: 0.695345  [2460/4210]\n",
      "loss: 0.120086  [2470/4210]\n",
      "loss: 0.247662  [2480/4210]\n",
      "loss: 0.181462  [2490/4210]\n",
      "loss: 0.223294  [2500/4210]\n",
      "In epoch 4 _ batch 2500 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.448208 \n",
      "\n",
      "loss: 0.420489  [2510/4210]\n",
      "loss: 0.114645  [2520/4210]\n",
      "loss: 0.161569  [2530/4210]\n",
      "loss: 0.374716  [2540/4210]\n",
      "loss: 0.415252  [2550/4210]\n",
      "loss: 0.260306  [2560/4210]\n",
      "loss: 0.162614  [2570/4210]\n",
      "loss: 0.436075  [2580/4210]\n",
      "loss: 0.125515  [2590/4210]\n",
      "loss: 0.449203  [2600/4210]\n",
      "In epoch 4 _ batch 2600 testing\n",
      "Test: \n",
      " Accuracy: 82.9%, Avg loss: 0.438821 \n",
      "\n",
      "loss: 0.149914  [2610/4210]\n",
      "loss: 0.154370  [2620/4210]\n",
      "loss: 0.269356  [2630/4210]\n",
      "loss: 0.483008  [2640/4210]\n",
      "loss: 0.155793  [2650/4210]\n",
      "loss: 0.608864  [2660/4210]\n",
      "loss: 0.120906  [2670/4210]\n",
      "loss: 0.197832  [2680/4210]\n",
      "loss: 0.391485  [2690/4210]\n",
      "loss: 0.193719  [2700/4210]\n",
      "In epoch 4 _ batch 2700 testing\n",
      "Test: \n",
      " Accuracy: 82.6%, Avg loss: 0.441367 \n",
      "\n",
      "loss: 0.149471  [2710/4210]\n",
      "loss: 0.515432  [2720/4210]\n",
      "loss: 0.166392  [2730/4210]\n",
      "loss: 0.342658  [2740/4210]\n",
      "loss: 0.267876  [2750/4210]\n",
      "loss: 0.246657  [2760/4210]\n",
      "loss: 0.134732  [2770/4210]\n",
      "loss: 0.096604  [2780/4210]\n",
      "loss: 0.141652  [2790/4210]\n",
      "loss: 0.179487  [2800/4210]\n",
      "In epoch 4 _ batch 2800 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.426195 \n",
      "\n",
      "loss: 0.092063  [2810/4210]\n",
      "loss: 0.122204  [2820/4210]\n",
      "loss: 0.318281  [2830/4210]\n",
      "loss: 0.317874  [2840/4210]\n",
      "loss: 0.067528  [2850/4210]\n",
      "loss: 0.339145  [2860/4210]\n",
      "loss: 0.388320  [2870/4210]\n",
      "loss: 0.076240  [2880/4210]\n",
      "loss: 0.109321  [2890/4210]\n",
      "loss: 0.202626  [2900/4210]\n",
      "In epoch 4 _ batch 2900 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.455237 \n",
      "\n",
      "loss: 0.278879  [2910/4210]\n",
      "loss: 0.089013  [2920/4210]\n",
      "loss: 0.508506  [2930/4210]\n",
      "loss: 0.128200  [2940/4210]\n",
      "loss: 0.092298  [2950/4210]\n",
      "loss: 0.171458  [2960/4210]\n",
      "loss: 0.730851  [2970/4210]\n",
      "loss: 0.213167  [2980/4210]\n",
      "loss: 0.228355  [2990/4210]\n",
      "loss: 0.203034  [3000/4210]\n",
      "In epoch 4 _ batch 3000 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.434227 \n",
      "\n",
      "loss: 0.080863  [3010/4210]\n",
      "loss: 0.162754  [3020/4210]\n",
      "loss: 0.181220  [3030/4210]\n",
      "loss: 0.097901  [3040/4210]\n",
      "loss: 0.223675  [3050/4210]\n",
      "loss: 0.089299  [3060/4210]\n",
      "loss: 0.327779  [3070/4210]\n",
      "loss: 0.387358  [3080/4210]\n",
      "loss: 0.174948  [3090/4210]\n",
      "loss: 0.205616  [3100/4210]\n",
      "In epoch 4 _ batch 3100 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.453192 \n",
      "\n",
      "loss: 0.194788  [3110/4210]\n",
      "loss: 0.283942  [3120/4210]\n",
      "loss: 0.196533  [3130/4210]\n",
      "loss: 0.174003  [3140/4210]\n",
      "loss: 0.371591  [3150/4210]\n",
      "loss: 0.147309  [3160/4210]\n",
      "loss: 0.620854  [3170/4210]\n",
      "loss: 0.178988  [3180/4210]\n",
      "loss: 0.210124  [3190/4210]\n",
      "loss: 0.124811  [3200/4210]\n",
      "In epoch 4 _ batch 3200 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.462937 \n",
      "\n",
      "loss: 0.867601  [3210/4210]\n",
      "loss: 0.142064  [3220/4210]\n",
      "loss: 0.381277  [3230/4210]\n",
      "loss: 0.413350  [3240/4210]\n",
      "loss: 0.039206  [3250/4210]\n",
      "loss: 0.062270  [3260/4210]\n",
      "loss: 0.157148  [3270/4210]\n",
      "loss: 0.205203  [3280/4210]\n",
      "loss: 0.395522  [3290/4210]\n",
      "loss: 0.187333  [3300/4210]\n",
      "In epoch 4 _ batch 3300 testing\n",
      "Test: \n",
      " Accuracy: 80.6%, Avg loss: 0.495518 \n",
      "\n",
      "loss: 0.376178  [3310/4210]\n",
      "loss: 0.107606  [3320/4210]\n",
      "loss: 0.307018  [3330/4210]\n",
      "loss: 0.276982  [3340/4210]\n",
      "loss: 0.047227  [3350/4210]\n",
      "loss: 0.235751  [3360/4210]\n",
      "loss: 0.097305  [3370/4210]\n",
      "loss: 0.170217  [3380/4210]\n",
      "loss: 0.116494  [3390/4210]\n",
      "loss: 0.458726  [3400/4210]\n",
      "In epoch 4 _ batch 3400 testing\n",
      "Test: \n",
      " Accuracy: 80.3%, Avg loss: 0.495745 \n",
      "\n",
      "loss: 0.088126  [3410/4210]\n",
      "loss: 0.540419  [3420/4210]\n",
      "loss: 0.565313  [3430/4210]\n",
      "loss: 0.070233  [3440/4210]\n",
      "loss: 0.354866  [3450/4210]\n",
      "loss: 0.136949  [3460/4210]\n",
      "loss: 0.545182  [3470/4210]\n",
      "loss: 0.224591  [3480/4210]\n",
      "loss: 0.061138  [3490/4210]\n",
      "loss: 0.326740  [3500/4210]\n",
      "In epoch 4 _ batch 3500 testing\n",
      "Test: \n",
      " Accuracy: 80.2%, Avg loss: 0.464198 \n",
      "\n",
      "loss: 0.255025  [3510/4210]\n",
      "loss: 0.665478  [3520/4210]\n",
      "loss: 0.394397  [3530/4210]\n",
      "loss: 0.250859  [3540/4210]\n",
      "loss: 0.136300  [3550/4210]\n",
      "loss: 0.448698  [3560/4210]\n",
      "loss: 0.267078  [3570/4210]\n",
      "loss: 0.252210  [3580/4210]\n",
      "loss: 0.319087  [3590/4210]\n",
      "loss: 0.446831  [3600/4210]\n",
      "In epoch 4 _ batch 3600 testing\n",
      "Test: \n",
      " Accuracy: 80.2%, Avg loss: 0.436486 \n",
      "\n",
      "loss: 0.148576  [3610/4210]\n",
      "loss: 0.341462  [3620/4210]\n",
      "loss: 0.243398  [3630/4210]\n",
      "loss: 0.115763  [3640/4210]\n",
      "loss: 0.408937  [3650/4210]\n",
      "loss: 0.583064  [3660/4210]\n",
      "loss: 0.229523  [3670/4210]\n",
      "loss: 0.152075  [3680/4210]\n",
      "loss: 0.133835  [3690/4210]\n",
      "loss: 0.377073  [3700/4210]\n",
      "In epoch 4 _ batch 3700 testing\n",
      "Test: \n",
      " Accuracy: 80.5%, Avg loss: 0.452449 \n",
      "\n",
      "loss: 0.138095  [3710/4210]\n",
      "loss: 0.277915  [3720/4210]\n",
      "loss: 0.137788  [3730/4210]\n",
      "loss: 0.272605  [3740/4210]\n",
      "loss: 0.389584  [3750/4210]\n",
      "loss: 0.098478  [3760/4210]\n",
      "loss: 0.189104  [3770/4210]\n",
      "loss: 0.172750  [3780/4210]\n",
      "loss: 0.139458  [3790/4210]\n",
      "loss: 0.120152  [3800/4210]\n",
      "In epoch 4 _ batch 3800 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.451006 \n",
      "\n",
      "loss: 0.320605  [3810/4210]\n",
      "loss: 0.134390  [3820/4210]\n",
      "loss: 0.278748  [3830/4210]\n",
      "loss: 0.274489  [3840/4210]\n",
      "loss: 0.151476  [3850/4210]\n",
      "loss: 0.225049  [3860/4210]\n",
      "loss: 0.136150  [3870/4210]\n",
      "loss: 0.185733  [3880/4210]\n",
      "loss: 0.520239  [3890/4210]\n",
      "loss: 0.225969  [3900/4210]\n",
      "In epoch 4 _ batch 3900 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.430829 \n",
      "\n",
      "loss: 0.344318  [3910/4210]\n",
      "loss: 0.303053  [3920/4210]\n",
      "loss: 0.514381  [3930/4210]\n",
      "loss: 0.155988  [3940/4210]\n",
      "loss: 0.370409  [3950/4210]\n",
      "loss: 0.269839  [3960/4210]\n",
      "loss: 0.214931  [3970/4210]\n",
      "loss: 0.096885  [3980/4210]\n",
      "loss: 0.397894  [3990/4210]\n",
      "loss: 0.129223  [4000/4210]\n",
      "In epoch 4 _ batch 4000 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.448481 \n",
      "\n",
      "loss: 0.139892  [4010/4210]\n",
      "loss: 0.536382  [4020/4210]\n",
      "loss: 0.218326  [4030/4210]\n",
      "loss: 0.321962  [4040/4210]\n",
      "loss: 0.523211  [4050/4210]\n",
      "loss: 0.078721  [4060/4210]\n",
      "loss: 0.263928  [4070/4210]\n",
      "loss: 0.416736  [4080/4210]\n",
      "loss: 0.353189  [4090/4210]\n",
      "loss: 0.098646  [4100/4210]\n",
      "In epoch 4 _ batch 4100 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.455183 \n",
      "\n",
      "loss: 0.333810  [4110/4210]\n",
      "loss: 0.386550  [4120/4210]\n",
      "loss: 0.252514  [4130/4210]\n",
      "loss: 0.154765  [4140/4210]\n",
      "loss: 0.215839  [4150/4210]\n",
      "loss: 0.175858  [4160/4210]\n",
      "loss: 0.122606  [4170/4210]\n",
      "loss: 0.269952  [4180/4210]\n",
      "loss: 0.306280  [4190/4210]\n",
      "loss: 0.292719  [4200/4210]\n",
      "In epoch 4 _ batch 4200 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.474614 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.297314  [  0/4210]\n",
      "In epoch 5 _ batch 0 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.455764 \n",
      "\n",
      "loss: 0.456507  [ 10/4210]\n",
      "loss: 0.279120  [ 20/4210]\n",
      "loss: 0.120701  [ 30/4210]\n",
      "loss: 0.094595  [ 40/4210]\n",
      "loss: 0.120980  [ 50/4210]\n",
      "loss: 0.104393  [ 60/4210]\n",
      "loss: 0.050307  [ 70/4210]\n",
      "loss: 0.191293  [ 80/4210]\n",
      "loss: 0.042424  [ 90/4210]\n",
      "loss: 0.147737  [100/4210]\n",
      "In epoch 5 _ batch 100 testing\n",
      "Test: \n",
      " Accuracy: 82.5%, Avg loss: 0.443408 \n",
      "\n",
      "loss: 0.684110  [110/4210]\n",
      "loss: 0.141008  [120/4210]\n",
      "loss: 0.362920  [130/4210]\n",
      "loss: 0.266991  [140/4210]\n",
      "loss: 0.275423  [150/4210]\n",
      "loss: 0.229527  [160/4210]\n",
      "loss: 0.160540  [170/4210]\n",
      "loss: 0.477718  [180/4210]\n",
      "loss: 0.176128  [190/4210]\n",
      "loss: 0.314107  [200/4210]\n",
      "In epoch 5 _ batch 200 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.432302 \n",
      "\n",
      "loss: 0.180069  [210/4210]\n",
      "loss: 0.300546  [220/4210]\n",
      "loss: 0.107094  [230/4210]\n",
      "loss: 0.197086  [240/4210]\n",
      "loss: 0.109806  [250/4210]\n",
      "loss: 0.125866  [260/4210]\n",
      "loss: 0.080533  [270/4210]\n",
      "loss: 0.109035  [280/4210]\n",
      "loss: 0.239051  [290/4210]\n",
      "loss: 0.294800  [300/4210]\n",
      "In epoch 5 _ batch 300 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.448445 \n",
      "\n",
      "loss: 0.177077  [310/4210]\n",
      "loss: 0.121823  [320/4210]\n",
      "loss: 0.177247  [330/4210]\n",
      "loss: 0.117605  [340/4210]\n",
      "loss: 0.219319  [350/4210]\n",
      "loss: 0.189582  [360/4210]\n",
      "loss: 0.146012  [370/4210]\n",
      "loss: 0.223263  [380/4210]\n",
      "loss: 0.377644  [390/4210]\n",
      "loss: 0.288440  [400/4210]\n",
      "In epoch 5 _ batch 400 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.451622 \n",
      "\n",
      "loss: 0.136206  [410/4210]\n",
      "loss: 0.142969  [420/4210]\n",
      "loss: 0.213903  [430/4210]\n",
      "loss: 0.138848  [440/4210]\n",
      "loss: 0.080656  [450/4210]\n",
      "loss: 0.519935  [460/4210]\n",
      "loss: 0.033281  [470/4210]\n",
      "loss: 0.150319  [480/4210]\n",
      "loss: 0.126113  [490/4210]\n",
      "loss: 0.265064  [500/4210]\n",
      "In epoch 5 _ batch 500 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.474097 \n",
      "\n",
      "loss: 0.311965  [510/4210]\n",
      "loss: 0.093984  [520/4210]\n",
      "loss: 0.133470  [530/4210]\n",
      "loss: 0.500855  [540/4210]\n",
      "loss: 0.097137  [550/4210]\n",
      "loss: 0.136172  [560/4210]\n",
      "loss: 0.594351  [570/4210]\n",
      "loss: 0.138959  [580/4210]\n",
      "loss: 0.669848  [590/4210]\n",
      "loss: 0.185073  [600/4210]\n",
      "In epoch 5 _ batch 600 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.471125 \n",
      "\n",
      "loss: 0.288922  [610/4210]\n",
      "loss: 0.116941  [620/4210]\n",
      "loss: 0.193916  [630/4210]\n",
      "loss: 0.167310  [640/4210]\n",
      "loss: 0.719318  [650/4210]\n",
      "loss: 0.477426  [660/4210]\n",
      "loss: 0.066379  [670/4210]\n",
      "loss: 0.105261  [680/4210]\n",
      "loss: 0.180092  [690/4210]\n",
      "loss: 0.195090  [700/4210]\n",
      "In epoch 5 _ batch 700 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.457213 \n",
      "\n",
      "loss: 0.275286  [710/4210]\n",
      "loss: 0.083635  [720/4210]\n",
      "loss: 0.652411  [730/4210]\n",
      "loss: 0.187905  [740/4210]\n",
      "loss: 0.144110  [750/4210]\n",
      "loss: 0.373736  [760/4210]\n",
      "loss: 0.137353  [770/4210]\n",
      "loss: 0.107660  [780/4210]\n",
      "loss: 0.264490  [790/4210]\n",
      "loss: 0.205505  [800/4210]\n",
      "In epoch 5 _ batch 800 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.470660 \n",
      "\n",
      "loss: 0.117926  [810/4210]\n",
      "loss: 0.186368  [820/4210]\n",
      "loss: 0.179877  [830/4210]\n",
      "loss: 0.330200  [840/4210]\n",
      "loss: 0.164040  [850/4210]\n",
      "loss: 0.064695  [860/4210]\n",
      "loss: 0.217396  [870/4210]\n",
      "loss: 0.304383  [880/4210]\n",
      "loss: 0.412046  [890/4210]\n",
      "loss: 0.174253  [900/4210]\n",
      "In epoch 5 _ batch 900 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.451915 \n",
      "\n",
      "loss: 0.216853  [910/4210]\n",
      "loss: 0.059526  [920/4210]\n",
      "loss: 0.214731  [930/4210]\n",
      "loss: 0.444071  [940/4210]\n",
      "loss: 0.174667  [950/4210]\n",
      "loss: 0.071624  [960/4210]\n",
      "loss: 0.354639  [970/4210]\n",
      "loss: 0.195619  [980/4210]\n",
      "loss: 0.217453  [990/4210]\n",
      "loss: 0.103272  [1000/4210]\n",
      "In epoch 5 _ batch 1000 testing\n",
      "Test: \n",
      " Accuracy: 80.3%, Avg loss: 0.482078 \n",
      "\n",
      "loss: 0.054289  [1010/4210]\n",
      "loss: 0.235529  [1020/4210]\n",
      "loss: 0.149382  [1030/4210]\n",
      "loss: 0.221279  [1040/4210]\n",
      "loss: 0.136808  [1050/4210]\n",
      "loss: 0.513526  [1060/4210]\n",
      "loss: 0.401302  [1070/4210]\n",
      "loss: 0.129874  [1080/4210]\n",
      "loss: 0.174074  [1090/4210]\n",
      "loss: 0.185805  [1100/4210]\n",
      "In epoch 5 _ batch 1100 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.440817 \n",
      "\n",
      "loss: 0.097767  [1110/4210]\n",
      "loss: 0.215879  [1120/4210]\n",
      "loss: 0.122801  [1130/4210]\n",
      "loss: 0.111207  [1140/4210]\n",
      "loss: 0.146088  [1150/4210]\n",
      "loss: 0.281641  [1160/4210]\n",
      "loss: 0.089299  [1170/4210]\n",
      "loss: 0.129143  [1180/4210]\n",
      "loss: 0.212377  [1190/4210]\n",
      "loss: 0.466916  [1200/4210]\n",
      "In epoch 5 _ batch 1200 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.467310 \n",
      "\n",
      "loss: 0.147118  [1210/4210]\n",
      "loss: 0.184616  [1220/4210]\n",
      "loss: 0.184801  [1230/4210]\n",
      "loss: 0.162176  [1240/4210]\n",
      "loss: 0.295751  [1250/4210]\n",
      "loss: 0.218387  [1260/4210]\n",
      "loss: 0.136491  [1270/4210]\n",
      "loss: 0.187763  [1280/4210]\n",
      "loss: 0.137496  [1290/4210]\n",
      "loss: 0.041271  [1300/4210]\n",
      "In epoch 5 _ batch 1300 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.503991 \n",
      "\n",
      "loss: 0.041905  [1310/4210]\n",
      "loss: 0.240955  [1320/4210]\n",
      "loss: 0.133552  [1330/4210]\n",
      "loss: 0.190361  [1340/4210]\n",
      "loss: 0.270117  [1350/4210]\n",
      "loss: 0.161408  [1360/4210]\n",
      "loss: 0.098977  [1370/4210]\n",
      "loss: 0.257110  [1380/4210]\n",
      "loss: 0.308781  [1390/4210]\n",
      "loss: 0.283646  [1400/4210]\n",
      "In epoch 5 _ batch 1400 testing\n",
      "Test: \n",
      " Accuracy: 82.3%, Avg loss: 0.456399 \n",
      "\n",
      "loss: 0.206912  [1410/4210]\n",
      "loss: 0.152164  [1420/4210]\n",
      "loss: 0.199444  [1430/4210]\n",
      "loss: 0.205214  [1440/4210]\n",
      "loss: 0.153491  [1450/4210]\n",
      "loss: 0.132564  [1460/4210]\n",
      "loss: 0.205935  [1470/4210]\n",
      "loss: 0.337622  [1480/4210]\n",
      "loss: 0.256629  [1490/4210]\n",
      "loss: 0.095982  [1500/4210]\n",
      "In epoch 5 _ batch 1500 testing\n",
      "Test: \n",
      " Accuracy: 82.3%, Avg loss: 0.460696 \n",
      "\n",
      "loss: 0.075089  [1510/4210]\n",
      "loss: 0.105489  [1520/4210]\n",
      "loss: 0.323879  [1530/4210]\n",
      "loss: 0.303651  [1540/4210]\n",
      "loss: 0.300994  [1550/4210]\n",
      "loss: 0.492102  [1560/4210]\n",
      "loss: 0.217627  [1570/4210]\n",
      "loss: 0.204657  [1580/4210]\n",
      "loss: 0.322239  [1590/4210]\n",
      "loss: 0.288715  [1600/4210]\n",
      "In epoch 5 _ batch 1600 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.444911 \n",
      "\n",
      "loss: 0.262232  [1610/4210]\n",
      "loss: 0.259613  [1620/4210]\n",
      "loss: 0.303839  [1630/4210]\n",
      "loss: 0.233714  [1640/4210]\n",
      "loss: 0.147687  [1650/4210]\n",
      "loss: 0.185356  [1660/4210]\n",
      "loss: 0.209899  [1670/4210]\n",
      "loss: 0.387943  [1680/4210]\n",
      "loss: 0.592596  [1690/4210]\n",
      "loss: 0.353012  [1700/4210]\n",
      "In epoch 5 _ batch 1700 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.466303 \n",
      "\n",
      "loss: 0.217889  [1710/4210]\n",
      "loss: 0.198873  [1720/4210]\n",
      "loss: 0.153288  [1730/4210]\n",
      "loss: 0.239702  [1740/4210]\n",
      "loss: 0.291855  [1750/4210]\n",
      "loss: 0.547758  [1760/4210]\n",
      "loss: 0.129424  [1770/4210]\n",
      "loss: 0.131181  [1780/4210]\n",
      "loss: 0.126040  [1790/4210]\n",
      "loss: 0.470982  [1800/4210]\n",
      "In epoch 5 _ batch 1800 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.520937 \n",
      "\n",
      "loss: 0.376058  [1810/4210]\n",
      "loss: 0.334648  [1820/4210]\n",
      "loss: 0.249483  [1830/4210]\n",
      "loss: 0.123597  [1840/4210]\n",
      "loss: 0.203343  [1850/4210]\n",
      "loss: 0.264196  [1860/4210]\n",
      "loss: 0.099247  [1870/4210]\n",
      "loss: 0.643841  [1880/4210]\n",
      "loss: 0.436264  [1890/4210]\n",
      "loss: 0.107889  [1900/4210]\n",
      "In epoch 5 _ batch 1900 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.474337 \n",
      "\n",
      "loss: 0.346870  [1910/4210]\n",
      "loss: 0.289606  [1920/4210]\n",
      "loss: 0.373291  [1930/4210]\n",
      "loss: 0.163342  [1940/4210]\n",
      "loss: 0.241163  [1950/4210]\n",
      "loss: 0.059546  [1960/4210]\n",
      "loss: 0.191833  [1970/4210]\n",
      "loss: 0.337965  [1980/4210]\n",
      "loss: 0.214111  [1990/4210]\n",
      "loss: 0.164346  [2000/4210]\n",
      "In epoch 5 _ batch 2000 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.478875 \n",
      "\n",
      "loss: 0.105105  [2010/4210]\n",
      "loss: 0.201505  [2020/4210]\n",
      "loss: 0.355558  [2030/4210]\n",
      "loss: 0.198909  [2040/4210]\n",
      "loss: 0.253925  [2050/4210]\n",
      "loss: 0.134868  [2060/4210]\n",
      "loss: 0.148433  [2070/4210]\n",
      "loss: 0.162281  [2080/4210]\n",
      "loss: 0.274933  [2090/4210]\n",
      "loss: 0.155713  [2100/4210]\n",
      "In epoch 5 _ batch 2100 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.476272 \n",
      "\n",
      "loss: 0.187272  [2110/4210]\n",
      "loss: 0.373179  [2120/4210]\n",
      "loss: 0.208653  [2130/4210]\n",
      "loss: 0.319269  [2140/4210]\n",
      "loss: 0.448931  [2150/4210]\n",
      "loss: 0.182651  [2160/4210]\n",
      "loss: 0.147638  [2170/4210]\n",
      "loss: 0.184443  [2180/4210]\n",
      "loss: 0.149614  [2190/4210]\n",
      "loss: 0.194839  [2200/4210]\n",
      "In epoch 5 _ batch 2200 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.445934 \n",
      "\n",
      "loss: 0.122526  [2210/4210]\n",
      "loss: 0.114318  [2220/4210]\n",
      "loss: 0.235836  [2230/4210]\n",
      "loss: 0.275736  [2240/4210]\n",
      "loss: 0.197148  [2250/4210]\n",
      "loss: 0.114423  [2260/4210]\n",
      "loss: 0.222106  [2270/4210]\n",
      "loss: 0.173929  [2280/4210]\n",
      "loss: 0.303140  [2290/4210]\n",
      "loss: 0.516743  [2300/4210]\n",
      "In epoch 5 _ batch 2300 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.478488 \n",
      "\n",
      "loss: 0.175318  [2310/4210]\n",
      "loss: 0.130207  [2320/4210]\n",
      "loss: 0.183044  [2330/4210]\n",
      "loss: 0.178289  [2340/4210]\n",
      "loss: 0.281423  [2350/4210]\n",
      "loss: 0.304979  [2360/4210]\n",
      "loss: 0.145064  [2370/4210]\n",
      "loss: 0.475811  [2380/4210]\n",
      "loss: 0.562987  [2390/4210]\n",
      "loss: 0.160776  [2400/4210]\n",
      "In epoch 5 _ batch 2400 testing\n",
      "Test: \n",
      " Accuracy: 80.0%, Avg loss: 0.471702 \n",
      "\n",
      "loss: 0.088719  [2410/4210]\n",
      "loss: 0.149841  [2420/4210]\n",
      "loss: 0.165036  [2430/4210]\n",
      "loss: 0.043752  [2440/4210]\n",
      "loss: 0.254095  [2450/4210]\n",
      "loss: 0.239006  [2460/4210]\n",
      "loss: 0.222442  [2470/4210]\n",
      "loss: 0.213556  [2480/4210]\n",
      "loss: 0.088074  [2490/4210]\n",
      "loss: 0.166616  [2500/4210]\n",
      "In epoch 5 _ batch 2500 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.442326 \n",
      "\n",
      "loss: 0.097257  [2510/4210]\n",
      "loss: 0.318958  [2520/4210]\n",
      "loss: 0.209227  [2530/4210]\n",
      "loss: 0.176532  [2540/4210]\n",
      "loss: 0.427991  [2550/4210]\n",
      "loss: 0.310210  [2560/4210]\n",
      "loss: 0.072428  [2570/4210]\n",
      "loss: 0.092804  [2580/4210]\n",
      "loss: 0.337854  [2590/4210]\n",
      "loss: 0.075668  [2600/4210]\n",
      "In epoch 5 _ batch 2600 testing\n",
      "Test: \n",
      " Accuracy: 80.3%, Avg loss: 0.457944 \n",
      "\n",
      "loss: 0.680013  [2610/4210]\n",
      "loss: 0.069476  [2620/4210]\n",
      "loss: 0.203627  [2630/4210]\n",
      "loss: 0.321871  [2640/4210]\n",
      "loss: 0.375172  [2650/4210]\n",
      "loss: 0.391821  [2660/4210]\n",
      "loss: 0.422409  [2670/4210]\n",
      "loss: 0.238023  [2680/4210]\n",
      "loss: 0.406809  [2690/4210]\n",
      "loss: 0.113241  [2700/4210]\n",
      "In epoch 5 _ batch 2700 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.455890 \n",
      "\n",
      "loss: 0.092399  [2710/4210]\n",
      "loss: 0.251149  [2720/4210]\n",
      "loss: 0.123517  [2730/4210]\n",
      "loss: 0.101031  [2740/4210]\n",
      "loss: 0.553623  [2750/4210]\n",
      "loss: 0.035682  [2760/4210]\n",
      "loss: 0.360601  [2770/4210]\n",
      "loss: 0.275729  [2780/4210]\n",
      "loss: 0.269334  [2790/4210]\n",
      "loss: 0.303002  [2800/4210]\n",
      "In epoch 5 _ batch 2800 testing\n",
      "Test: \n",
      " Accuracy: 80.5%, Avg loss: 0.490441 \n",
      "\n",
      "loss: 0.216240  [2810/4210]\n",
      "loss: 0.293738  [2820/4210]\n",
      "loss: 0.225652  [2830/4210]\n",
      "loss: 0.268463  [2840/4210]\n",
      "loss: 0.466972  [2850/4210]\n",
      "loss: 0.157463  [2860/4210]\n",
      "loss: 0.218312  [2870/4210]\n",
      "loss: 0.359532  [2880/4210]\n",
      "loss: 0.261825  [2890/4210]\n",
      "loss: 0.127346  [2900/4210]\n",
      "In epoch 5 _ batch 2900 testing\n",
      "Test: \n",
      " Accuracy: 82.6%, Avg loss: 0.426130 \n",
      "\n",
      "loss: 0.240532  [2910/4210]\n",
      "loss: 0.119474  [2920/4210]\n",
      "loss: 0.230065  [2930/4210]\n",
      "loss: 0.108373  [2940/4210]\n",
      "loss: 0.403755  [2950/4210]\n",
      "loss: 0.202639  [2960/4210]\n",
      "loss: 0.161504  [2970/4210]\n",
      "loss: 0.276421  [2980/4210]\n",
      "loss: 0.115838  [2990/4210]\n",
      "loss: 0.262091  [3000/4210]\n",
      "In epoch 5 _ batch 3000 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.464293 \n",
      "\n",
      "loss: 0.600738  [3010/4210]\n",
      "loss: 0.048192  [3020/4210]\n",
      "loss: 0.332974  [3030/4210]\n",
      "loss: 0.120288  [3040/4210]\n",
      "loss: 0.243725  [3050/4210]\n",
      "loss: 0.215173  [3060/4210]\n",
      "loss: 0.107984  [3070/4210]\n",
      "loss: 0.186660  [3080/4210]\n",
      "loss: 0.118022  [3090/4210]\n",
      "loss: 0.159347  [3100/4210]\n",
      "In epoch 5 _ batch 3100 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.490339 \n",
      "\n",
      "loss: 0.203629  [3110/4210]\n",
      "loss: 0.114479  [3120/4210]\n",
      "loss: 0.081487  [3130/4210]\n",
      "loss: 0.159987  [3140/4210]\n",
      "loss: 0.149691  [3150/4210]\n",
      "loss: 0.168091  [3160/4210]\n",
      "loss: 0.278046  [3170/4210]\n",
      "loss: 0.110238  [3180/4210]\n",
      "loss: 0.216336  [3190/4210]\n",
      "loss: 0.559243  [3200/4210]\n",
      "In epoch 5 _ batch 3200 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.494221 \n",
      "\n",
      "loss: 0.512107  [3210/4210]\n",
      "loss: 0.192047  [3220/4210]\n",
      "loss: 0.229101  [3230/4210]\n",
      "loss: 0.205432  [3240/4210]\n",
      "loss: 0.117520  [3250/4210]\n",
      "loss: 0.260023  [3260/4210]\n",
      "loss: 0.171927  [3270/4210]\n",
      "loss: 0.248630  [3280/4210]\n",
      "loss: 0.188350  [3290/4210]\n",
      "loss: 0.244127  [3300/4210]\n",
      "In epoch 5 _ batch 3300 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.473413 \n",
      "\n",
      "loss: 0.331824  [3310/4210]\n",
      "loss: 0.383726  [3320/4210]\n",
      "loss: 0.331262  [3330/4210]\n",
      "loss: 0.194731  [3340/4210]\n",
      "loss: 0.360873  [3350/4210]\n",
      "loss: 0.206571  [3360/4210]\n",
      "loss: 0.265636  [3370/4210]\n",
      "loss: 0.074370  [3380/4210]\n",
      "loss: 0.097481  [3390/4210]\n",
      "loss: 0.309565  [3400/4210]\n",
      "In epoch 5 _ batch 3400 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.512336 \n",
      "\n",
      "loss: 0.479320  [3410/4210]\n",
      "loss: 0.194121  [3420/4210]\n",
      "loss: 0.124769  [3430/4210]\n",
      "loss: 0.246707  [3440/4210]\n",
      "loss: 0.203168  [3450/4210]\n",
      "loss: 0.126651  [3460/4210]\n",
      "loss: 0.273919  [3470/4210]\n",
      "loss: 0.408509  [3480/4210]\n",
      "loss: 1.116042  [3490/4210]\n",
      "loss: 0.089429  [3500/4210]\n",
      "In epoch 5 _ batch 3500 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.501552 \n",
      "\n",
      "loss: 0.073807  [3510/4210]\n",
      "loss: 0.241987  [3520/4210]\n",
      "loss: 0.461904  [3530/4210]\n",
      "loss: 0.366396  [3540/4210]\n",
      "loss: 0.176710  [3550/4210]\n",
      "loss: 0.092118  [3560/4210]\n",
      "loss: 0.157179  [3570/4210]\n",
      "loss: 0.068769  [3580/4210]\n",
      "loss: 0.201187  [3590/4210]\n",
      "loss: 0.079306  [3600/4210]\n",
      "In epoch 5 _ batch 3600 testing\n",
      "Test: \n",
      " Accuracy: 82.6%, Avg loss: 0.453108 \n",
      "\n",
      "loss: 0.153781  [3610/4210]\n",
      "loss: 0.333760  [3620/4210]\n",
      "loss: 0.291867  [3630/4210]\n",
      "loss: 0.214202  [3640/4210]\n",
      "loss: 0.152220  [3650/4210]\n",
      "loss: 0.139224  [3660/4210]\n",
      "loss: 0.200458  [3670/4210]\n",
      "loss: 0.109570  [3680/4210]\n",
      "loss: 0.211353  [3690/4210]\n",
      "loss: 0.098946  [3700/4210]\n",
      "In epoch 5 _ batch 3700 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.457856 \n",
      "\n",
      "loss: 0.213784  [3710/4210]\n",
      "loss: 0.296902  [3720/4210]\n",
      "loss: 0.159109  [3730/4210]\n",
      "loss: 0.134514  [3740/4210]\n",
      "loss: 0.289051  [3750/4210]\n",
      "loss: 0.093159  [3760/4210]\n",
      "loss: 0.041502  [3770/4210]\n",
      "loss: 0.107468  [3780/4210]\n",
      "loss: 0.354142  [3790/4210]\n",
      "loss: 0.154312  [3800/4210]\n",
      "In epoch 5 _ batch 3800 testing\n",
      "Test: \n",
      " Accuracy: 80.7%, Avg loss: 0.518401 \n",
      "\n",
      "loss: 0.123419  [3810/4210]\n",
      "loss: 0.140150  [3820/4210]\n",
      "loss: 0.108323  [3830/4210]\n",
      "loss: 0.279067  [3840/4210]\n",
      "loss: 0.175757  [3850/4210]\n",
      "loss: 0.081445  [3860/4210]\n",
      "loss: 0.320076  [3870/4210]\n",
      "loss: 0.301785  [3880/4210]\n",
      "loss: 0.329039  [3890/4210]\n",
      "loss: 0.241064  [3900/4210]\n",
      "In epoch 5 _ batch 3900 testing\n",
      "Test: \n",
      " Accuracy: 80.6%, Avg loss: 0.456581 \n",
      "\n",
      "loss: 0.526012  [3910/4210]\n",
      "loss: 0.200330  [3920/4210]\n",
      "loss: 0.350491  [3930/4210]\n",
      "loss: 0.256200  [3940/4210]\n",
      "loss: 0.260450  [3950/4210]\n",
      "loss: 0.094734  [3960/4210]\n",
      "loss: 0.121612  [3970/4210]\n",
      "loss: 0.214038  [3980/4210]\n",
      "loss: 0.282134  [3990/4210]\n",
      "loss: 0.091488  [4000/4210]\n",
      "In epoch 5 _ batch 4000 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.460256 \n",
      "\n",
      "loss: 0.228524  [4010/4210]\n",
      "loss: 0.104086  [4020/4210]\n",
      "loss: 0.382669  [4030/4210]\n",
      "loss: 0.159819  [4040/4210]\n",
      "loss: 0.143108  [4050/4210]\n",
      "loss: 0.206476  [4060/4210]\n",
      "loss: 0.044832  [4070/4210]\n",
      "loss: 0.247683  [4080/4210]\n",
      "loss: 0.186096  [4090/4210]\n",
      "loss: 0.081945  [4100/4210]\n",
      "In epoch 5 _ batch 4100 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.457256 \n",
      "\n",
      "loss: 0.113426  [4110/4210]\n",
      "loss: 0.287588  [4120/4210]\n",
      "loss: 0.412595  [4130/4210]\n",
      "loss: 0.493850  [4140/4210]\n",
      "loss: 0.220183  [4150/4210]\n",
      "loss: 0.201687  [4160/4210]\n",
      "loss: 0.286245  [4170/4210]\n",
      "loss: 0.236610  [4180/4210]\n",
      "loss: 0.155793  [4190/4210]\n",
      "loss: 0.182050  [4200/4210]\n",
      "In epoch 5 _ batch 4200 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.469702 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.284947  [  0/4210]\n",
      "In epoch 6 _ batch 0 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.469785 \n",
      "\n",
      "loss: 0.137581  [ 10/4210]\n",
      "loss: 0.234161  [ 20/4210]\n",
      "loss: 0.154394  [ 30/4210]\n",
      "loss: 0.207968  [ 40/4210]\n",
      "loss: 0.304344  [ 50/4210]\n",
      "loss: 0.192741  [ 60/4210]\n",
      "loss: 0.156643  [ 70/4210]\n",
      "loss: 0.142676  [ 80/4210]\n",
      "loss: 0.148784  [ 90/4210]\n",
      "loss: 0.335265  [100/4210]\n",
      "In epoch 6 _ batch 100 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.467000 \n",
      "\n",
      "loss: 0.048283  [110/4210]\n",
      "loss: 0.184621  [120/4210]\n",
      "loss: 0.058568  [130/4210]\n",
      "loss: 0.347411  [140/4210]\n",
      "loss: 0.100746  [150/4210]\n",
      "loss: 0.113129  [160/4210]\n",
      "loss: 0.268498  [170/4210]\n",
      "loss: 0.169352  [180/4210]\n",
      "loss: 0.270875  [190/4210]\n",
      "loss: 0.127058  [200/4210]\n",
      "In epoch 6 _ batch 200 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.476869 \n",
      "\n",
      "loss: 0.566407  [210/4210]\n",
      "loss: 0.169246  [220/4210]\n",
      "loss: 0.090762  [230/4210]\n",
      "loss: 0.425175  [240/4210]\n",
      "loss: 0.188128  [250/4210]\n",
      "loss: 0.075021  [260/4210]\n",
      "loss: 0.129270  [270/4210]\n",
      "loss: 0.080932  [280/4210]\n",
      "loss: 0.220907  [290/4210]\n",
      "loss: 0.120685  [300/4210]\n",
      "In epoch 6 _ batch 300 testing\n",
      "Test: \n",
      " Accuracy: 80.3%, Avg loss: 0.490691 \n",
      "\n",
      "loss: 0.503489  [310/4210]\n",
      "loss: 0.238416  [320/4210]\n",
      "loss: 0.249331  [330/4210]\n",
      "loss: 0.269315  [340/4210]\n",
      "loss: 0.099009  [350/4210]\n",
      "loss: 0.596967  [360/4210]\n",
      "loss: 0.085346  [370/4210]\n",
      "loss: 0.225075  [380/4210]\n",
      "loss: 0.193732  [390/4210]\n",
      "loss: 0.054122  [400/4210]\n",
      "In epoch 6 _ batch 400 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.504580 \n",
      "\n",
      "loss: 0.094050  [410/4210]\n",
      "loss: 0.252520  [420/4210]\n",
      "loss: 0.305688  [430/4210]\n",
      "loss: 0.200235  [440/4210]\n",
      "loss: 0.098172  [450/4210]\n",
      "loss: 0.304093  [460/4210]\n",
      "loss: 0.148067  [470/4210]\n",
      "loss: 0.072837  [480/4210]\n",
      "loss: 0.423383  [490/4210]\n",
      "loss: 0.240941  [500/4210]\n",
      "In epoch 6 _ batch 500 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.504605 \n",
      "\n",
      "loss: 0.077357  [510/4210]\n",
      "loss: 0.315567  [520/4210]\n",
      "loss: 0.106611  [530/4210]\n",
      "loss: 0.138942  [540/4210]\n",
      "loss: 0.543225  [550/4210]\n",
      "loss: 0.241421  [560/4210]\n",
      "loss: 0.213499  [570/4210]\n",
      "loss: 0.279178  [580/4210]\n",
      "loss: 0.336951  [590/4210]\n",
      "loss: 0.259168  [600/4210]\n",
      "In epoch 6 _ batch 600 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.483854 \n",
      "\n",
      "loss: 0.165881  [610/4210]\n",
      "loss: 0.133030  [620/4210]\n",
      "loss: 0.069307  [630/4210]\n",
      "loss: 0.431199  [640/4210]\n",
      "loss: 0.092559  [650/4210]\n",
      "loss: 0.259467  [660/4210]\n",
      "loss: 0.235988  [670/4210]\n",
      "loss: 0.182610  [680/4210]\n",
      "loss: 0.230065  [690/4210]\n",
      "loss: 0.189391  [700/4210]\n",
      "In epoch 6 _ batch 700 testing\n",
      "Test: \n",
      " Accuracy: 80.7%, Avg loss: 0.471056 \n",
      "\n",
      "loss: 0.384737  [710/4210]\n",
      "loss: 0.173870  [720/4210]\n",
      "loss: 0.255817  [730/4210]\n",
      "loss: 0.447129  [740/4210]\n",
      "loss: 0.334368  [750/4210]\n",
      "loss: 0.139361  [760/4210]\n",
      "loss: 0.114809  [770/4210]\n",
      "loss: 0.091455  [780/4210]\n",
      "loss: 0.232279  [790/4210]\n",
      "loss: 0.032441  [800/4210]\n",
      "In epoch 6 _ batch 800 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.478618 \n",
      "\n",
      "loss: 0.154063  [810/4210]\n",
      "loss: 0.018964  [820/4210]\n",
      "loss: 0.199945  [830/4210]\n",
      "loss: 0.260409  [840/4210]\n",
      "loss: 0.163066  [850/4210]\n",
      "loss: 0.264570  [860/4210]\n",
      "loss: 0.060683  [870/4210]\n",
      "loss: 0.284257  [880/4210]\n",
      "loss: 0.127434  [890/4210]\n",
      "loss: 0.252103  [900/4210]\n",
      "In epoch 6 _ batch 900 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.487016 \n",
      "\n",
      "loss: 0.245105  [910/4210]\n",
      "loss: 0.436076  [920/4210]\n",
      "loss: 0.349648  [930/4210]\n",
      "loss: 0.114506  [940/4210]\n",
      "loss: 0.200340  [950/4210]\n",
      "loss: 0.083784  [960/4210]\n",
      "loss: 0.150927  [970/4210]\n",
      "loss: 0.206853  [980/4210]\n",
      "loss: 0.105938  [990/4210]\n",
      "loss: 0.100795  [1000/4210]\n",
      "In epoch 6 _ batch 1000 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.516725 \n",
      "\n",
      "loss: 0.053095  [1010/4210]\n",
      "loss: 0.037202  [1020/4210]\n",
      "loss: 0.172115  [1030/4210]\n",
      "loss: 0.116235  [1040/4210]\n",
      "loss: 0.069160  [1050/4210]\n",
      "loss: 0.164721  [1060/4210]\n",
      "loss: 0.195307  [1070/4210]\n",
      "loss: 0.339557  [1080/4210]\n",
      "loss: 0.045886  [1090/4210]\n",
      "loss: 0.081066  [1100/4210]\n",
      "In epoch 6 _ batch 1100 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.502455 \n",
      "\n",
      "loss: 0.340860  [1110/4210]\n",
      "loss: 0.150491  [1120/4210]\n",
      "loss: 0.173017  [1130/4210]\n",
      "loss: 0.254249  [1140/4210]\n",
      "loss: 0.104856  [1150/4210]\n",
      "loss: 0.114279  [1160/4210]\n",
      "loss: 0.223651  [1170/4210]\n",
      "loss: 0.345348  [1180/4210]\n",
      "loss: 0.166961  [1190/4210]\n",
      "loss: 0.146602  [1200/4210]\n",
      "In epoch 6 _ batch 1200 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.521136 \n",
      "\n",
      "loss: 0.027079  [1210/4210]\n",
      "loss: 0.323245  [1220/4210]\n",
      "loss: 0.135647  [1230/4210]\n",
      "loss: 0.189348  [1240/4210]\n",
      "loss: 0.152972  [1250/4210]\n",
      "loss: 0.116313  [1260/4210]\n",
      "loss: 0.177279  [1270/4210]\n",
      "loss: 0.415273  [1280/4210]\n",
      "loss: 0.160714  [1290/4210]\n",
      "loss: 0.060938  [1300/4210]\n",
      "In epoch 6 _ batch 1300 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.519844 \n",
      "\n",
      "loss: 0.326050  [1310/4210]\n",
      "loss: 0.028309  [1320/4210]\n",
      "loss: 0.494050  [1330/4210]\n",
      "loss: 0.177825  [1340/4210]\n",
      "loss: 0.615391  [1350/4210]\n",
      "loss: 0.083855  [1360/4210]\n",
      "loss: 0.095521  [1370/4210]\n",
      "loss: 0.216008  [1380/4210]\n",
      "loss: 0.160274  [1390/4210]\n",
      "loss: 0.114356  [1400/4210]\n",
      "In epoch 6 _ batch 1400 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.463797 \n",
      "\n",
      "loss: 0.126801  [1410/4210]\n",
      "loss: 0.207203  [1420/4210]\n",
      "loss: 0.170550  [1430/4210]\n",
      "loss: 0.146229  [1440/4210]\n",
      "loss: 0.142283  [1450/4210]\n",
      "loss: 0.280898  [1460/4210]\n",
      "loss: 0.160090  [1470/4210]\n",
      "loss: 0.567715  [1480/4210]\n",
      "loss: 0.110804  [1490/4210]\n",
      "loss: 0.122058  [1500/4210]\n",
      "In epoch 6 _ batch 1500 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.477026 \n",
      "\n",
      "loss: 0.186447  [1510/4210]\n",
      "loss: 0.150131  [1520/4210]\n",
      "loss: 0.234792  [1530/4210]\n",
      "loss: 0.197621  [1540/4210]\n",
      "loss: 0.212501  [1550/4210]\n",
      "loss: 0.198881  [1560/4210]\n",
      "loss: 0.211876  [1570/4210]\n",
      "loss: 0.049891  [1580/4210]\n",
      "loss: 0.112010  [1590/4210]\n",
      "loss: 0.139646  [1600/4210]\n",
      "In epoch 6 _ batch 1600 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.511924 \n",
      "\n",
      "loss: 0.142892  [1610/4210]\n",
      "loss: 0.101356  [1620/4210]\n",
      "loss: 0.060538  [1630/4210]\n",
      "loss: 0.207747  [1640/4210]\n",
      "loss: 0.068906  [1650/4210]\n",
      "loss: 0.187640  [1660/4210]\n",
      "loss: 0.297130  [1670/4210]\n",
      "loss: 0.356591  [1680/4210]\n",
      "loss: 0.167199  [1690/4210]\n",
      "loss: 0.117017  [1700/4210]\n",
      "In epoch 6 _ batch 1700 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.565439 \n",
      "\n",
      "loss: 0.250370  [1710/4210]\n",
      "loss: 0.385970  [1720/4210]\n",
      "loss: 0.210927  [1730/4210]\n",
      "loss: 0.355142  [1740/4210]\n",
      "loss: 0.258565  [1750/4210]\n",
      "loss: 0.505288  [1760/4210]\n",
      "loss: 0.105119  [1770/4210]\n",
      "loss: 0.468761  [1780/4210]\n",
      "loss: 0.179386  [1790/4210]\n",
      "loss: 0.040028  [1800/4210]\n",
      "In epoch 6 _ batch 1800 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.510678 \n",
      "\n",
      "loss: 0.169535  [1810/4210]\n",
      "loss: 0.221583  [1820/4210]\n",
      "loss: 0.162964  [1830/4210]\n",
      "loss: 0.199884  [1840/4210]\n",
      "loss: 0.042031  [1850/4210]\n",
      "loss: 0.051434  [1860/4210]\n",
      "loss: 0.484881  [1870/4210]\n",
      "loss: 0.397189  [1880/4210]\n",
      "loss: 0.643235  [1890/4210]\n",
      "loss: 0.066064  [1900/4210]\n",
      "In epoch 6 _ batch 1900 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.475759 \n",
      "\n",
      "loss: 0.068020  [1910/4210]\n",
      "loss: 0.358480  [1920/4210]\n",
      "loss: 0.102101  [1930/4210]\n",
      "loss: 0.337429  [1940/4210]\n",
      "loss: 0.186825  [1950/4210]\n",
      "loss: 0.054010  [1960/4210]\n",
      "loss: 0.395176  [1970/4210]\n",
      "loss: 0.254093  [1980/4210]\n",
      "loss: 0.133180  [1990/4210]\n",
      "loss: 0.156415  [2000/4210]\n",
      "In epoch 6 _ batch 2000 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.480634 \n",
      "\n",
      "loss: 0.115106  [2010/4210]\n",
      "loss: 0.285887  [2020/4210]\n",
      "loss: 0.317944  [2030/4210]\n",
      "loss: 0.133833  [2040/4210]\n",
      "loss: 0.260603  [2050/4210]\n",
      "loss: 0.459994  [2060/4210]\n",
      "loss: 0.049316  [2070/4210]\n",
      "loss: 0.270558  [2080/4210]\n",
      "loss: 0.528355  [2090/4210]\n",
      "loss: 0.215105  [2100/4210]\n",
      "In epoch 6 _ batch 2100 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.501160 \n",
      "\n",
      "loss: 0.564324  [2110/4210]\n",
      "loss: 0.127148  [2120/4210]\n",
      "loss: 0.356071  [2130/4210]\n",
      "loss: 0.172005  [2140/4210]\n",
      "loss: 0.138195  [2150/4210]\n",
      "loss: 0.316082  [2160/4210]\n",
      "loss: 0.286036  [2170/4210]\n",
      "loss: 0.243856  [2180/4210]\n",
      "loss: 0.196599  [2190/4210]\n",
      "loss: 0.428870  [2200/4210]\n",
      "In epoch 6 _ batch 2200 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.494503 \n",
      "\n",
      "loss: 0.272973  [2210/4210]\n",
      "loss: 0.020287  [2220/4210]\n",
      "loss: 0.664621  [2230/4210]\n",
      "loss: 0.073501  [2240/4210]\n",
      "loss: 0.115206  [2250/4210]\n",
      "loss: 0.205586  [2260/4210]\n",
      "loss: 0.171189  [2270/4210]\n",
      "loss: 0.125364  [2280/4210]\n",
      "loss: 0.118031  [2290/4210]\n",
      "loss: 0.290434  [2300/4210]\n",
      "In epoch 6 _ batch 2300 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.483503 \n",
      "\n",
      "loss: 0.216530  [2310/4210]\n",
      "loss: 0.252940  [2320/4210]\n",
      "loss: 0.329412  [2330/4210]\n",
      "loss: 0.140185  [2340/4210]\n",
      "loss: 0.125270  [2350/4210]\n",
      "loss: 0.164478  [2360/4210]\n",
      "loss: 0.297471  [2370/4210]\n",
      "loss: 0.320284  [2380/4210]\n",
      "loss: 0.373569  [2390/4210]\n",
      "loss: 0.163042  [2400/4210]\n",
      "In epoch 6 _ batch 2400 testing\n",
      "Test: \n",
      " Accuracy: 82.6%, Avg loss: 0.484072 \n",
      "\n",
      "loss: 0.028192  [2410/4210]\n",
      "loss: 0.308849  [2420/4210]\n",
      "loss: 0.118762  [2430/4210]\n",
      "loss: 0.194143  [2440/4210]\n",
      "loss: 0.158150  [2450/4210]\n",
      "loss: 0.119530  [2460/4210]\n",
      "loss: 0.115822  [2470/4210]\n",
      "loss: 0.261322  [2480/4210]\n",
      "loss: 0.304385  [2490/4210]\n",
      "loss: 0.195301  [2500/4210]\n",
      "In epoch 6 _ batch 2500 testing\n",
      "Test: \n",
      " Accuracy: 80.7%, Avg loss: 0.494592 \n",
      "\n",
      "loss: 0.186105  [2510/4210]\n",
      "loss: 0.202083  [2520/4210]\n",
      "loss: 0.151497  [2530/4210]\n",
      "loss: 0.178696  [2540/4210]\n",
      "loss: 0.162788  [2550/4210]\n",
      "loss: 0.219438  [2560/4210]\n",
      "loss: 0.210831  [2570/4210]\n",
      "loss: 0.197449  [2580/4210]\n",
      "loss: 0.288782  [2590/4210]\n",
      "loss: 0.178017  [2600/4210]\n",
      "In epoch 6 _ batch 2600 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.509644 \n",
      "\n",
      "loss: 0.177217  [2610/4210]\n",
      "loss: 0.137157  [2620/4210]\n",
      "loss: 0.088603  [2630/4210]\n",
      "loss: 0.013349  [2640/4210]\n",
      "loss: 0.074949  [2650/4210]\n",
      "loss: 0.196747  [2660/4210]\n",
      "loss: 0.159114  [2670/4210]\n",
      "loss: 0.112493  [2680/4210]\n",
      "loss: 0.248815  [2690/4210]\n",
      "loss: 0.396515  [2700/4210]\n",
      "In epoch 6 _ batch 2700 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.464692 \n",
      "\n",
      "loss: 0.224379  [2710/4210]\n",
      "loss: 0.065691  [2720/4210]\n",
      "loss: 0.578013  [2730/4210]\n",
      "loss: 0.166553  [2740/4210]\n",
      "loss: 0.108278  [2750/4210]\n",
      "loss: 0.025623  [2760/4210]\n",
      "loss: 0.069895  [2770/4210]\n",
      "loss: 0.100442  [2780/4210]\n",
      "loss: 0.187386  [2790/4210]\n",
      "loss: 0.062389  [2800/4210]\n",
      "In epoch 6 _ batch 2800 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.532881 \n",
      "\n",
      "loss: 0.111585  [2810/4210]\n",
      "loss: 0.269406  [2820/4210]\n",
      "loss: 0.165997  [2830/4210]\n",
      "loss: 0.198854  [2840/4210]\n",
      "loss: 0.229271  [2850/4210]\n",
      "loss: 0.177581  [2860/4210]\n",
      "loss: 0.381482  [2870/4210]\n",
      "loss: 0.567085  [2880/4210]\n",
      "loss: 0.420578  [2890/4210]\n",
      "loss: 0.126331  [2900/4210]\n",
      "In epoch 6 _ batch 2900 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.468463 \n",
      "\n",
      "loss: 0.124234  [2910/4210]\n",
      "loss: 0.126029  [2920/4210]\n",
      "loss: 0.151108  [2930/4210]\n",
      "loss: 0.229761  [2940/4210]\n",
      "loss: 0.158319  [2950/4210]\n",
      "loss: 0.411772  [2960/4210]\n",
      "loss: 0.104073  [2970/4210]\n",
      "loss: 0.074884  [2980/4210]\n",
      "loss: 0.320682  [2990/4210]\n",
      "loss: 0.353988  [3000/4210]\n",
      "In epoch 6 _ batch 3000 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.487255 \n",
      "\n",
      "loss: 0.179354  [3010/4210]\n",
      "loss: 0.142702  [3020/4210]\n",
      "loss: 0.357706  [3030/4210]\n",
      "loss: 0.115711  [3040/4210]\n",
      "loss: 0.124833  [3050/4210]\n",
      "loss: 0.155083  [3060/4210]\n",
      "loss: 0.185499  [3070/4210]\n",
      "loss: 0.157571  [3080/4210]\n",
      "loss: 0.267380  [3090/4210]\n",
      "loss: 0.152607  [3100/4210]\n",
      "In epoch 6 _ batch 3100 testing\n",
      "Test: \n",
      " Accuracy: 80.2%, Avg loss: 0.510132 \n",
      "\n",
      "loss: 0.491069  [3110/4210]\n",
      "loss: 0.216466  [3120/4210]\n",
      "loss: 0.045393  [3130/4210]\n",
      "loss: 0.222015  [3140/4210]\n",
      "loss: 0.160096  [3150/4210]\n",
      "loss: 0.201804  [3160/4210]\n",
      "loss: 0.115265  [3170/4210]\n",
      "loss: 0.071740  [3180/4210]\n",
      "loss: 0.206655  [3190/4210]\n",
      "loss: 0.029030  [3200/4210]\n",
      "In epoch 6 _ batch 3200 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.520295 \n",
      "\n",
      "loss: 0.252298  [3210/4210]\n",
      "loss: 0.192057  [3220/4210]\n",
      "loss: 0.141269  [3230/4210]\n",
      "loss: 0.254908  [3240/4210]\n",
      "loss: 0.058925  [3250/4210]\n",
      "loss: 0.227036  [3260/4210]\n",
      "loss: 0.190560  [3270/4210]\n",
      "loss: 0.249002  [3280/4210]\n",
      "loss: 0.213453  [3290/4210]\n",
      "loss: 0.107231  [3300/4210]\n",
      "In epoch 6 _ batch 3300 testing\n",
      "Test: \n",
      " Accuracy: 80.0%, Avg loss: 0.502632 \n",
      "\n",
      "loss: 0.550095  [3310/4210]\n",
      "loss: 0.135699  [3320/4210]\n",
      "loss: 0.453363  [3330/4210]\n",
      "loss: 0.112432  [3340/4210]\n",
      "loss: 0.187012  [3350/4210]\n",
      "loss: 0.155720  [3360/4210]\n",
      "loss: 0.362640  [3370/4210]\n",
      "loss: 0.061341  [3380/4210]\n",
      "loss: 0.115618  [3390/4210]\n",
      "loss: 0.120253  [3400/4210]\n",
      "In epoch 6 _ batch 3400 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.469944 \n",
      "\n",
      "loss: 0.235705  [3410/4210]\n",
      "loss: 0.321993  [3420/4210]\n",
      "loss: 0.205382  [3430/4210]\n",
      "loss: 0.256680  [3440/4210]\n",
      "loss: 0.277816  [3450/4210]\n",
      "loss: 0.087025  [3460/4210]\n",
      "loss: 0.073594  [3470/4210]\n",
      "loss: 0.142932  [3480/4210]\n",
      "loss: 0.307718  [3490/4210]\n",
      "loss: 0.082174  [3500/4210]\n",
      "In epoch 6 _ batch 3500 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.464171 \n",
      "\n",
      "loss: 0.376263  [3510/4210]\n",
      "loss: 0.281386  [3520/4210]\n",
      "loss: 0.112133  [3530/4210]\n",
      "loss: 0.521338  [3540/4210]\n",
      "loss: 0.120207  [3550/4210]\n",
      "loss: 0.201602  [3560/4210]\n",
      "loss: 0.355326  [3570/4210]\n",
      "loss: 0.087051  [3580/4210]\n",
      "loss: 0.408661  [3590/4210]\n",
      "loss: 0.323091  [3600/4210]\n",
      "In epoch 6 _ batch 3600 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.479662 \n",
      "\n",
      "loss: 0.222995  [3610/4210]\n",
      "loss: 0.382781  [3620/4210]\n",
      "loss: 0.172738  [3630/4210]\n",
      "loss: 0.108945  [3640/4210]\n",
      "loss: 0.214755  [3650/4210]\n",
      "loss: 0.057963  [3660/4210]\n",
      "loss: 0.302810  [3670/4210]\n",
      "loss: 0.285551  [3680/4210]\n",
      "loss: 0.177591  [3690/4210]\n",
      "loss: 0.080525  [3700/4210]\n",
      "In epoch 6 _ batch 3700 testing\n",
      "Test: \n",
      " Accuracy: 80.6%, Avg loss: 0.496942 \n",
      "\n",
      "loss: 0.343548  [3710/4210]\n",
      "loss: 0.271379  [3720/4210]\n",
      "loss: 0.306373  [3730/4210]\n",
      "loss: 0.337507  [3740/4210]\n",
      "loss: 0.178199  [3750/4210]\n",
      "loss: 0.131485  [3760/4210]\n",
      "loss: 0.255104  [3770/4210]\n",
      "loss: 0.267160  [3780/4210]\n",
      "loss: 0.197859  [3790/4210]\n",
      "loss: 0.157856  [3800/4210]\n",
      "In epoch 6 _ batch 3800 testing\n",
      "Test: \n",
      " Accuracy: 80.0%, Avg loss: 0.496799 \n",
      "\n",
      "loss: 0.362210  [3810/4210]\n",
      "loss: 0.667376  [3820/4210]\n",
      "loss: 0.111165  [3830/4210]\n",
      "loss: 0.355509  [3840/4210]\n",
      "loss: 0.200964  [3850/4210]\n",
      "loss: 0.058011  [3860/4210]\n",
      "loss: 0.137888  [3870/4210]\n",
      "loss: 0.223596  [3880/4210]\n",
      "loss: 0.276623  [3890/4210]\n",
      "loss: 0.310205  [3900/4210]\n",
      "In epoch 6 _ batch 3900 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.509722 \n",
      "\n",
      "loss: 0.175402  [3910/4210]\n",
      "loss: 0.306100  [3920/4210]\n",
      "loss: 0.035396  [3930/4210]\n",
      "loss: 0.410377  [3940/4210]\n",
      "loss: 0.123550  [3950/4210]\n",
      "loss: 0.076343  [3960/4210]\n",
      "loss: 0.704045  [3970/4210]\n",
      "loss: 0.290651  [3980/4210]\n",
      "loss: 0.322664  [3990/4210]\n",
      "loss: 0.256574  [4000/4210]\n",
      "In epoch 6 _ batch 4000 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.491930 \n",
      "\n",
      "loss: 0.127916  [4010/4210]\n",
      "loss: 0.188864  [4020/4210]\n",
      "loss: 0.095242  [4030/4210]\n",
      "loss: 0.378219  [4040/4210]\n",
      "loss: 0.081241  [4050/4210]\n",
      "loss: 0.196734  [4060/4210]\n",
      "loss: 0.623796  [4070/4210]\n",
      "loss: 0.125746  [4080/4210]\n",
      "loss: 0.176567  [4090/4210]\n",
      "loss: 0.136749  [4100/4210]\n",
      "In epoch 6 _ batch 4100 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.482349 \n",
      "\n",
      "loss: 0.255742  [4110/4210]\n",
      "loss: 0.382193  [4120/4210]\n",
      "loss: 0.376641  [4130/4210]\n",
      "loss: 0.443068  [4140/4210]\n",
      "loss: 0.068800  [4150/4210]\n",
      "loss: 0.060544  [4160/4210]\n",
      "loss: 0.246272  [4170/4210]\n",
      "loss: 0.258556  [4180/4210]\n",
      "loss: 0.511126  [4190/4210]\n",
      "loss: 0.059050  [4200/4210]\n",
      "In epoch 6 _ batch 4200 testing\n",
      "Test: \n",
      " Accuracy: 80.5%, Avg loss: 0.510821 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.257100  [  0/4210]\n",
      "In epoch 7 _ batch 0 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.506272 \n",
      "\n",
      "loss: 0.057502  [ 10/4210]\n",
      "loss: 0.332819  [ 20/4210]\n",
      "loss: 0.170452  [ 30/4210]\n",
      "loss: 0.214502  [ 40/4210]\n",
      "loss: 0.139275  [ 50/4210]\n",
      "loss: 0.160607  [ 60/4210]\n",
      "loss: 0.217842  [ 70/4210]\n",
      "loss: 0.171797  [ 80/4210]\n",
      "loss: 0.601860  [ 90/4210]\n",
      "loss: 0.115614  [100/4210]\n",
      "In epoch 7 _ batch 100 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.514646 \n",
      "\n",
      "loss: 0.134796  [110/4210]\n",
      "loss: 0.020573  [120/4210]\n",
      "loss: 0.067695  [130/4210]\n",
      "loss: 0.268688  [140/4210]\n",
      "loss: 0.412273  [150/4210]\n",
      "loss: 0.438496  [160/4210]\n",
      "loss: 0.104240  [170/4210]\n",
      "loss: 0.174082  [180/4210]\n",
      "loss: 0.133817  [190/4210]\n",
      "loss: 0.147406  [200/4210]\n",
      "In epoch 7 _ batch 200 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.476970 \n",
      "\n",
      "loss: 0.293382  [210/4210]\n",
      "loss: 0.025832  [220/4210]\n",
      "loss: 0.062565  [230/4210]\n",
      "loss: 0.133791  [240/4210]\n",
      "loss: 0.517848  [250/4210]\n",
      "loss: 0.183484  [260/4210]\n",
      "loss: 0.204193  [270/4210]\n",
      "loss: 0.181414  [280/4210]\n",
      "loss: 0.178594  [290/4210]\n",
      "loss: 0.203817  [300/4210]\n",
      "In epoch 7 _ batch 300 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.488856 \n",
      "\n",
      "loss: 0.144229  [310/4210]\n",
      "loss: 0.238496  [320/4210]\n",
      "loss: 0.160218  [330/4210]\n",
      "loss: 0.040932  [340/4210]\n",
      "loss: 0.310472  [350/4210]\n",
      "loss: 0.075457  [360/4210]\n",
      "loss: 0.381036  [370/4210]\n",
      "loss: 0.114189  [380/4210]\n",
      "loss: 0.306819  [390/4210]\n",
      "loss: 0.270883  [400/4210]\n",
      "In epoch 7 _ batch 400 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.471575 \n",
      "\n",
      "loss: 0.109962  [410/4210]\n",
      "loss: 0.143093  [420/4210]\n",
      "loss: 0.122701  [430/4210]\n",
      "loss: 0.082919  [440/4210]\n",
      "loss: 0.048608  [450/4210]\n",
      "loss: 0.286389  [460/4210]\n",
      "loss: 0.204507  [470/4210]\n",
      "loss: 0.213207  [480/4210]\n",
      "loss: 0.235301  [490/4210]\n",
      "loss: 0.297073  [500/4210]\n",
      "In epoch 7 _ batch 500 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.476288 \n",
      "\n",
      "loss: 0.233276  [510/4210]\n",
      "loss: 0.141732  [520/4210]\n",
      "loss: 0.237563  [530/4210]\n",
      "loss: 0.534692  [540/4210]\n",
      "loss: 0.065926  [550/4210]\n",
      "loss: 0.103335  [560/4210]\n",
      "loss: 0.186727  [570/4210]\n",
      "loss: 0.406634  [580/4210]\n",
      "loss: 0.216232  [590/4210]\n",
      "loss: 0.264385  [600/4210]\n",
      "In epoch 7 _ batch 600 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.486304 \n",
      "\n",
      "loss: 0.127556  [610/4210]\n",
      "loss: 0.095422  [620/4210]\n",
      "loss: 0.046240  [630/4210]\n",
      "loss: 0.206780  [640/4210]\n",
      "loss: 0.096190  [650/4210]\n",
      "loss: 0.071687  [660/4210]\n",
      "loss: 0.265783  [670/4210]\n",
      "loss: 0.119756  [680/4210]\n",
      "loss: 0.149220  [690/4210]\n",
      "loss: 0.076547  [700/4210]\n",
      "In epoch 7 _ batch 700 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.498083 \n",
      "\n",
      "loss: 0.102964  [710/4210]\n",
      "loss: 0.756108  [720/4210]\n",
      "loss: 0.233233  [730/4210]\n",
      "loss: 0.373954  [740/4210]\n",
      "loss: 0.064681  [750/4210]\n",
      "loss: 0.131993  [760/4210]\n",
      "loss: 0.233998  [770/4210]\n",
      "loss: 0.387617  [780/4210]\n",
      "loss: 0.315487  [790/4210]\n",
      "loss: 0.097952  [800/4210]\n",
      "In epoch 7 _ batch 800 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.489068 \n",
      "\n",
      "loss: 0.288668  [810/4210]\n",
      "loss: 0.212092  [820/4210]\n",
      "loss: 0.086970  [830/4210]\n",
      "loss: 0.063043  [840/4210]\n",
      "loss: 0.113126  [850/4210]\n",
      "loss: 0.087850  [860/4210]\n",
      "loss: 0.041705  [870/4210]\n",
      "loss: 0.239104  [880/4210]\n",
      "loss: 0.287080  [890/4210]\n",
      "loss: 0.517696  [900/4210]\n",
      "In epoch 7 _ batch 900 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.505367 \n",
      "\n",
      "loss: 0.038736  [910/4210]\n",
      "loss: 0.065646  [920/4210]\n",
      "loss: 0.162982  [930/4210]\n",
      "loss: 0.188701  [940/4210]\n",
      "loss: 0.124098  [950/4210]\n",
      "loss: 0.158301  [960/4210]\n",
      "loss: 0.424910  [970/4210]\n",
      "loss: 0.166627  [980/4210]\n",
      "loss: 0.082468  [990/4210]\n",
      "loss: 0.445613  [1000/4210]\n",
      "In epoch 7 _ batch 1000 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.490020 \n",
      "\n",
      "loss: 0.330009  [1010/4210]\n",
      "loss: 0.470714  [1020/4210]\n",
      "loss: 0.089707  [1030/4210]\n",
      "loss: 0.197644  [1040/4210]\n",
      "loss: 0.088388  [1050/4210]\n",
      "loss: 0.245515  [1060/4210]\n",
      "loss: 0.535387  [1070/4210]\n",
      "loss: 0.099537  [1080/4210]\n",
      "loss: 0.083957  [1090/4210]\n",
      "loss: 0.137665  [1100/4210]\n",
      "In epoch 7 _ batch 1100 testing\n",
      "Test: \n",
      " Accuracy: 82.3%, Avg loss: 0.466715 \n",
      "\n",
      "loss: 0.184134  [1110/4210]\n",
      "loss: 0.060154  [1120/4210]\n",
      "loss: 0.112361  [1130/4210]\n",
      "loss: 0.117584  [1140/4210]\n",
      "loss: 0.063622  [1150/4210]\n",
      "loss: 0.067073  [1160/4210]\n",
      "loss: 0.261391  [1170/4210]\n",
      "loss: 0.110791  [1180/4210]\n",
      "loss: 0.127122  [1190/4210]\n",
      "loss: 0.248317  [1200/4210]\n",
      "In epoch 7 _ batch 1200 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.510437 \n",
      "\n",
      "loss: 0.416910  [1210/4210]\n",
      "loss: 0.024958  [1220/4210]\n",
      "loss: 0.138758  [1230/4210]\n",
      "loss: 0.198691  [1240/4210]\n",
      "loss: 0.079007  [1250/4210]\n",
      "loss: 0.089444  [1260/4210]\n",
      "loss: 0.154813  [1270/4210]\n",
      "loss: 0.172771  [1280/4210]\n",
      "loss: 0.256737  [1290/4210]\n",
      "loss: 0.125005  [1300/4210]\n",
      "In epoch 7 _ batch 1300 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.514647 \n",
      "\n",
      "loss: 0.153283  [1310/4210]\n",
      "loss: 0.154823  [1320/4210]\n",
      "loss: 0.041369  [1330/4210]\n",
      "loss: 0.045805  [1340/4210]\n",
      "loss: 0.156066  [1350/4210]\n",
      "loss: 0.110730  [1360/4210]\n",
      "loss: 0.144674  [1370/4210]\n",
      "loss: 0.125128  [1380/4210]\n",
      "loss: 0.085570  [1390/4210]\n",
      "loss: 0.036557  [1400/4210]\n",
      "In epoch 7 _ batch 1400 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.501276 \n",
      "\n",
      "loss: 0.127660  [1410/4210]\n",
      "loss: 0.074110  [1420/4210]\n",
      "loss: 0.316852  [1430/4210]\n",
      "loss: 0.237406  [1440/4210]\n",
      "loss: 0.098869  [1450/4210]\n",
      "loss: 0.228239  [1460/4210]\n",
      "loss: 0.099087  [1470/4210]\n",
      "loss: 0.255216  [1480/4210]\n",
      "loss: 0.454362  [1490/4210]\n",
      "loss: 0.189934  [1500/4210]\n",
      "In epoch 7 _ batch 1500 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.513928 \n",
      "\n",
      "loss: 0.366726  [1510/4210]\n",
      "loss: 0.364645  [1520/4210]\n",
      "loss: 0.091320  [1530/4210]\n",
      "loss: 0.152303  [1540/4210]\n",
      "loss: 0.236889  [1550/4210]\n",
      "loss: 0.154690  [1560/4210]\n",
      "loss: 0.212285  [1570/4210]\n",
      "loss: 0.202139  [1580/4210]\n",
      "loss: 0.182413  [1590/4210]\n",
      "loss: 0.118816  [1600/4210]\n",
      "In epoch 7 _ batch 1600 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.486535 \n",
      "\n",
      "loss: 0.248809  [1610/4210]\n",
      "loss: 0.414800  [1620/4210]\n",
      "loss: 0.351797  [1630/4210]\n",
      "loss: 0.275884  [1640/4210]\n",
      "loss: 0.288848  [1650/4210]\n",
      "loss: 0.392811  [1660/4210]\n",
      "loss: 0.100885  [1670/4210]\n",
      "loss: 0.139780  [1680/4210]\n",
      "loss: 0.117345  [1690/4210]\n",
      "loss: 0.337406  [1700/4210]\n",
      "In epoch 7 _ batch 1700 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.462957 \n",
      "\n",
      "loss: 0.168252  [1710/4210]\n",
      "loss: 0.189898  [1720/4210]\n",
      "loss: 0.166759  [1730/4210]\n",
      "loss: 0.100072  [1740/4210]\n",
      "loss: 0.129998  [1750/4210]\n",
      "loss: 0.208170  [1760/4210]\n",
      "loss: 0.087288  [1770/4210]\n",
      "loss: 0.104228  [1780/4210]\n",
      "loss: 0.110117  [1790/4210]\n",
      "loss: 0.240561  [1800/4210]\n",
      "In epoch 7 _ batch 1800 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.500198 \n",
      "\n",
      "loss: 0.401470  [1810/4210]\n",
      "loss: 0.099888  [1820/4210]\n",
      "loss: 0.305777  [1830/4210]\n",
      "loss: 0.250552  [1840/4210]\n",
      "loss: 0.106612  [1850/4210]\n",
      "loss: 0.101644  [1860/4210]\n",
      "loss: 0.081004  [1870/4210]\n",
      "loss: 0.130127  [1880/4210]\n",
      "loss: 0.140155  [1890/4210]\n",
      "loss: 0.136398  [1900/4210]\n",
      "In epoch 7 _ batch 1900 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.504778 \n",
      "\n",
      "loss: 0.112327  [1910/4210]\n",
      "loss: 0.433673  [1920/4210]\n",
      "loss: 0.073221  [1930/4210]\n",
      "loss: 0.145873  [1940/4210]\n",
      "loss: 0.345456  [1950/4210]\n",
      "loss: 0.094235  [1960/4210]\n",
      "loss: 0.227521  [1970/4210]\n",
      "loss: 0.398413  [1980/4210]\n",
      "loss: 0.140583  [1990/4210]\n",
      "loss: 0.085778  [2000/4210]\n",
      "In epoch 7 _ batch 2000 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.499458 \n",
      "\n",
      "loss: 0.131626  [2010/4210]\n",
      "loss: 0.066066  [2020/4210]\n",
      "loss: 0.264217  [2030/4210]\n",
      "loss: 0.024574  [2040/4210]\n",
      "loss: 0.061553  [2050/4210]\n",
      "loss: 0.230012  [2060/4210]\n",
      "loss: 0.192242  [2070/4210]\n",
      "loss: 0.104352  [2080/4210]\n",
      "loss: 0.339838  [2090/4210]\n",
      "loss: 0.050202  [2100/4210]\n",
      "In epoch 7 _ batch 2100 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.496661 \n",
      "\n",
      "loss: 0.226994  [2110/4210]\n",
      "loss: 0.148454  [2120/4210]\n",
      "loss: 0.548672  [2130/4210]\n",
      "loss: 0.157020  [2140/4210]\n",
      "loss: 0.216304  [2150/4210]\n",
      "loss: 0.204594  [2160/4210]\n",
      "loss: 0.171343  [2170/4210]\n",
      "loss: 0.119141  [2180/4210]\n",
      "loss: 0.193949  [2190/4210]\n",
      "loss: 0.083563  [2200/4210]\n",
      "In epoch 7 _ batch 2200 testing\n",
      "Test: \n",
      " Accuracy: 79.9%, Avg loss: 0.526660 \n",
      "\n",
      "loss: 0.082366  [2210/4210]\n",
      "loss: 0.290119  [2220/4210]\n",
      "loss: 0.399213  [2230/4210]\n",
      "loss: 0.228478  [2240/4210]\n",
      "loss: 0.060917  [2250/4210]\n",
      "loss: 0.261266  [2260/4210]\n",
      "loss: 0.364366  [2270/4210]\n",
      "loss: 0.168641  [2280/4210]\n",
      "loss: 0.068801  [2290/4210]\n",
      "loss: 0.410517  [2300/4210]\n",
      "In epoch 7 _ batch 2300 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.509485 \n",
      "\n",
      "loss: 0.239801  [2310/4210]\n",
      "loss: 0.207203  [2320/4210]\n",
      "loss: 0.236316  [2330/4210]\n",
      "loss: 0.240731  [2340/4210]\n",
      "loss: 0.238959  [2350/4210]\n",
      "loss: 0.186244  [2360/4210]\n",
      "loss: 0.242668  [2370/4210]\n",
      "loss: 0.266874  [2380/4210]\n",
      "loss: 0.107999  [2390/4210]\n",
      "loss: 0.026844  [2400/4210]\n",
      "In epoch 7 _ batch 2400 testing\n",
      "Test: \n",
      " Accuracy: 80.2%, Avg loss: 0.481091 \n",
      "\n",
      "loss: 0.266179  [2410/4210]\n",
      "loss: 0.240058  [2420/4210]\n",
      "loss: 0.162744  [2430/4210]\n",
      "loss: 0.048389  [2440/4210]\n",
      "loss: 0.341021  [2450/4210]\n",
      "loss: 0.123367  [2460/4210]\n",
      "loss: 0.334387  [2470/4210]\n",
      "loss: 0.290067  [2480/4210]\n",
      "loss: 0.228448  [2490/4210]\n",
      "loss: 0.191603  [2500/4210]\n",
      "In epoch 7 _ batch 2500 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.504417 \n",
      "\n",
      "loss: 0.153979  [2510/4210]\n",
      "loss: 0.200036  [2520/4210]\n",
      "loss: 0.120848  [2530/4210]\n",
      "loss: 0.155980  [2540/4210]\n",
      "loss: 0.488516  [2550/4210]\n",
      "loss: 0.095743  [2560/4210]\n",
      "loss: 0.091692  [2570/4210]\n",
      "loss: 0.274879  [2580/4210]\n",
      "loss: 0.108503  [2590/4210]\n",
      "loss: 0.432577  [2600/4210]\n",
      "In epoch 7 _ batch 2600 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.521504 \n",
      "\n",
      "loss: 0.349790  [2610/4210]\n",
      "loss: 0.193902  [2620/4210]\n",
      "loss: 0.056096  [2630/4210]\n",
      "loss: 0.806050  [2640/4210]\n",
      "loss: 0.227117  [2650/4210]\n",
      "loss: 0.144129  [2660/4210]\n",
      "loss: 0.063317  [2670/4210]\n",
      "loss: 0.129986  [2680/4210]\n",
      "loss: 0.084382  [2690/4210]\n",
      "loss: 0.027446  [2700/4210]\n",
      "In epoch 7 _ batch 2700 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.530914 \n",
      "\n",
      "loss: 0.187531  [2710/4210]\n",
      "loss: 0.029942  [2720/4210]\n",
      "loss: 0.343015  [2730/4210]\n",
      "loss: 0.391443  [2740/4210]\n",
      "loss: 0.124280  [2750/4210]\n",
      "loss: 0.169865  [2760/4210]\n",
      "loss: 0.136142  [2770/4210]\n",
      "loss: 0.179138  [2780/4210]\n",
      "loss: 0.587403  [2790/4210]\n",
      "loss: 0.063111  [2800/4210]\n",
      "In epoch 7 _ batch 2800 testing\n",
      "Test: \n",
      " Accuracy: 80.5%, Avg loss: 0.571915 \n",
      "\n",
      "loss: 0.063353  [2810/4210]\n",
      "loss: 0.308354  [2820/4210]\n",
      "loss: 0.092761  [2830/4210]\n",
      "loss: 0.143791  [2840/4210]\n",
      "loss: 0.103238  [2850/4210]\n",
      "loss: 0.274377  [2860/4210]\n",
      "loss: 0.095294  [2870/4210]\n",
      "loss: 0.225026  [2880/4210]\n",
      "loss: 0.560086  [2890/4210]\n",
      "loss: 0.079630  [2900/4210]\n",
      "In epoch 7 _ batch 2900 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.519536 \n",
      "\n",
      "loss: 0.300554  [2910/4210]\n",
      "loss: 0.055019  [2920/4210]\n",
      "loss: 0.137322  [2930/4210]\n",
      "loss: 0.254887  [2940/4210]\n",
      "loss: 0.058615  [2950/4210]\n",
      "loss: 0.191840  [2960/4210]\n",
      "loss: 0.207103  [2970/4210]\n",
      "loss: 0.197289  [2980/4210]\n",
      "loss: 0.193782  [2990/4210]\n",
      "loss: 0.341025  [3000/4210]\n",
      "In epoch 7 _ batch 3000 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.502889 \n",
      "\n",
      "loss: 0.388793  [3010/4210]\n",
      "loss: 0.060673  [3020/4210]\n",
      "loss: 0.110795  [3030/4210]\n",
      "loss: 0.334535  [3040/4210]\n",
      "loss: 0.210647  [3050/4210]\n",
      "loss: 0.085798  [3060/4210]\n",
      "loss: 0.089161  [3070/4210]\n",
      "loss: 0.167550  [3080/4210]\n",
      "loss: 0.017328  [3090/4210]\n",
      "loss: 0.072099  [3100/4210]\n",
      "In epoch 7 _ batch 3100 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.518301 \n",
      "\n",
      "loss: 0.131129  [3110/4210]\n",
      "loss: 0.144576  [3120/4210]\n",
      "loss: 0.494642  [3130/4210]\n",
      "loss: 0.037146  [3140/4210]\n",
      "loss: 0.179909  [3150/4210]\n",
      "loss: 0.393384  [3160/4210]\n",
      "loss: 0.140951  [3170/4210]\n",
      "loss: 0.093614  [3180/4210]\n",
      "loss: 0.128085  [3190/4210]\n",
      "loss: 0.096662  [3200/4210]\n",
      "In epoch 7 _ batch 3200 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.527287 \n",
      "\n",
      "loss: 0.149183  [3210/4210]\n",
      "loss: 0.111377  [3220/4210]\n",
      "loss: 0.047637  [3230/4210]\n",
      "loss: 0.404223  [3240/4210]\n",
      "loss: 0.370016  [3250/4210]\n",
      "loss: 0.056337  [3260/4210]\n",
      "loss: 0.141959  [3270/4210]\n",
      "loss: 0.087015  [3280/4210]\n",
      "loss: 0.152120  [3290/4210]\n",
      "loss: 0.435534  [3300/4210]\n",
      "In epoch 7 _ batch 3300 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.508918 \n",
      "\n",
      "loss: 0.137490  [3310/4210]\n",
      "loss: 0.232519  [3320/4210]\n",
      "loss: 0.155552  [3330/4210]\n",
      "loss: 0.505741  [3340/4210]\n",
      "loss: 0.112441  [3350/4210]\n",
      "loss: 0.151540  [3360/4210]\n",
      "loss: 0.354906  [3370/4210]\n",
      "loss: 0.200593  [3380/4210]\n",
      "loss: 0.067572  [3390/4210]\n",
      "loss: 0.172622  [3400/4210]\n",
      "In epoch 7 _ batch 3400 testing\n",
      "Test: \n",
      " Accuracy: 80.2%, Avg loss: 0.543789 \n",
      "\n",
      "loss: 0.121507  [3410/4210]\n",
      "loss: 0.226794  [3420/4210]\n",
      "loss: 0.319986  [3430/4210]\n",
      "loss: 0.133533  [3440/4210]\n",
      "loss: 0.132159  [3450/4210]\n",
      "loss: 0.263544  [3460/4210]\n",
      "loss: 0.065846  [3470/4210]\n",
      "loss: 0.097089  [3480/4210]\n",
      "loss: 0.250590  [3490/4210]\n",
      "loss: 0.107128  [3500/4210]\n",
      "In epoch 7 _ batch 3500 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.534976 \n",
      "\n",
      "loss: 0.076076  [3510/4210]\n",
      "loss: 0.270943  [3520/4210]\n",
      "loss: 0.065107  [3530/4210]\n",
      "loss: 0.193942  [3540/4210]\n",
      "loss: 0.257478  [3550/4210]\n",
      "loss: 0.391039  [3560/4210]\n",
      "loss: 0.162280  [3570/4210]\n",
      "loss: 0.291326  [3580/4210]\n",
      "loss: 0.158843  [3590/4210]\n",
      "loss: 0.122217  [3600/4210]\n",
      "In epoch 7 _ batch 3600 testing\n",
      "Test: \n",
      " Accuracy: 79.7%, Avg loss: 0.541192 \n",
      "\n",
      "loss: 0.058702  [3610/4210]\n",
      "loss: 0.203789  [3620/4210]\n",
      "loss: 0.167735  [3630/4210]\n",
      "loss: 0.126072  [3640/4210]\n",
      "loss: 0.122487  [3650/4210]\n",
      "loss: 0.063408  [3660/4210]\n",
      "loss: 0.261768  [3670/4210]\n",
      "loss: 0.133099  [3680/4210]\n",
      "loss: 0.076075  [3690/4210]\n",
      "loss: 0.119408  [3700/4210]\n",
      "In epoch 7 _ batch 3700 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.544005 \n",
      "\n",
      "loss: 0.128711  [3710/4210]\n",
      "loss: 0.117978  [3720/4210]\n",
      "loss: 0.159290  [3730/4210]\n",
      "loss: 0.206923  [3740/4210]\n",
      "loss: 0.058806  [3750/4210]\n",
      "loss: 0.243653  [3760/4210]\n",
      "loss: 0.255526  [3770/4210]\n",
      "loss: 0.478458  [3780/4210]\n",
      "loss: 0.084474  [3790/4210]\n",
      "loss: 0.173676  [3800/4210]\n",
      "In epoch 7 _ batch 3800 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.532366 \n",
      "\n",
      "loss: 0.090195  [3810/4210]\n",
      "loss: 0.152651  [3820/4210]\n",
      "loss: 0.319915  [3830/4210]\n",
      "loss: 0.116386  [3840/4210]\n",
      "loss: 0.049249  [3850/4210]\n",
      "loss: 0.081224  [3860/4210]\n",
      "loss: 0.221786  [3870/4210]\n",
      "loss: 0.140612  [3880/4210]\n",
      "loss: 0.381307  [3890/4210]\n",
      "loss: 0.320263  [3900/4210]\n",
      "In epoch 7 _ batch 3900 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.496687 \n",
      "\n",
      "loss: 0.196117  [3910/4210]\n",
      "loss: 0.094696  [3920/4210]\n",
      "loss: 0.169077  [3930/4210]\n",
      "loss: 0.232195  [3940/4210]\n",
      "loss: 0.358487  [3950/4210]\n",
      "loss: 0.048020  [3960/4210]\n",
      "loss: 0.579264  [3970/4210]\n",
      "loss: 0.196719  [3980/4210]\n",
      "loss: 0.136218  [3990/4210]\n",
      "loss: 0.243829  [4000/4210]\n",
      "In epoch 7 _ batch 4000 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.514211 \n",
      "\n",
      "loss: 0.233961  [4010/4210]\n",
      "loss: 0.090550  [4020/4210]\n",
      "loss: 0.465669  [4030/4210]\n",
      "loss: 0.145013  [4040/4210]\n",
      "loss: 0.152484  [4050/4210]\n",
      "loss: 0.233735  [4060/4210]\n",
      "loss: 0.298124  [4070/4210]\n",
      "loss: 0.116058  [4080/4210]\n",
      "loss: 0.225582  [4090/4210]\n",
      "loss: 0.095096  [4100/4210]\n",
      "In epoch 7 _ batch 4100 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.511450 \n",
      "\n",
      "loss: 0.263004  [4110/4210]\n",
      "loss: 0.371346  [4120/4210]\n",
      "loss: 0.246021  [4130/4210]\n",
      "loss: 0.268503  [4140/4210]\n",
      "loss: 0.221960  [4150/4210]\n",
      "loss: 0.311500  [4160/4210]\n",
      "loss: 0.340807  [4170/4210]\n",
      "loss: 0.132312  [4180/4210]\n",
      "loss: 0.187584  [4190/4210]\n",
      "loss: 0.064487  [4200/4210]\n",
      "In epoch 7 _ batch 4200 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.516352 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.249329  [  0/4210]\n",
      "In epoch 8 _ batch 0 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.511752 \n",
      "\n",
      "loss: 0.663870  [ 10/4210]\n",
      "loss: 0.236544  [ 20/4210]\n",
      "loss: 0.067768  [ 30/4210]\n",
      "loss: 0.163277  [ 40/4210]\n",
      "loss: 0.252406  [ 50/4210]\n",
      "loss: 0.167759  [ 60/4210]\n",
      "loss: 0.190577  [ 70/4210]\n",
      "loss: 0.105008  [ 80/4210]\n",
      "loss: 0.115195  [ 90/4210]\n",
      "loss: 0.430700  [100/4210]\n",
      "In epoch 8 _ batch 100 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.532267 \n",
      "\n",
      "loss: 0.256476  [110/4210]\n",
      "loss: 0.058849  [120/4210]\n",
      "loss: 0.090068  [130/4210]\n",
      "loss: 0.053917  [140/4210]\n",
      "loss: 0.135094  [150/4210]\n",
      "loss: 0.194890  [160/4210]\n",
      "loss: 0.239761  [170/4210]\n",
      "loss: 0.133318  [180/4210]\n",
      "loss: 0.064213  [190/4210]\n",
      "loss: 0.264796  [200/4210]\n",
      "In epoch 8 _ batch 200 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.532814 \n",
      "\n",
      "loss: 0.164191  [210/4210]\n",
      "loss: 0.016618  [220/4210]\n",
      "loss: 0.165110  [230/4210]\n",
      "loss: 0.198946  [240/4210]\n",
      "loss: 0.071017  [250/4210]\n",
      "loss: 0.107963  [260/4210]\n",
      "loss: 0.252723  [270/4210]\n",
      "loss: 0.375124  [280/4210]\n",
      "loss: 0.326423  [290/4210]\n",
      "loss: 0.138831  [300/4210]\n",
      "In epoch 8 _ batch 300 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.526465 \n",
      "\n",
      "loss: 0.268344  [310/4210]\n",
      "loss: 0.104440  [320/4210]\n",
      "loss: 0.135048  [330/4210]\n",
      "loss: 0.191423  [340/4210]\n",
      "loss: 0.137569  [350/4210]\n",
      "loss: 0.168454  [360/4210]\n",
      "loss: 0.259826  [370/4210]\n",
      "loss: 0.170934  [380/4210]\n",
      "loss: 0.060526  [390/4210]\n",
      "loss: 0.092081  [400/4210]\n",
      "In epoch 8 _ batch 400 testing\n",
      "Test: \n",
      " Accuracy: 80.5%, Avg loss: 0.539504 \n",
      "\n",
      "loss: 0.068338  [410/4210]\n",
      "loss: 0.153121  [420/4210]\n",
      "loss: 0.391461  [430/4210]\n",
      "loss: 0.128984  [440/4210]\n",
      "loss: 0.141291  [450/4210]\n",
      "loss: 0.328125  [460/4210]\n",
      "loss: 0.366955  [470/4210]\n",
      "loss: 0.256189  [480/4210]\n",
      "loss: 0.305741  [490/4210]\n",
      "loss: 0.288040  [500/4210]\n",
      "In epoch 8 _ batch 500 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.511451 \n",
      "\n",
      "loss: 0.194792  [510/4210]\n",
      "loss: 0.051917  [520/4210]\n",
      "loss: 0.392030  [530/4210]\n",
      "loss: 0.179654  [540/4210]\n",
      "loss: 0.656583  [550/4210]\n",
      "loss: 0.156648  [560/4210]\n",
      "loss: 0.090719  [570/4210]\n",
      "loss: 0.239393  [580/4210]\n",
      "loss: 0.144712  [590/4210]\n",
      "loss: 0.117724  [600/4210]\n",
      "In epoch 8 _ batch 600 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.514591 \n",
      "\n",
      "loss: 0.035535  [610/4210]\n",
      "loss: 0.054773  [620/4210]\n",
      "loss: 0.220689  [630/4210]\n",
      "loss: 0.139907  [640/4210]\n",
      "loss: 0.168556  [650/4210]\n",
      "loss: 0.093829  [660/4210]\n",
      "loss: 0.491324  [670/4210]\n",
      "loss: 0.335764  [680/4210]\n",
      "loss: 0.081312  [690/4210]\n",
      "loss: 0.342522  [700/4210]\n",
      "In epoch 8 _ batch 700 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.520819 \n",
      "\n",
      "loss: 0.171913  [710/4210]\n",
      "loss: 0.036430  [720/4210]\n",
      "loss: 0.125326  [730/4210]\n",
      "loss: 0.142667  [740/4210]\n",
      "loss: 0.187239  [750/4210]\n",
      "loss: 0.135704  [760/4210]\n",
      "loss: 0.087187  [770/4210]\n",
      "loss: 0.136630  [780/4210]\n",
      "loss: 0.493712  [790/4210]\n",
      "loss: 0.135563  [800/4210]\n",
      "In epoch 8 _ batch 800 testing\n",
      "Test: \n",
      " Accuracy: 80.0%, Avg loss: 0.518246 \n",
      "\n",
      "loss: 0.115025  [810/4210]\n",
      "loss: 0.159363  [820/4210]\n",
      "loss: 0.499717  [830/4210]\n",
      "loss: 0.348916  [840/4210]\n",
      "loss: 0.372919  [850/4210]\n",
      "loss: 0.167521  [860/4210]\n",
      "loss: 0.147932  [870/4210]\n",
      "loss: 0.083344  [880/4210]\n",
      "loss: 0.144211  [890/4210]\n",
      "loss: 0.328115  [900/4210]\n",
      "In epoch 8 _ batch 900 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.519568 \n",
      "\n",
      "loss: 0.219587  [910/4210]\n",
      "loss: 0.372545  [920/4210]\n",
      "loss: 0.181160  [930/4210]\n",
      "loss: 0.190865  [940/4210]\n",
      "loss: 0.096382  [950/4210]\n",
      "loss: 0.035603  [960/4210]\n",
      "loss: 0.095775  [970/4210]\n",
      "loss: 0.285007  [980/4210]\n",
      "loss: 0.151244  [990/4210]\n",
      "loss: 0.059314  [1000/4210]\n",
      "In epoch 8 _ batch 1000 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.508194 \n",
      "\n",
      "loss: 0.271970  [1010/4210]\n",
      "loss: 0.466263  [1020/4210]\n",
      "loss: 0.142820  [1030/4210]\n",
      "loss: 0.073725  [1040/4210]\n",
      "loss: 0.154157  [1050/4210]\n",
      "loss: 0.087685  [1060/4210]\n",
      "loss: 0.043437  [1070/4210]\n",
      "loss: 0.178905  [1080/4210]\n",
      "loss: 0.277497  [1090/4210]\n",
      "loss: 0.329635  [1100/4210]\n",
      "In epoch 8 _ batch 1100 testing\n",
      "Test: \n",
      " Accuracy: 80.0%, Avg loss: 0.521538 \n",
      "\n",
      "loss: 0.272471  [1110/4210]\n",
      "loss: 0.100786  [1120/4210]\n",
      "loss: 0.126364  [1130/4210]\n",
      "loss: 0.069451  [1140/4210]\n",
      "loss: 0.106909  [1150/4210]\n",
      "loss: 0.250896  [1160/4210]\n",
      "loss: 0.080158  [1170/4210]\n",
      "loss: 0.133400  [1180/4210]\n",
      "loss: 0.133903  [1190/4210]\n",
      "loss: 0.303461  [1200/4210]\n",
      "In epoch 8 _ batch 1200 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.497473 \n",
      "\n",
      "loss: 0.039610  [1210/4210]\n",
      "loss: 0.401811  [1220/4210]\n",
      "loss: 0.050243  [1230/4210]\n",
      "loss: 0.275056  [1240/4210]\n",
      "loss: 0.071069  [1250/4210]\n",
      "loss: 0.188070  [1260/4210]\n",
      "loss: 0.070457  [1270/4210]\n",
      "loss: 0.233925  [1280/4210]\n",
      "loss: 0.319792  [1290/4210]\n",
      "loss: 0.131829  [1300/4210]\n",
      "In epoch 8 _ batch 1300 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.490785 \n",
      "\n",
      "loss: 0.162150  [1310/4210]\n",
      "loss: 0.252407  [1320/4210]\n",
      "loss: 0.164254  [1330/4210]\n",
      "loss: 0.033766  [1340/4210]\n",
      "loss: 0.194586  [1350/4210]\n",
      "loss: 0.073236  [1360/4210]\n",
      "loss: 0.093854  [1370/4210]\n",
      "loss: 0.532455  [1380/4210]\n",
      "loss: 0.403074  [1390/4210]\n",
      "loss: 0.428101  [1400/4210]\n",
      "In epoch 8 _ batch 1400 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.515579 \n",
      "\n",
      "loss: 0.152574  [1410/4210]\n",
      "loss: 0.058304  [1420/4210]\n",
      "loss: 0.232955  [1430/4210]\n",
      "loss: 0.487753  [1440/4210]\n",
      "loss: 0.247233  [1450/4210]\n",
      "loss: 0.041597  [1460/4210]\n",
      "loss: 0.192810  [1470/4210]\n",
      "loss: 0.151525  [1480/4210]\n",
      "loss: 0.095932  [1490/4210]\n",
      "loss: 0.073319  [1500/4210]\n",
      "In epoch 8 _ batch 1500 testing\n",
      "Test: \n",
      " Accuracy: 80.7%, Avg loss: 0.521895 \n",
      "\n",
      "loss: 0.126791  [1510/4210]\n",
      "loss: 0.539293  [1520/4210]\n",
      "loss: 0.076323  [1530/4210]\n",
      "loss: 0.190622  [1540/4210]\n",
      "loss: 0.087131  [1550/4210]\n",
      "loss: 0.114463  [1560/4210]\n",
      "loss: 0.183032  [1570/4210]\n",
      "loss: 0.407256  [1580/4210]\n",
      "loss: 0.128196  [1590/4210]\n",
      "loss: 0.249800  [1600/4210]\n",
      "In epoch 8 _ batch 1600 testing\n",
      "Test: \n",
      " Accuracy: 80.6%, Avg loss: 0.558027 \n",
      "\n",
      "loss: 0.458334  [1610/4210]\n",
      "loss: 0.156673  [1620/4210]\n",
      "loss: 0.181978  [1630/4210]\n",
      "loss: 0.213695  [1640/4210]\n",
      "loss: 0.046850  [1650/4210]\n",
      "loss: 0.401244  [1660/4210]\n",
      "loss: 0.351325  [1670/4210]\n",
      "loss: 0.107309  [1680/4210]\n",
      "loss: 0.532445  [1690/4210]\n",
      "loss: 0.291332  [1700/4210]\n",
      "In epoch 8 _ batch 1700 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.518424 \n",
      "\n",
      "loss: 0.146356  [1710/4210]\n",
      "loss: 0.322707  [1720/4210]\n",
      "loss: 0.417670  [1730/4210]\n",
      "loss: 0.178143  [1740/4210]\n",
      "loss: 0.159778  [1750/4210]\n",
      "loss: 0.252802  [1760/4210]\n",
      "loss: 0.171534  [1770/4210]\n",
      "loss: 0.185718  [1780/4210]\n",
      "loss: 0.069895  [1790/4210]\n",
      "loss: 0.087555  [1800/4210]\n",
      "In epoch 8 _ batch 1800 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.546621 \n",
      "\n",
      "loss: 0.313652  [1810/4210]\n",
      "loss: 0.133075  [1820/4210]\n",
      "loss: 0.131561  [1830/4210]\n",
      "loss: 0.268307  [1840/4210]\n",
      "loss: 0.021590  [1850/4210]\n",
      "loss: 0.318722  [1860/4210]\n",
      "loss: 0.169057  [1870/4210]\n",
      "loss: 0.090947  [1880/4210]\n",
      "loss: 0.050687  [1890/4210]\n",
      "loss: 0.170985  [1900/4210]\n",
      "In epoch 8 _ batch 1900 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.529497 \n",
      "\n",
      "loss: 0.307389  [1910/4210]\n",
      "loss: 0.050727  [1920/4210]\n",
      "loss: 0.592392  [1930/4210]\n",
      "loss: 0.153552  [1940/4210]\n",
      "loss: 0.491416  [1950/4210]\n",
      "loss: 0.155063  [1960/4210]\n",
      "loss: 0.171389  [1970/4210]\n",
      "loss: 0.143992  [1980/4210]\n",
      "loss: 0.168366  [1990/4210]\n",
      "loss: 0.039431  [2000/4210]\n",
      "In epoch 8 _ batch 2000 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.529576 \n",
      "\n",
      "loss: 0.321181  [2010/4210]\n",
      "loss: 0.201102  [2020/4210]\n",
      "loss: 0.054961  [2030/4210]\n",
      "loss: 0.295172  [2040/4210]\n",
      "loss: 0.166914  [2050/4210]\n",
      "loss: 0.202402  [2060/4210]\n",
      "loss: 0.058882  [2070/4210]\n",
      "loss: 0.071939  [2080/4210]\n",
      "loss: 0.165971  [2090/4210]\n",
      "loss: 0.162068  [2100/4210]\n",
      "In epoch 8 _ batch 2100 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.539398 \n",
      "\n",
      "loss: 0.084702  [2110/4210]\n",
      "loss: 0.275093  [2120/4210]\n",
      "loss: 0.044813  [2130/4210]\n",
      "loss: 0.084378  [2140/4210]\n",
      "loss: 0.376918  [2150/4210]\n",
      "loss: 0.494232  [2160/4210]\n",
      "loss: 0.044195  [2170/4210]\n",
      "loss: 0.072046  [2180/4210]\n",
      "loss: 0.111531  [2190/4210]\n",
      "loss: 0.441299  [2200/4210]\n",
      "In epoch 8 _ batch 2200 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.538389 \n",
      "\n",
      "loss: 0.106377  [2210/4210]\n",
      "loss: 0.200432  [2220/4210]\n",
      "loss: 0.377708  [2230/4210]\n",
      "loss: 0.142246  [2240/4210]\n",
      "loss: 0.065858  [2250/4210]\n",
      "loss: 0.165795  [2260/4210]\n",
      "loss: 0.064898  [2270/4210]\n",
      "loss: 0.447825  [2280/4210]\n",
      "loss: 0.141024  [2290/4210]\n",
      "loss: 0.069921  [2300/4210]\n",
      "In epoch 8 _ batch 2300 testing\n",
      "Test: \n",
      " Accuracy: 80.6%, Avg loss: 0.552979 \n",
      "\n",
      "loss: 0.101568  [2310/4210]\n",
      "loss: 0.394557  [2320/4210]\n",
      "loss: 0.310159  [2330/4210]\n",
      "loss: 0.371765  [2340/4210]\n",
      "loss: 0.238144  [2350/4210]\n",
      "loss: 0.141708  [2360/4210]\n",
      "loss: 0.206746  [2370/4210]\n",
      "loss: 0.307328  [2380/4210]\n",
      "loss: 0.045831  [2390/4210]\n",
      "loss: 0.548733  [2400/4210]\n",
      "In epoch 8 _ batch 2400 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.499169 \n",
      "\n",
      "loss: 0.132898  [2410/4210]\n",
      "loss: 0.134697  [2420/4210]\n",
      "loss: 0.765008  [2430/4210]\n",
      "loss: 0.197090  [2440/4210]\n",
      "loss: 0.027287  [2450/4210]\n",
      "loss: 0.045614  [2460/4210]\n",
      "loss: 0.081604  [2470/4210]\n",
      "loss: 0.108017  [2480/4210]\n",
      "loss: 0.131858  [2490/4210]\n",
      "loss: 0.212686  [2500/4210]\n",
      "In epoch 8 _ batch 2500 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.502394 \n",
      "\n",
      "loss: 0.120550  [2510/4210]\n",
      "loss: 0.080810  [2520/4210]\n",
      "loss: 0.272680  [2530/4210]\n",
      "loss: 0.123056  [2540/4210]\n",
      "loss: 0.103142  [2550/4210]\n",
      "loss: 0.171327  [2560/4210]\n",
      "loss: 0.219799  [2570/4210]\n",
      "loss: 0.099557  [2580/4210]\n",
      "loss: 0.152136  [2590/4210]\n",
      "loss: 0.083879  [2600/4210]\n",
      "In epoch 8 _ batch 2600 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.517516 \n",
      "\n",
      "loss: 0.461802  [2610/4210]\n",
      "loss: 0.033566  [2620/4210]\n",
      "loss: 0.584702  [2630/4210]\n",
      "loss: 0.073438  [2640/4210]\n",
      "loss: 0.264585  [2650/4210]\n",
      "loss: 0.081754  [2660/4210]\n",
      "loss: 0.204614  [2670/4210]\n",
      "loss: 0.088973  [2680/4210]\n",
      "loss: 0.333494  [2690/4210]\n",
      "loss: 0.300084  [2700/4210]\n",
      "In epoch 8 _ batch 2700 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.544218 \n",
      "\n",
      "loss: 0.253617  [2710/4210]\n",
      "loss: 0.205155  [2720/4210]\n",
      "loss: 0.374890  [2730/4210]\n",
      "loss: 0.204158  [2740/4210]\n",
      "loss: 0.352908  [2750/4210]\n",
      "loss: 0.086474  [2760/4210]\n",
      "loss: 0.200293  [2770/4210]\n",
      "loss: 0.140392  [2780/4210]\n",
      "loss: 0.040267  [2790/4210]\n",
      "loss: 0.154057  [2800/4210]\n",
      "In epoch 8 _ batch 2800 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.500479 \n",
      "\n",
      "loss: 0.105575  [2810/4210]\n",
      "loss: 0.479273  [2820/4210]\n",
      "loss: 0.277806  [2830/4210]\n",
      "loss: 0.181437  [2840/4210]\n",
      "loss: 0.127006  [2850/4210]\n",
      "loss: 0.199340  [2860/4210]\n",
      "loss: 0.105422  [2870/4210]\n",
      "loss: 0.156656  [2880/4210]\n",
      "loss: 0.124948  [2890/4210]\n",
      "loss: 0.320645  [2900/4210]\n",
      "In epoch 8 _ batch 2900 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.521525 \n",
      "\n",
      "loss: 0.474089  [2910/4210]\n",
      "loss: 0.160466  [2920/4210]\n",
      "loss: 0.062764  [2930/4210]\n",
      "loss: 0.350219  [2940/4210]\n",
      "loss: 0.338887  [2950/4210]\n",
      "loss: 0.222647  [2960/4210]\n",
      "loss: 0.276391  [2970/4210]\n",
      "loss: 0.211264  [2980/4210]\n",
      "loss: 0.064514  [2990/4210]\n",
      "loss: 0.479778  [3000/4210]\n",
      "In epoch 8 _ batch 3000 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.516643 \n",
      "\n",
      "loss: 0.219352  [3010/4210]\n",
      "loss: 0.369400  [3020/4210]\n",
      "loss: 0.591573  [3030/4210]\n",
      "loss: 0.073589  [3040/4210]\n",
      "loss: 0.070478  [3050/4210]\n",
      "loss: 0.121529  [3060/4210]\n",
      "loss: 0.111282  [3070/4210]\n",
      "loss: 0.245362  [3080/4210]\n",
      "loss: 0.208970  [3090/4210]\n",
      "loss: 0.039203  [3100/4210]\n",
      "In epoch 8 _ batch 3100 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.513680 \n",
      "\n",
      "loss: 0.359078  [3110/4210]\n",
      "loss: 0.204774  [3120/4210]\n",
      "loss: 0.284739  [3130/4210]\n",
      "loss: 0.229503  [3140/4210]\n",
      "loss: 0.046957  [3150/4210]\n",
      "loss: 0.148245  [3160/4210]\n",
      "loss: 0.062978  [3170/4210]\n",
      "loss: 0.097411  [3180/4210]\n",
      "loss: 0.100464  [3190/4210]\n",
      "loss: 0.064128  [3200/4210]\n",
      "In epoch 8 _ batch 3200 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.517174 \n",
      "\n",
      "loss: 0.074202  [3210/4210]\n",
      "loss: 0.345610  [3220/4210]\n",
      "loss: 0.156688  [3230/4210]\n",
      "loss: 0.216022  [3240/4210]\n",
      "loss: 0.231220  [3250/4210]\n",
      "loss: 0.126507  [3260/4210]\n",
      "loss: 0.376001  [3270/4210]\n",
      "loss: 0.085888  [3280/4210]\n",
      "loss: 0.283819  [3290/4210]\n",
      "loss: 0.074546  [3300/4210]\n",
      "In epoch 8 _ batch 3300 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.507888 \n",
      "\n",
      "loss: 0.111356  [3310/4210]\n",
      "loss: 0.191865  [3320/4210]\n",
      "loss: 0.167413  [3330/4210]\n",
      "loss: 0.641569  [3340/4210]\n",
      "loss: 0.333271  [3350/4210]\n",
      "loss: 0.322910  [3360/4210]\n",
      "loss: 0.172254  [3370/4210]\n",
      "loss: 0.242318  [3380/4210]\n",
      "loss: 0.292962  [3390/4210]\n",
      "loss: 0.123273  [3400/4210]\n",
      "In epoch 8 _ batch 3400 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.492571 \n",
      "\n",
      "loss: 0.159143  [3410/4210]\n",
      "loss: 0.243497  [3420/4210]\n",
      "loss: 0.078426  [3430/4210]\n",
      "loss: 0.121593  [3440/4210]\n",
      "loss: 0.202082  [3450/4210]\n",
      "loss: 0.086238  [3460/4210]\n",
      "loss: 0.473186  [3470/4210]\n",
      "loss: 0.179607  [3480/4210]\n",
      "loss: 0.263800  [3490/4210]\n",
      "loss: 0.236058  [3500/4210]\n",
      "In epoch 8 _ batch 3500 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.493431 \n",
      "\n",
      "loss: 0.109660  [3510/4210]\n",
      "loss: 0.031296  [3520/4210]\n",
      "loss: 0.179068  [3530/4210]\n",
      "loss: 0.227212  [3540/4210]\n",
      "loss: 0.037766  [3550/4210]\n",
      "loss: 0.109884  [3560/4210]\n",
      "loss: 0.109991  [3570/4210]\n",
      "loss: 0.238169  [3580/4210]\n",
      "loss: 0.048128  [3590/4210]\n",
      "loss: 0.162187  [3600/4210]\n",
      "In epoch 8 _ batch 3600 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.529598 \n",
      "\n",
      "loss: 0.146185  [3610/4210]\n",
      "loss: 0.192265  [3620/4210]\n",
      "loss: 0.128285  [3630/4210]\n",
      "loss: 0.174829  [3640/4210]\n",
      "loss: 0.406886  [3650/4210]\n",
      "loss: 0.046754  [3660/4210]\n",
      "loss: 0.160307  [3670/4210]\n",
      "loss: 0.165006  [3680/4210]\n",
      "loss: 0.082960  [3690/4210]\n",
      "loss: 0.114245  [3700/4210]\n",
      "In epoch 8 _ batch 3700 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.535328 \n",
      "\n",
      "loss: 0.399682  [3710/4210]\n",
      "loss: 0.024311  [3720/4210]\n",
      "loss: 0.184623  [3730/4210]\n",
      "loss: 0.123614  [3740/4210]\n",
      "loss: 0.138626  [3750/4210]\n",
      "loss: 0.223671  [3760/4210]\n",
      "loss: 0.117683  [3770/4210]\n",
      "loss: 0.134576  [3780/4210]\n",
      "loss: 0.245817  [3790/4210]\n",
      "loss: 0.110650  [3800/4210]\n",
      "In epoch 8 _ batch 3800 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.521816 \n",
      "\n",
      "loss: 0.147531  [3810/4210]\n",
      "loss: 0.191208  [3820/4210]\n",
      "loss: 0.221132  [3830/4210]\n",
      "loss: 0.038422  [3840/4210]\n",
      "loss: 0.368455  [3850/4210]\n",
      "loss: 0.176079  [3860/4210]\n",
      "loss: 0.066999  [3870/4210]\n",
      "loss: 0.305357  [3880/4210]\n",
      "loss: 0.132313  [3890/4210]\n",
      "loss: 0.234025  [3900/4210]\n",
      "In epoch 8 _ batch 3900 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.509401 \n",
      "\n",
      "loss: 0.049343  [3910/4210]\n",
      "loss: 0.280270  [3920/4210]\n",
      "loss: 0.163267  [3930/4210]\n",
      "loss: 0.130754  [3940/4210]\n",
      "loss: 0.085260  [3950/4210]\n",
      "loss: 0.177364  [3960/4210]\n",
      "loss: 0.102970  [3970/4210]\n",
      "loss: 0.202664  [3980/4210]\n",
      "loss: 0.227417  [3990/4210]\n",
      "loss: 0.168958  [4000/4210]\n",
      "In epoch 8 _ batch 4000 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.514934 \n",
      "\n",
      "loss: 0.076602  [4010/4210]\n",
      "loss: 0.099839  [4020/4210]\n",
      "loss: 0.181423  [4030/4210]\n",
      "loss: 0.162827  [4040/4210]\n",
      "loss: 0.128349  [4050/4210]\n",
      "loss: 0.119903  [4060/4210]\n",
      "loss: 0.183816  [4070/4210]\n",
      "loss: 0.139440  [4080/4210]\n",
      "loss: 0.351693  [4090/4210]\n",
      "loss: 0.454171  [4100/4210]\n",
      "In epoch 8 _ batch 4100 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.521723 \n",
      "\n",
      "loss: 0.101993  [4110/4210]\n",
      "loss: 0.115712  [4120/4210]\n",
      "loss: 0.050144  [4130/4210]\n",
      "loss: 0.166822  [4140/4210]\n",
      "loss: 0.160531  [4150/4210]\n",
      "loss: 0.057788  [4160/4210]\n",
      "loss: 0.199220  [4170/4210]\n",
      "loss: 0.727512  [4180/4210]\n",
      "loss: 0.162874  [4190/4210]\n",
      "loss: 0.178901  [4200/4210]\n",
      "In epoch 8 _ batch 4200 testing\n",
      "Test: \n",
      " Accuracy: 80.7%, Avg loss: 0.512118 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.278780  [  0/4210]\n",
      "In epoch 9 _ batch 0 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.513795 \n",
      "\n",
      "loss: 0.086343  [ 10/4210]\n",
      "loss: 0.075377  [ 20/4210]\n",
      "loss: 0.107562  [ 30/4210]\n",
      "loss: 0.064870  [ 40/4210]\n",
      "loss: 0.077615  [ 50/4210]\n",
      "loss: 0.152699  [ 60/4210]\n",
      "loss: 0.053198  [ 70/4210]\n",
      "loss: 0.398160  [ 80/4210]\n",
      "loss: 0.297592  [ 90/4210]\n",
      "loss: 0.190452  [100/4210]\n",
      "In epoch 9 _ batch 100 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.526187 \n",
      "\n",
      "loss: 0.115492  [110/4210]\n",
      "loss: 0.404773  [120/4210]\n",
      "loss: 0.065612  [130/4210]\n",
      "loss: 0.016783  [140/4210]\n",
      "loss: 0.060523  [150/4210]\n",
      "loss: 0.036887  [160/4210]\n",
      "loss: 0.160785  [170/4210]\n",
      "loss: 0.319637  [180/4210]\n",
      "loss: 0.121946  [190/4210]\n",
      "loss: 0.183812  [200/4210]\n",
      "In epoch 9 _ batch 200 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.517855 \n",
      "\n",
      "loss: 0.225881  [210/4210]\n",
      "loss: 0.184043  [220/4210]\n",
      "loss: 0.152545  [230/4210]\n",
      "loss: 0.057131  [240/4210]\n",
      "loss: 0.179246  [250/4210]\n",
      "loss: 0.130905  [260/4210]\n",
      "loss: 0.154038  [270/4210]\n",
      "loss: 0.114554  [280/4210]\n",
      "loss: 0.163475  [290/4210]\n",
      "loss: 0.172771  [300/4210]\n",
      "In epoch 9 _ batch 300 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.554012 \n",
      "\n",
      "loss: 0.062821  [310/4210]\n",
      "loss: 0.034791  [320/4210]\n",
      "loss: 0.240293  [330/4210]\n",
      "loss: 0.202191  [340/4210]\n",
      "loss: 0.131866  [350/4210]\n",
      "loss: 0.089934  [360/4210]\n",
      "loss: 0.195751  [370/4210]\n",
      "loss: 0.409365  [380/4210]\n",
      "loss: 0.168926  [390/4210]\n",
      "loss: 0.253464  [400/4210]\n",
      "In epoch 9 _ batch 400 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.507909 \n",
      "\n",
      "loss: 0.131564  [410/4210]\n",
      "loss: 0.146432  [420/4210]\n",
      "loss: 0.087821  [430/4210]\n",
      "loss: 0.141591  [440/4210]\n",
      "loss: 0.135793  [450/4210]\n",
      "loss: 0.081388  [460/4210]\n",
      "loss: 0.029198  [470/4210]\n",
      "loss: 0.290360  [480/4210]\n",
      "loss: 0.152418  [490/4210]\n",
      "loss: 0.070515  [500/4210]\n",
      "In epoch 9 _ batch 500 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.525386 \n",
      "\n",
      "loss: 0.121418  [510/4210]\n",
      "loss: 0.168977  [520/4210]\n",
      "loss: 0.025610  [530/4210]\n",
      "loss: 0.118774  [540/4210]\n",
      "loss: 0.108327  [550/4210]\n",
      "loss: 0.400918  [560/4210]\n",
      "loss: 0.288169  [570/4210]\n",
      "loss: 0.123300  [580/4210]\n",
      "loss: 0.604781  [590/4210]\n",
      "loss: 0.176145  [600/4210]\n",
      "In epoch 9 _ batch 600 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.531673 \n",
      "\n",
      "loss: 0.384644  [610/4210]\n",
      "loss: 0.496307  [620/4210]\n",
      "loss: 0.227817  [630/4210]\n",
      "loss: 0.269005  [640/4210]\n",
      "loss: 0.830964  [650/4210]\n",
      "loss: 0.497267  [660/4210]\n",
      "loss: 0.083296  [670/4210]\n",
      "loss: 0.140418  [680/4210]\n",
      "loss: 0.105508  [690/4210]\n",
      "loss: 0.195320  [700/4210]\n",
      "In epoch 9 _ batch 700 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.501931 \n",
      "\n",
      "loss: 0.503567  [710/4210]\n",
      "loss: 0.236507  [720/4210]\n",
      "loss: 0.310138  [730/4210]\n",
      "loss: 0.511461  [740/4210]\n",
      "loss: 0.332985  [750/4210]\n",
      "loss: 0.822996  [760/4210]\n",
      "loss: 0.121932  [770/4210]\n",
      "loss: 0.217407  [780/4210]\n",
      "loss: 0.331003  [790/4210]\n",
      "loss: 0.210403  [800/4210]\n",
      "In epoch 9 _ batch 800 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.522561 \n",
      "\n",
      "loss: 0.123074  [810/4210]\n",
      "loss: 0.754147  [820/4210]\n",
      "loss: 0.121834  [830/4210]\n",
      "loss: 0.202700  [840/4210]\n",
      "loss: 0.342463  [850/4210]\n",
      "loss: 0.181281  [860/4210]\n",
      "loss: 0.291494  [870/4210]\n",
      "loss: 0.131543  [880/4210]\n",
      "loss: 0.079129  [890/4210]\n",
      "loss: 0.198250  [900/4210]\n",
      "In epoch 9 _ batch 900 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.500957 \n",
      "\n",
      "loss: 0.188551  [910/4210]\n",
      "loss: 0.293077  [920/4210]\n",
      "loss: 0.125206  [930/4210]\n",
      "loss: 0.103027  [940/4210]\n",
      "loss: 0.598251  [950/4210]\n",
      "loss: 0.139649  [960/4210]\n",
      "loss: 0.234494  [970/4210]\n",
      "loss: 0.153495  [980/4210]\n",
      "loss: 0.083454  [990/4210]\n",
      "loss: 0.158820  [1000/4210]\n",
      "In epoch 9 _ batch 1000 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.520611 \n",
      "\n",
      "loss: 0.070761  [1010/4210]\n",
      "loss: 0.100734  [1020/4210]\n",
      "loss: 0.221437  [1030/4210]\n",
      "loss: 0.119873  [1040/4210]\n",
      "loss: 0.409130  [1050/4210]\n",
      "loss: 0.164938  [1060/4210]\n",
      "loss: 0.358089  [1070/4210]\n",
      "loss: 0.092959  [1080/4210]\n",
      "loss: 0.095158  [1090/4210]\n",
      "loss: 0.124638  [1100/4210]\n",
      "In epoch 9 _ batch 1100 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.527092 \n",
      "\n",
      "loss: 0.262084  [1110/4210]\n",
      "loss: 0.668055  [1120/4210]\n",
      "loss: 0.206110  [1130/4210]\n",
      "loss: 0.477032  [1140/4210]\n",
      "loss: 0.064002  [1150/4210]\n",
      "loss: 0.042205  [1160/4210]\n",
      "loss: 0.216463  [1170/4210]\n",
      "loss: 0.100021  [1180/4210]\n",
      "loss: 0.124257  [1190/4210]\n",
      "loss: 0.231229  [1200/4210]\n",
      "In epoch 9 _ batch 1200 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.541417 \n",
      "\n",
      "loss: 0.256850  [1210/4210]\n",
      "loss: 0.090370  [1220/4210]\n",
      "loss: 0.623647  [1230/4210]\n",
      "loss: 0.096002  [1240/4210]\n",
      "loss: 0.136709  [1250/4210]\n",
      "loss: 0.278305  [1260/4210]\n",
      "loss: 0.296964  [1270/4210]\n",
      "loss: 0.114401  [1280/4210]\n",
      "loss: 0.483557  [1290/4210]\n",
      "loss: 0.148617  [1300/4210]\n",
      "In epoch 9 _ batch 1300 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.553306 \n",
      "\n",
      "loss: 0.030133  [1310/4210]\n",
      "loss: 0.179913  [1320/4210]\n",
      "loss: 0.062373  [1330/4210]\n",
      "loss: 0.773408  [1340/4210]\n",
      "loss: 0.027946  [1350/4210]\n",
      "loss: 0.184732  [1360/4210]\n",
      "loss: 0.173212  [1370/4210]\n",
      "loss: 0.022493  [1380/4210]\n",
      "loss: 0.100146  [1390/4210]\n",
      "loss: 0.098834  [1400/4210]\n",
      "In epoch 9 _ batch 1400 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.541742 \n",
      "\n",
      "loss: 0.352066  [1410/4210]\n",
      "loss: 0.124722  [1420/4210]\n",
      "loss: 0.199899  [1430/4210]\n",
      "loss: 0.200120  [1440/4210]\n",
      "loss: 0.095856  [1450/4210]\n",
      "loss: 0.155048  [1460/4210]\n",
      "loss: 0.141855  [1470/4210]\n",
      "loss: 0.362631  [1480/4210]\n",
      "loss: 0.485439  [1490/4210]\n",
      "loss: 0.080356  [1500/4210]\n",
      "In epoch 9 _ batch 1500 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.536781 \n",
      "\n",
      "loss: 0.256978  [1510/4210]\n",
      "loss: 0.093527  [1520/4210]\n",
      "loss: 0.071578  [1530/4210]\n",
      "loss: 0.064429  [1540/4210]\n",
      "loss: 0.208008  [1550/4210]\n",
      "loss: 0.035015  [1560/4210]\n",
      "loss: 0.175089  [1570/4210]\n",
      "loss: 0.202992  [1580/4210]\n",
      "loss: 0.217408  [1590/4210]\n",
      "loss: 0.055568  [1600/4210]\n",
      "In epoch 9 _ batch 1600 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.548459 \n",
      "\n",
      "loss: 0.318063  [1610/4210]\n",
      "loss: 0.141780  [1620/4210]\n",
      "loss: 0.110205  [1630/4210]\n",
      "loss: 0.501406  [1640/4210]\n",
      "loss: 0.205732  [1650/4210]\n",
      "loss: 0.369207  [1660/4210]\n",
      "loss: 0.347810  [1670/4210]\n",
      "loss: 0.276805  [1680/4210]\n",
      "loss: 0.263935  [1690/4210]\n",
      "loss: 0.195822  [1700/4210]\n",
      "In epoch 9 _ batch 1700 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.531482 \n",
      "\n",
      "loss: 0.114139  [1710/4210]\n",
      "loss: 0.067431  [1720/4210]\n",
      "loss: 0.153594  [1730/4210]\n",
      "loss: 0.144206  [1740/4210]\n",
      "loss: 0.052098  [1750/4210]\n",
      "loss: 0.172390  [1760/4210]\n",
      "loss: 0.065901  [1770/4210]\n",
      "loss: 0.113591  [1780/4210]\n",
      "loss: 0.363324  [1790/4210]\n",
      "loss: 0.266380  [1800/4210]\n",
      "In epoch 9 _ batch 1800 testing\n",
      "Test: \n",
      " Accuracy: 80.7%, Avg loss: 0.544912 \n",
      "\n",
      "loss: 0.127940  [1810/4210]\n",
      "loss: 0.037349  [1820/4210]\n",
      "loss: 0.086273  [1830/4210]\n",
      "loss: 0.347636  [1840/4210]\n",
      "loss: 0.249186  [1850/4210]\n",
      "loss: 0.101972  [1860/4210]\n",
      "loss: 0.307391  [1870/4210]\n",
      "loss: 0.129012  [1880/4210]\n",
      "loss: 0.119365  [1890/4210]\n",
      "loss: 0.123817  [1900/4210]\n",
      "In epoch 9 _ batch 1900 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.532258 \n",
      "\n",
      "loss: 0.226363  [1910/4210]\n",
      "loss: 0.132797  [1920/4210]\n",
      "loss: 0.223577  [1930/4210]\n",
      "loss: 0.194776  [1940/4210]\n",
      "loss: 0.085765  [1950/4210]\n",
      "loss: 0.042385  [1960/4210]\n",
      "loss: 0.492343  [1970/4210]\n",
      "loss: 0.219887  [1980/4210]\n",
      "loss: 0.122885  [1990/4210]\n",
      "loss: 0.130418  [2000/4210]\n",
      "In epoch 9 _ batch 2000 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.543096 \n",
      "\n",
      "loss: 0.330137  [2010/4210]\n",
      "loss: 0.144069  [2020/4210]\n",
      "loss: 0.215028  [2030/4210]\n",
      "loss: 0.236820  [2040/4210]\n",
      "loss: 0.061609  [2050/4210]\n",
      "loss: 0.129884  [2060/4210]\n",
      "loss: 0.285038  [2070/4210]\n",
      "loss: 0.047670  [2080/4210]\n",
      "loss: 0.047125  [2090/4210]\n",
      "loss: 0.095283  [2100/4210]\n",
      "In epoch 9 _ batch 2100 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.539609 \n",
      "\n",
      "loss: 0.075019  [2110/4210]\n",
      "loss: 0.127151  [2120/4210]\n",
      "loss: 0.167530  [2130/4210]\n",
      "loss: 0.220182  [2140/4210]\n",
      "loss: 0.483035  [2150/4210]\n",
      "loss: 0.061473  [2160/4210]\n",
      "loss: 0.192947  [2170/4210]\n",
      "loss: 0.429954  [2180/4210]\n",
      "loss: 0.439487  [2190/4210]\n",
      "loss: 0.293309  [2200/4210]\n",
      "In epoch 9 _ batch 2200 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.521389 \n",
      "\n",
      "loss: 0.193775  [2210/4210]\n",
      "loss: 0.150561  [2220/4210]\n",
      "loss: 0.027848  [2230/4210]\n",
      "loss: 0.159655  [2240/4210]\n",
      "loss: 0.063881  [2250/4210]\n",
      "loss: 0.183678  [2260/4210]\n",
      "loss: 0.079335  [2270/4210]\n",
      "loss: 0.208204  [2280/4210]\n",
      "loss: 0.124078  [2290/4210]\n",
      "loss: 0.555778  [2300/4210]\n",
      "In epoch 9 _ batch 2300 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.522010 \n",
      "\n",
      "loss: 0.091197  [2310/4210]\n",
      "loss: 0.121349  [2320/4210]\n",
      "loss: 0.066830  [2330/4210]\n",
      "loss: 0.095925  [2340/4210]\n",
      "loss: 0.122609  [2350/4210]\n",
      "loss: 0.236479  [2360/4210]\n",
      "loss: 0.309445  [2370/4210]\n",
      "loss: 0.130492  [2380/4210]\n",
      "loss: 0.157110  [2390/4210]\n",
      "loss: 0.042864  [2400/4210]\n",
      "In epoch 9 _ batch 2400 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.530194 \n",
      "\n",
      "loss: 0.077318  [2410/4210]\n",
      "loss: 0.080323  [2420/4210]\n",
      "loss: 0.268109  [2430/4210]\n",
      "loss: 0.160035  [2440/4210]\n",
      "loss: 0.117109  [2450/4210]\n",
      "loss: 0.349511  [2460/4210]\n",
      "loss: 0.084162  [2470/4210]\n",
      "loss: 0.466570  [2480/4210]\n",
      "loss: 0.119281  [2490/4210]\n",
      "loss: 0.128297  [2500/4210]\n",
      "In epoch 9 _ batch 2500 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.548318 \n",
      "\n",
      "loss: 0.030007  [2510/4210]\n",
      "loss: 0.205265  [2520/4210]\n",
      "loss: 0.074995  [2530/4210]\n",
      "loss: 0.153497  [2540/4210]\n",
      "loss: 0.205915  [2550/4210]\n",
      "loss: 0.096701  [2560/4210]\n",
      "loss: 0.064379  [2570/4210]\n",
      "loss: 0.038915  [2580/4210]\n",
      "loss: 0.105003  [2590/4210]\n",
      "loss: 0.349299  [2600/4210]\n",
      "In epoch 9 _ batch 2600 testing\n",
      "Test: \n",
      " Accuracy: 80.8%, Avg loss: 0.557879 \n",
      "\n",
      "loss: 0.036749  [2610/4210]\n",
      "loss: 0.119767  [2620/4210]\n",
      "loss: 0.329749  [2630/4210]\n",
      "loss: 0.222865  [2640/4210]\n",
      "loss: 0.113925  [2650/4210]\n",
      "loss: 0.084557  [2660/4210]\n",
      "loss: 0.277556  [2670/4210]\n",
      "loss: 0.184058  [2680/4210]\n",
      "loss: 0.081211  [2690/4210]\n",
      "loss: 0.232931  [2700/4210]\n",
      "In epoch 9 _ batch 2700 testing\n",
      "Test: \n",
      " Accuracy: 80.3%, Avg loss: 0.567729 \n",
      "\n",
      "loss: 0.154975  [2710/4210]\n",
      "loss: 0.379364  [2720/4210]\n",
      "loss: 0.078384  [2730/4210]\n",
      "loss: 0.250787  [2740/4210]\n",
      "loss: 0.242938  [2750/4210]\n",
      "loss: 0.180341  [2760/4210]\n",
      "loss: 0.075787  [2770/4210]\n",
      "loss: 0.070745  [2780/4210]\n",
      "loss: 0.263240  [2790/4210]\n",
      "loss: 0.196525  [2800/4210]\n",
      "In epoch 9 _ batch 2800 testing\n",
      "Test: \n",
      " Accuracy: 80.3%, Avg loss: 0.564221 \n",
      "\n",
      "loss: 0.253364  [2810/4210]\n",
      "loss: 0.017370  [2820/4210]\n",
      "loss: 0.125756  [2830/4210]\n",
      "loss: 0.104799  [2840/4210]\n",
      "loss: 0.143529  [2850/4210]\n",
      "loss: 0.112465  [2860/4210]\n",
      "loss: 0.491669  [2870/4210]\n",
      "loss: 0.177067  [2880/4210]\n",
      "loss: 0.011648  [2890/4210]\n",
      "loss: 0.107898  [2900/4210]\n",
      "In epoch 9 _ batch 2900 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.531267 \n",
      "\n",
      "loss: 0.151219  [2910/4210]\n",
      "loss: 0.205254  [2920/4210]\n",
      "loss: 0.133320  [2930/4210]\n",
      "loss: 0.164385  [2940/4210]\n",
      "loss: 0.101537  [2950/4210]\n",
      "loss: 0.202170  [2960/4210]\n",
      "loss: 0.112533  [2970/4210]\n",
      "loss: 0.290584  [2980/4210]\n",
      "loss: 0.046593  [2990/4210]\n",
      "loss: 0.106975  [3000/4210]\n",
      "In epoch 9 _ batch 3000 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.536992 \n",
      "\n",
      "loss: 0.243945  [3010/4210]\n",
      "loss: 0.152388  [3020/4210]\n",
      "loss: 0.489379  [3030/4210]\n",
      "loss: 0.667750  [3040/4210]\n",
      "loss: 0.084371  [3050/4210]\n",
      "loss: 0.119179  [3060/4210]\n",
      "loss: 0.037264  [3070/4210]\n",
      "loss: 0.128336  [3080/4210]\n",
      "loss: 0.140058  [3090/4210]\n",
      "loss: 0.268390  [3100/4210]\n",
      "In epoch 9 _ batch 3100 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.546682 \n",
      "\n",
      "loss: 0.554200  [3110/4210]\n",
      "loss: 0.184950  [3120/4210]\n",
      "loss: 0.044058  [3130/4210]\n",
      "loss: 0.023368  [3140/4210]\n",
      "loss: 0.175865  [3150/4210]\n",
      "loss: 0.077954  [3160/4210]\n",
      "loss: 0.225859  [3170/4210]\n",
      "loss: 0.625187  [3180/4210]\n",
      "loss: 0.193431  [3190/4210]\n",
      "loss: 0.086716  [3200/4210]\n",
      "In epoch 9 _ batch 3200 testing\n",
      "Test: \n",
      " Accuracy: 81.3%, Avg loss: 0.538576 \n",
      "\n",
      "loss: 0.137881  [3210/4210]\n",
      "loss: 0.060691  [3220/4210]\n",
      "loss: 0.164800  [3230/4210]\n",
      "loss: 0.111049  [3240/4210]\n",
      "loss: 0.201583  [3250/4210]\n",
      "loss: 0.587982  [3260/4210]\n",
      "loss: 0.119078  [3270/4210]\n",
      "loss: 0.091097  [3280/4210]\n",
      "loss: 0.147351  [3290/4210]\n",
      "loss: 0.479113  [3300/4210]\n",
      "In epoch 9 _ batch 3300 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.525375 \n",
      "\n",
      "loss: 0.154093  [3310/4210]\n",
      "loss: 0.215675  [3320/4210]\n",
      "loss: 0.156903  [3330/4210]\n",
      "loss: 0.251010  [3340/4210]\n",
      "loss: 0.157455  [3350/4210]\n",
      "loss: 0.243149  [3360/4210]\n",
      "loss: 0.155854  [3370/4210]\n",
      "loss: 0.365821  [3380/4210]\n",
      "loss: 0.144715  [3390/4210]\n",
      "loss: 0.216409  [3400/4210]\n",
      "In epoch 9 _ batch 3400 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.534070 \n",
      "\n",
      "loss: 0.101162  [3410/4210]\n",
      "loss: 0.110067  [3420/4210]\n",
      "loss: 0.125423  [3430/4210]\n",
      "loss: 0.020855  [3440/4210]\n",
      "loss: 0.071480  [3450/4210]\n",
      "loss: 0.038513  [3460/4210]\n",
      "loss: 0.040313  [3470/4210]\n",
      "loss: 0.109359  [3480/4210]\n",
      "loss: 0.060749  [3490/4210]\n",
      "loss: 0.107941  [3500/4210]\n",
      "In epoch 9 _ batch 3500 testing\n",
      "Test: \n",
      " Accuracy: 81.0%, Avg loss: 0.539546 \n",
      "\n",
      "loss: 0.209787  [3510/4210]\n",
      "loss: 0.391826  [3520/4210]\n",
      "loss: 0.115710  [3530/4210]\n",
      "loss: 0.217764  [3540/4210]\n",
      "loss: 0.188644  [3550/4210]\n",
      "loss: 0.131114  [3560/4210]\n",
      "loss: 0.224545  [3570/4210]\n",
      "loss: 0.285162  [3580/4210]\n",
      "loss: 0.056579  [3590/4210]\n",
      "loss: 0.233852  [3600/4210]\n",
      "In epoch 9 _ batch 3600 testing\n",
      "Test: \n",
      " Accuracy: 82.5%, Avg loss: 0.536528 \n",
      "\n",
      "loss: 0.153628  [3610/4210]\n",
      "loss: 0.300578  [3620/4210]\n",
      "loss: 0.207585  [3630/4210]\n",
      "loss: 0.391596  [3640/4210]\n",
      "loss: 0.173391  [3650/4210]\n",
      "loss: 0.033652  [3660/4210]\n",
      "loss: 0.269986  [3670/4210]\n",
      "loss: 0.080640  [3680/4210]\n",
      "loss: 0.327731  [3690/4210]\n",
      "loss: 0.119856  [3700/4210]\n",
      "In epoch 9 _ batch 3700 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.553684 \n",
      "\n",
      "loss: 0.376928  [3710/4210]\n",
      "loss: 0.063235  [3720/4210]\n",
      "loss: 0.208076  [3730/4210]\n",
      "loss: 0.119070  [3740/4210]\n",
      "loss: 0.224804  [3750/4210]\n",
      "loss: 0.067348  [3760/4210]\n",
      "loss: 0.145134  [3770/4210]\n",
      "loss: 0.182432  [3780/4210]\n",
      "loss: 0.524024  [3790/4210]\n",
      "loss: 0.126553  [3800/4210]\n",
      "In epoch 9 _ batch 3800 testing\n",
      "Test: \n",
      " Accuracy: 80.4%, Avg loss: 0.562650 \n",
      "\n",
      "loss: 0.196498  [3810/4210]\n",
      "loss: 0.176525  [3820/4210]\n",
      "loss: 0.169904  [3830/4210]\n",
      "loss: 0.189230  [3840/4210]\n",
      "loss: 0.186197  [3850/4210]\n",
      "loss: 0.212441  [3860/4210]\n",
      "loss: 0.116549  [3870/4210]\n",
      "loss: 0.091139  [3880/4210]\n",
      "loss: 0.316949  [3890/4210]\n",
      "loss: 0.176348  [3900/4210]\n",
      "In epoch 9 _ batch 3900 testing\n",
      "Test: \n",
      " Accuracy: 82.3%, Avg loss: 0.547708 \n",
      "\n",
      "loss: 0.241290  [3910/4210]\n",
      "loss: 0.376455  [3920/4210]\n",
      "loss: 0.080703  [3930/4210]\n",
      "loss: 0.323621  [3940/4210]\n",
      "loss: 0.118884  [3950/4210]\n",
      "loss: 0.043766  [3960/4210]\n",
      "loss: 0.155432  [3970/4210]\n",
      "loss: 0.043869  [3980/4210]\n",
      "loss: 0.115298  [3990/4210]\n",
      "loss: 0.485874  [4000/4210]\n",
      "In epoch 9 _ batch 4000 testing\n",
      "Test: \n",
      " Accuracy: 80.7%, Avg loss: 0.546991 \n",
      "\n",
      "loss: 0.223420  [4010/4210]\n",
      "loss: 0.382344  [4020/4210]\n",
      "loss: 0.103301  [4030/4210]\n",
      "loss: 0.104274  [4040/4210]\n",
      "loss: 0.078171  [4050/4210]\n",
      "loss: 0.107630  [4060/4210]\n",
      "loss: 0.040844  [4070/4210]\n",
      "loss: 0.242305  [4080/4210]\n",
      "loss: 0.331005  [4090/4210]\n",
      "loss: 0.063345  [4100/4210]\n",
      "In epoch 9 _ batch 4100 testing\n",
      "Test: \n",
      " Accuracy: 80.7%, Avg loss: 0.548276 \n",
      "\n",
      "loss: 0.785931  [4110/4210]\n",
      "loss: 0.120729  [4120/4210]\n",
      "loss: 0.368629  [4130/4210]\n",
      "loss: 0.124756  [4140/4210]\n",
      "loss: 0.111986  [4150/4210]\n",
      "loss: 0.064298  [4160/4210]\n",
      "loss: 0.155962  [4170/4210]\n",
      "loss: 0.068245  [4180/4210]\n",
      "loss: 0.044402  [4190/4210]\n",
      "loss: 0.127403  [4200/4210]\n",
      "In epoch 9 _ batch 4200 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.541265 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.197162  [  0/4210]\n",
      "In epoch 10 _ batch 0 testing\n",
      "Test: \n",
      " Accuracy: 80.7%, Avg loss: 0.546456 \n",
      "\n",
      "loss: 0.113543  [ 10/4210]\n",
      "loss: 0.083472  [ 20/4210]\n",
      "loss: 0.135310  [ 30/4210]\n",
      "loss: 0.216605  [ 40/4210]\n",
      "loss: 0.383208  [ 50/4210]\n",
      "loss: 0.353773  [ 60/4210]\n",
      "loss: 0.087290  [ 70/4210]\n",
      "loss: 0.255487  [ 80/4210]\n",
      "loss: 0.054650  [ 90/4210]\n",
      "loss: 0.146686  [100/4210]\n",
      "In epoch 10 _ batch 100 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.533497 \n",
      "\n",
      "loss: 0.146017  [110/4210]\n",
      "loss: 0.061526  [120/4210]\n",
      "loss: 0.026300  [130/4210]\n",
      "loss: 0.440612  [140/4210]\n",
      "loss: 0.126634  [150/4210]\n",
      "loss: 0.099380  [160/4210]\n",
      "loss: 0.348152  [170/4210]\n",
      "loss: 0.198329  [180/4210]\n",
      "loss: 0.198716  [190/4210]\n",
      "loss: 0.637175  [200/4210]\n",
      "In epoch 10 _ batch 200 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.549116 \n",
      "\n",
      "loss: 0.056933  [210/4210]\n",
      "loss: 0.021603  [220/4210]\n",
      "loss: 0.381307  [230/4210]\n",
      "loss: 0.196906  [240/4210]\n",
      "loss: 0.292495  [250/4210]\n",
      "loss: 0.077146  [260/4210]\n",
      "loss: 0.119227  [270/4210]\n",
      "loss: 0.173309  [280/4210]\n",
      "loss: 0.096645  [290/4210]\n",
      "loss: 0.111171  [300/4210]\n",
      "In epoch 10 _ batch 300 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.540598 \n",
      "\n",
      "loss: 0.264228  [310/4210]\n",
      "loss: 0.099127  [320/4210]\n",
      "loss: 0.608751  [330/4210]\n",
      "loss: 0.143000  [340/4210]\n",
      "loss: 0.128702  [350/4210]\n",
      "loss: 0.340124  [360/4210]\n",
      "loss: 0.188137  [370/4210]\n",
      "loss: 0.534892  [380/4210]\n",
      "loss: 0.177100  [390/4210]\n",
      "loss: 0.063453  [400/4210]\n",
      "In epoch 10 _ batch 400 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.548228 \n",
      "\n",
      "loss: 0.184721  [410/4210]\n",
      "loss: 0.105446  [420/4210]\n",
      "loss: 0.117982  [430/4210]\n",
      "loss: 0.083324  [440/4210]\n",
      "loss: 0.278264  [450/4210]\n",
      "loss: 0.405519  [460/4210]\n",
      "loss: 0.071707  [470/4210]\n",
      "loss: 0.150681  [480/4210]\n",
      "loss: 0.160801  [490/4210]\n",
      "loss: 0.146044  [500/4210]\n",
      "In epoch 10 _ batch 500 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.551841 \n",
      "\n",
      "loss: 0.051041  [510/4210]\n",
      "loss: 0.136485  [520/4210]\n",
      "loss: 0.131352  [530/4210]\n",
      "loss: 0.041489  [540/4210]\n",
      "loss: 0.061970  [550/4210]\n",
      "loss: 0.195283  [560/4210]\n",
      "loss: 0.256573  [570/4210]\n",
      "loss: 0.261798  [580/4210]\n",
      "loss: 0.112085  [590/4210]\n",
      "loss: 0.084508  [600/4210]\n",
      "In epoch 10 _ batch 600 testing\n",
      "Test: \n",
      " Accuracy: 81.5%, Avg loss: 0.555914 \n",
      "\n",
      "loss: 0.084275  [610/4210]\n",
      "loss: 0.077319  [620/4210]\n",
      "loss: 0.074122  [630/4210]\n",
      "loss: 0.164624  [640/4210]\n",
      "loss: 0.064866  [650/4210]\n",
      "loss: 0.085688  [660/4210]\n",
      "loss: 0.328971  [670/4210]\n",
      "loss: 0.262671  [680/4210]\n",
      "loss: 0.228356  [690/4210]\n",
      "loss: 0.187606  [700/4210]\n",
      "In epoch 10 _ batch 700 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.562044 \n",
      "\n",
      "loss: 0.201667  [710/4210]\n",
      "loss: 0.172390  [720/4210]\n",
      "loss: 0.167479  [730/4210]\n",
      "loss: 0.066188  [740/4210]\n",
      "loss: 0.331069  [750/4210]\n",
      "loss: 0.019786  [760/4210]\n",
      "loss: 0.244885  [770/4210]\n",
      "loss: 0.124441  [780/4210]\n",
      "loss: 0.035847  [790/4210]\n",
      "loss: 0.076712  [800/4210]\n",
      "In epoch 10 _ batch 800 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.554607 \n",
      "\n",
      "loss: 0.299715  [810/4210]\n",
      "loss: 0.050205  [820/4210]\n",
      "loss: 0.247814  [830/4210]\n",
      "loss: 0.322708  [840/4210]\n",
      "loss: 0.095205  [850/4210]\n",
      "loss: 0.060733  [860/4210]\n",
      "loss: 0.047895  [870/4210]\n",
      "loss: 0.138053  [880/4210]\n",
      "loss: 0.057657  [890/4210]\n",
      "loss: 0.085027  [900/4210]\n",
      "In epoch 10 _ batch 900 testing\n",
      "Test: \n",
      " Accuracy: 82.3%, Avg loss: 0.561600 \n",
      "\n",
      "loss: 0.065394  [910/4210]\n",
      "loss: 0.061690  [920/4210]\n",
      "loss: 0.139620  [930/4210]\n",
      "loss: 0.216838  [940/4210]\n",
      "loss: 0.078736  [950/4210]\n",
      "loss: 0.115828  [960/4210]\n",
      "loss: 0.218901  [970/4210]\n",
      "loss: 0.447085  [980/4210]\n",
      "loss: 0.022607  [990/4210]\n",
      "loss: 0.139395  [1000/4210]\n",
      "In epoch 10 _ batch 1000 testing\n",
      "Test: \n",
      " Accuracy: 82.3%, Avg loss: 0.563822 \n",
      "\n",
      "loss: 0.065270  [1010/4210]\n",
      "loss: 0.108911  [1020/4210]\n",
      "loss: 0.136901  [1030/4210]\n",
      "loss: 0.517577  [1040/4210]\n",
      "loss: 0.150456  [1050/4210]\n",
      "loss: 0.481085  [1060/4210]\n",
      "loss: 0.066852  [1070/4210]\n",
      "loss: 0.085411  [1080/4210]\n",
      "loss: 0.153047  [1090/4210]\n",
      "loss: 0.127066  [1100/4210]\n",
      "In epoch 10 _ batch 1100 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.562189 \n",
      "\n",
      "loss: 0.065215  [1110/4210]\n",
      "loss: 0.171808  [1120/4210]\n",
      "loss: 0.063518  [1130/4210]\n",
      "loss: 0.121622  [1140/4210]\n",
      "loss: 0.597952  [1150/4210]\n",
      "loss: 0.230773  [1160/4210]\n",
      "loss: 0.117517  [1170/4210]\n",
      "loss: 0.121394  [1180/4210]\n",
      "loss: 0.027230  [1190/4210]\n",
      "loss: 0.639356  [1200/4210]\n",
      "In epoch 10 _ batch 1200 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.556810 \n",
      "\n",
      "loss: 0.076405  [1210/4210]\n",
      "loss: 0.017173  [1220/4210]\n",
      "loss: 0.091979  [1230/4210]\n",
      "loss: 0.176629  [1240/4210]\n",
      "loss: 0.183715  [1250/4210]\n",
      "loss: 0.062542  [1260/4210]\n",
      "loss: 0.299692  [1270/4210]\n",
      "loss: 0.117658  [1280/4210]\n",
      "loss: 0.099999  [1290/4210]\n",
      "loss: 0.148734  [1300/4210]\n",
      "In epoch 10 _ batch 1300 testing\n",
      "Test: \n",
      " Accuracy: 82.3%, Avg loss: 0.554156 \n",
      "\n",
      "loss: 0.046484  [1310/4210]\n",
      "loss: 0.078521  [1320/4210]\n",
      "loss: 0.107201  [1330/4210]\n",
      "loss: 0.156604  [1340/4210]\n",
      "loss: 0.148022  [1350/4210]\n",
      "loss: 0.246705  [1360/4210]\n",
      "loss: 0.105052  [1370/4210]\n",
      "loss: 0.087918  [1380/4210]\n",
      "loss: 0.463818  [1390/4210]\n",
      "loss: 0.212668  [1400/4210]\n",
      "In epoch 10 _ batch 1400 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.552185 \n",
      "\n",
      "loss: 0.376718  [1410/4210]\n",
      "loss: 0.443857  [1420/4210]\n",
      "loss: 0.031225  [1430/4210]\n",
      "loss: 0.339805  [1440/4210]\n",
      "loss: 0.188421  [1450/4210]\n",
      "loss: 0.256249  [1460/4210]\n",
      "loss: 0.223184  [1470/4210]\n",
      "loss: 0.086692  [1480/4210]\n",
      "loss: 0.179219  [1490/4210]\n",
      "loss: 0.346440  [1500/4210]\n",
      "In epoch 10 _ batch 1500 testing\n",
      "Test: \n",
      " Accuracy: 82.6%, Avg loss: 0.533170 \n",
      "\n",
      "loss: 0.274633  [1510/4210]\n",
      "loss: 0.293434  [1520/4210]\n",
      "loss: 0.182523  [1530/4210]\n",
      "loss: 0.115072  [1540/4210]\n",
      "loss: 0.166424  [1550/4210]\n",
      "loss: 0.231791  [1560/4210]\n",
      "loss: 0.062128  [1570/4210]\n",
      "loss: 0.081116  [1580/4210]\n",
      "loss: 0.185978  [1590/4210]\n",
      "loss: 0.022142  [1600/4210]\n",
      "In epoch 10 _ batch 1600 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.529607 \n",
      "\n",
      "loss: 0.105610  [1610/4210]\n",
      "loss: 0.530591  [1620/4210]\n",
      "loss: 0.052946  [1630/4210]\n",
      "loss: 0.190551  [1640/4210]\n",
      "loss: 0.164309  [1650/4210]\n",
      "loss: 0.287832  [1660/4210]\n",
      "loss: 0.239453  [1670/4210]\n",
      "loss: 0.036290  [1680/4210]\n",
      "loss: 0.630227  [1690/4210]\n",
      "loss: 0.032217  [1700/4210]\n",
      "In epoch 10 _ batch 1700 testing\n",
      "Test: \n",
      " Accuracy: 81.2%, Avg loss: 0.539892 \n",
      "\n",
      "loss: 0.044143  [1710/4210]\n",
      "loss: 0.267134  [1720/4210]\n",
      "loss: 0.117511  [1730/4210]\n",
      "loss: 0.408719  [1740/4210]\n",
      "loss: 0.094194  [1750/4210]\n",
      "loss: 0.318938  [1760/4210]\n",
      "loss: 0.081753  [1770/4210]\n",
      "loss: 0.091482  [1780/4210]\n",
      "loss: 0.227437  [1790/4210]\n",
      "loss: 0.227089  [1800/4210]\n",
      "In epoch 10 _ batch 1800 testing\n",
      "Test: \n",
      " Accuracy: 81.1%, Avg loss: 0.548759 \n",
      "\n",
      "loss: 0.091246  [1810/4210]\n",
      "loss: 0.293422  [1820/4210]\n",
      "loss: 0.015668  [1830/4210]\n",
      "loss: 0.127645  [1840/4210]\n",
      "loss: 0.073957  [1850/4210]\n",
      "loss: 0.162863  [1860/4210]\n",
      "loss: 0.096119  [1870/4210]\n",
      "loss: 0.147782  [1880/4210]\n",
      "loss: 0.066843  [1890/4210]\n",
      "loss: 0.119187  [1900/4210]\n",
      "In epoch 10 _ batch 1900 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.538495 \n",
      "\n",
      "loss: 0.219866  [1910/4210]\n",
      "loss: 0.185897  [1920/4210]\n",
      "loss: 0.117416  [1930/4210]\n",
      "loss: 0.141862  [1940/4210]\n",
      "loss: 0.153401  [1950/4210]\n",
      "loss: 0.049205  [1960/4210]\n",
      "loss: 0.088357  [1970/4210]\n",
      "loss: 0.224569  [1980/4210]\n",
      "loss: 0.052656  [1990/4210]\n",
      "loss: 0.122001  [2000/4210]\n",
      "In epoch 10 _ batch 2000 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.546947 \n",
      "\n",
      "loss: 0.105171  [2010/4210]\n",
      "loss: 0.334650  [2020/4210]\n",
      "loss: 0.096024  [2030/4210]\n",
      "loss: 0.044372  [2040/4210]\n",
      "loss: 0.312591  [2050/4210]\n",
      "loss: 0.141177  [2060/4210]\n",
      "loss: 0.456343  [2070/4210]\n",
      "loss: 0.125468  [2080/4210]\n",
      "loss: 0.255458  [2090/4210]\n",
      "loss: 0.585943  [2100/4210]\n",
      "In epoch 10 _ batch 2100 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.533195 \n",
      "\n",
      "loss: 0.255098  [2110/4210]\n",
      "loss: 0.037134  [2120/4210]\n",
      "loss: 0.192872  [2130/4210]\n",
      "loss: 0.021944  [2140/4210]\n",
      "loss: 0.308972  [2150/4210]\n",
      "loss: 0.049851  [2160/4210]\n",
      "loss: 0.049165  [2170/4210]\n",
      "loss: 0.033854  [2180/4210]\n",
      "loss: 0.150091  [2190/4210]\n",
      "loss: 0.052708  [2200/4210]\n",
      "In epoch 10 _ batch 2200 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.545850 \n",
      "\n",
      "loss: 0.240374  [2210/4210]\n",
      "loss: 0.077041  [2220/4210]\n",
      "loss: 0.208923  [2230/4210]\n",
      "loss: 0.086655  [2240/4210]\n",
      "loss: 0.181371  [2250/4210]\n",
      "loss: 0.152774  [2260/4210]\n",
      "loss: 0.177541  [2270/4210]\n",
      "loss: 0.192878  [2280/4210]\n",
      "loss: 0.180780  [2290/4210]\n",
      "loss: 0.144966  [2300/4210]\n",
      "In epoch 10 _ batch 2300 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.554979 \n",
      "\n",
      "loss: 0.120943  [2310/4210]\n",
      "loss: 0.055693  [2320/4210]\n",
      "loss: 0.141738  [2330/4210]\n",
      "loss: 0.122429  [2340/4210]\n",
      "loss: 0.004227  [2350/4210]\n",
      "loss: 0.136461  [2360/4210]\n",
      "loss: 0.089459  [2370/4210]\n",
      "loss: 0.147843  [2380/4210]\n",
      "loss: 0.290032  [2390/4210]\n",
      "loss: 0.122263  [2400/4210]\n",
      "In epoch 10 _ batch 2400 testing\n",
      "Test: \n",
      " Accuracy: 81.4%, Avg loss: 0.550283 \n",
      "\n",
      "loss: 0.150697  [2410/4210]\n",
      "loss: 0.064915  [2420/4210]\n",
      "loss: 0.084349  [2430/4210]\n",
      "loss: 0.076106  [2440/4210]\n",
      "loss: 0.465578  [2450/4210]\n",
      "loss: 0.150789  [2460/4210]\n",
      "loss: 0.089811  [2470/4210]\n",
      "loss: 0.145085  [2480/4210]\n",
      "loss: 0.132114  [2490/4210]\n",
      "loss: 0.335346  [2500/4210]\n",
      "In epoch 10 _ batch 2500 testing\n",
      "Test: \n",
      " Accuracy: 82.2%, Avg loss: 0.548017 \n",
      "\n",
      "loss: 0.168313  [2510/4210]\n",
      "loss: 0.078331  [2520/4210]\n",
      "loss: 0.076291  [2530/4210]\n",
      "loss: 0.021914  [2540/4210]\n",
      "loss: 0.091564  [2550/4210]\n",
      "loss: 0.184267  [2560/4210]\n",
      "loss: 0.064425  [2570/4210]\n",
      "loss: 0.366669  [2580/4210]\n",
      "loss: 0.219784  [2590/4210]\n",
      "loss: 0.102247  [2600/4210]\n",
      "In epoch 10 _ batch 2600 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.547117 \n",
      "\n",
      "loss: 0.226757  [2610/4210]\n",
      "loss: 0.171386  [2620/4210]\n",
      "loss: 0.186512  [2630/4210]\n",
      "loss: 0.206455  [2640/4210]\n",
      "loss: 0.113386  [2650/4210]\n",
      "loss: 0.530075  [2660/4210]\n",
      "loss: 0.262497  [2670/4210]\n",
      "loss: 0.040132  [2680/4210]\n",
      "loss: 0.359530  [2690/4210]\n",
      "loss: 0.075321  [2700/4210]\n",
      "In epoch 10 _ batch 2700 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.545998 \n",
      "\n",
      "loss: 0.052633  [2710/4210]\n",
      "loss: 0.035293  [2720/4210]\n",
      "loss: 0.350556  [2730/4210]\n",
      "loss: 0.213853  [2740/4210]\n",
      "loss: 0.260977  [2750/4210]\n",
      "loss: 0.234957  [2760/4210]\n",
      "loss: 0.150960  [2770/4210]\n",
      "loss: 0.112736  [2780/4210]\n",
      "loss: 0.146659  [2790/4210]\n",
      "loss: 0.061825  [2800/4210]\n",
      "In epoch 10 _ batch 2800 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.554747 \n",
      "\n",
      "loss: 0.179759  [2810/4210]\n",
      "loss: 0.095477  [2820/4210]\n",
      "loss: 0.026345  [2830/4210]\n",
      "loss: 0.147560  [2840/4210]\n",
      "loss: 0.150526  [2850/4210]\n",
      "loss: 0.058585  [2860/4210]\n",
      "loss: 0.107513  [2870/4210]\n",
      "loss: 0.401248  [2880/4210]\n",
      "loss: 0.073900  [2890/4210]\n",
      "loss: 0.141938  [2900/4210]\n",
      "In epoch 10 _ batch 2900 testing\n",
      "Test: \n",
      " Accuracy: 82.0%, Avg loss: 0.555824 \n",
      "\n",
      "loss: 0.096433  [2910/4210]\n",
      "loss: 0.254813  [2920/4210]\n",
      "loss: 0.081348  [2930/4210]\n",
      "loss: 0.030851  [2940/4210]\n",
      "loss: 0.276882  [2950/4210]\n",
      "loss: 0.161278  [2960/4210]\n",
      "loss: 0.061866  [2970/4210]\n",
      "loss: 0.009084  [2980/4210]\n",
      "loss: 0.237747  [2990/4210]\n",
      "loss: 0.370848  [3000/4210]\n",
      "In epoch 10 _ batch 3000 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.552145 \n",
      "\n",
      "loss: 0.018959  [3010/4210]\n",
      "loss: 0.285835  [3020/4210]\n",
      "loss: 0.446855  [3030/4210]\n",
      "loss: 0.082131  [3040/4210]\n",
      "loss: 0.230387  [3050/4210]\n",
      "loss: 0.063546  [3060/4210]\n",
      "loss: 0.090181  [3070/4210]\n",
      "loss: 0.273762  [3080/4210]\n",
      "loss: 0.118814  [3090/4210]\n",
      "loss: 0.185699  [3100/4210]\n",
      "In epoch 10 _ batch 3100 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.555650 \n",
      "\n",
      "loss: 0.324258  [3110/4210]\n",
      "loss: 0.150839  [3120/4210]\n",
      "loss: 0.206252  [3130/4210]\n",
      "loss: 0.166428  [3140/4210]\n",
      "loss: 0.048149  [3150/4210]\n",
      "loss: 0.218181  [3160/4210]\n",
      "loss: 0.143247  [3170/4210]\n",
      "loss: 0.097428  [3180/4210]\n",
      "loss: 0.088739  [3190/4210]\n",
      "loss: 0.038736  [3200/4210]\n",
      "In epoch 10 _ batch 3200 testing\n",
      "Test: \n",
      " Accuracy: 81.8%, Avg loss: 0.558417 \n",
      "\n",
      "loss: 0.088713  [3210/4210]\n",
      "loss: 0.018374  [3220/4210]\n",
      "loss: 0.095532  [3230/4210]\n",
      "loss: 0.065767  [3240/4210]\n",
      "loss: 0.049805  [3250/4210]\n",
      "loss: 0.158644  [3260/4210]\n",
      "loss: 0.193317  [3270/4210]\n",
      "loss: 0.252128  [3280/4210]\n",
      "loss: 0.224702  [3290/4210]\n",
      "loss: 0.089503  [3300/4210]\n",
      "In epoch 10 _ batch 3300 testing\n",
      "Test: \n",
      " Accuracy: 81.7%, Avg loss: 0.549407 \n",
      "\n",
      "loss: 0.179821  [3310/4210]\n",
      "loss: 0.092458  [3320/4210]\n",
      "loss: 0.118611  [3330/4210]\n",
      "loss: 0.122315  [3340/4210]\n",
      "loss: 0.136292  [3350/4210]\n",
      "loss: 0.159561  [3360/4210]\n",
      "loss: 0.370209  [3370/4210]\n",
      "loss: 0.201109  [3380/4210]\n",
      "loss: 0.145720  [3390/4210]\n",
      "loss: 0.287499  [3400/4210]\n",
      "In epoch 10 _ batch 3400 testing\n",
      "Test: \n",
      " Accuracy: 81.9%, Avg loss: 0.549542 \n",
      "\n",
      "loss: 0.107257  [3410/4210]\n",
      "loss: 0.090158  [3420/4210]\n",
      "loss: 0.094851  [3430/4210]\n",
      "loss: 0.225009  [3440/4210]\n",
      "loss: 0.140175  [3450/4210]\n",
      "loss: 0.056566  [3460/4210]\n",
      "loss: 0.062563  [3470/4210]\n",
      "loss: 0.262974  [3480/4210]\n",
      "loss: 0.075543  [3490/4210]\n",
      "loss: 0.094355  [3500/4210]\n",
      "In epoch 10 _ batch 3500 testing\n",
      "Test: \n",
      " Accuracy: 82.5%, Avg loss: 0.545455 \n",
      "\n",
      "loss: 0.176634  [3510/4210]\n",
      "loss: 0.325127  [3520/4210]\n",
      "loss: 0.213726  [3530/4210]\n",
      "loss: 0.160880  [3540/4210]\n",
      "loss: 0.083184  [3550/4210]\n",
      "loss: 0.061641  [3560/4210]\n",
      "loss: 0.079649  [3570/4210]\n",
      "loss: 0.189211  [3580/4210]\n",
      "loss: 0.126590  [3590/4210]\n",
      "loss: 0.082748  [3600/4210]\n",
      "In epoch 10 _ batch 3600 testing\n",
      "Test: \n",
      " Accuracy: 82.1%, Avg loss: 0.552403 \n",
      "\n",
      "loss: 0.215617  [3610/4210]\n",
      "loss: 0.254344  [3620/4210]\n",
      "loss: 0.259426  [3630/4210]\n",
      "loss: 0.353812  [3640/4210]\n",
      "loss: 0.231484  [3650/4210]\n",
      "loss: 0.330742  [3660/4210]\n",
      "loss: 0.146503  [3670/4210]\n",
      "loss: 0.450347  [3680/4210]\n",
      "loss: 0.151808  [3690/4210]\n",
      "loss: 0.196908  [3700/4210]\n",
      "In epoch 10 _ batch 3700 testing\n",
      "Test: \n",
      " Accuracy: 82.3%, Avg loss: 0.551197 \n",
      "\n",
      "loss: 0.070248  [3710/4210]\n",
      "loss: 0.279253  [3720/4210]\n",
      "loss: 0.131723  [3730/4210]\n",
      "loss: 0.208819  [3740/4210]\n",
      "loss: 0.183666  [3750/4210]\n",
      "loss: 0.256223  [3760/4210]\n",
      "loss: 0.185480  [3770/4210]\n",
      "loss: 0.375445  [3780/4210]\n",
      "loss: 0.046343  [3790/4210]\n",
      "loss: 0.091637  [3800/4210]\n",
      "In epoch 10 _ batch 3800 testing\n",
      "Test: \n",
      " Accuracy: 82.3%, Avg loss: 0.555641 \n",
      "\n",
      "loss: 0.142459  [3810/4210]\n",
      "loss: 0.147514  [3820/4210]\n",
      "loss: 0.312678  [3830/4210]\n",
      "loss: 0.090145  [3840/4210]\n",
      "loss: 0.320396  [3850/4210]\n",
      "loss: 0.091439  [3860/4210]\n",
      "loss: 0.680388  [3870/4210]\n",
      "loss: 0.272893  [3880/4210]\n",
      "loss: 0.234539  [3890/4210]\n",
      "loss: 0.107103  [3900/4210]\n",
      "In epoch 10 _ batch 3900 testing\n",
      "Test: \n",
      " Accuracy: 82.3%, Avg loss: 0.547517 \n",
      "\n",
      "loss: 0.066028  [3910/4210]\n",
      "loss: 0.087182  [3920/4210]\n",
      "loss: 0.149188  [3930/4210]\n",
      "loss: 0.116206  [3940/4210]\n",
      "loss: 0.261524  [3950/4210]\n",
      "loss: 0.070537  [3960/4210]\n",
      "loss: 0.631677  [3970/4210]\n",
      "loss: 0.079290  [3980/4210]\n",
      "loss: 0.065436  [3990/4210]\n",
      "loss: 0.095908  [4000/4210]\n",
      "In epoch 10 _ batch 4000 testing\n",
      "Test: \n",
      " Accuracy: 82.5%, Avg loss: 0.551157 \n",
      "\n",
      "loss: 0.074663  [4010/4210]\n",
      "loss: 0.124645  [4020/4210]\n",
      "loss: 0.202610  [4030/4210]\n",
      "loss: 0.022877  [4040/4210]\n",
      "loss: 0.208317  [4050/4210]\n",
      "loss: 0.210797  [4060/4210]\n",
      "loss: 0.126266  [4070/4210]\n",
      "loss: 0.128689  [4080/4210]\n",
      "loss: 0.339887  [4090/4210]\n",
      "loss: 0.045100  [4100/4210]\n",
      "In epoch 10 _ batch 4100 testing\n",
      "Test: \n",
      " Accuracy: 82.5%, Avg loss: 0.549395 \n",
      "\n",
      "loss: 0.099644  [4110/4210]\n",
      "loss: 0.093320  [4120/4210]\n",
      "loss: 0.254248  [4130/4210]\n",
      "loss: 0.075738  [4140/4210]\n",
      "loss: 0.151618  [4150/4210]\n",
      "loss: 0.466536  [4160/4210]\n",
      "loss: 0.104439  [4170/4210]\n",
      "loss: 0.141175  [4180/4210]\n",
      "loss: 0.294581  [4190/4210]\n",
      "loss: 0.173115  [4200/4210]\n",
      "In epoch 10 _ batch 4200 testing\n",
      "Test: \n",
      " Accuracy: 82.5%, Avg loss: 0.549384 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_and_test(bert_sst_2, train_dataset, dev_dataset, optimizer, epoch)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('gmy_temp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70ebebcdeef06a312d77b32c45f125e7c769ee521d3b94b978c7aed3067c711d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
