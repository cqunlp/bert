{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import mindspore\n",
    "from mindspore import context\n",
    "from mindspore import nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import ops\n",
    "from src.bert import BertForPretraining\n",
    "from src.config import BertConfig\n",
    "from src.tokenizer import BertTokenizer\n",
    "from model import BertBinaryClassificationModel\n",
    "from mindnlp.mindnlp.mindtext.dataset.classification.sst import SST2Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST-2 dataset loadding: 100%|██████████| 872/872 [00:00<00:00, 13524.71it/s]\n",
      "SST-2 dataset loadding: 100%|██████████| 1821/1821 [00:00<00:00, 16425.93it/s]\n",
      "SST-2 dataset loadding: 100%|██████████| 67349/67349 [00:04<00:00, 16717.60it/s] \n",
      "SST-2 train dataset sentence preprocess bar(tokenize).: 100%|██████████| 67349/67349 [00:23<00:00, 2823.69it/s]\n",
      "SST-2 train dataset processing.: 100%|██████████| 67349/67349 [00:03<00:00, 17073.86it/s] \n",
      "[WARNING] ME(3260:140362055714560,MainProcess):2022-11-26-13:36:57.612.677 [mindspore/dataset/core/validator_helpers.py:806] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "Writing data to .mindrecord file: 100%|██████████| 67349/67349 [03:10<00:00, 353.66it/s]\n",
      "SST-2 dev dataset sentence preprocess bar(tokenize).: 100%|██████████| 872/872 [00:00<00:00, 1875.74it/s]\n",
      "SST-2 dev dataset processing.: 100%|██████████| 872/872 [00:00<00:00, 16211.02it/s]\n",
      "[WARNING] ME(3260:140362055714560,MainProcess):2022-11-26-13:40:09.467.863 [mindspore/dataset/core/validator_helpers.py:806] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "Writing data to .mindrecord file: 100%|██████████| 872/872 [00:02<00:00, 315.00it/s]\n",
      "SST-2 test dataset sentence preprocess bar(tokenize).: 100%|██████████| 1821/1821 [00:00<00:00, 1828.86it/s]\n",
      "SST-2 test dataset processing.: 100%|██████████| 1821/1821 [00:00<00:00, 13649.12it/s]\n",
      "[WARNING] ME(3260:140362055714560,MainProcess):2022-11-26-13:40:13.475.994 [mindspore/dataset/core/validator_helpers.py:806] 'TypeCast' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'TypeCast' from mindspore.dataset.transforms instead.\n",
      "Writing data to .mindrecord file: 100%|██████████| 1821/1821 [00:05<00:00, 346.85it/s]\n"
     ]
    }
   ],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE)\n",
    "#context.set_context(mode=context.PYNATIVE_MODE)\n",
    "\n",
    "\n",
    "dataset2 = SST2Dataset(paths='SST-2',\n",
    "                      tokenizer=\"bert-base-uncased\",\n",
    "                      max_length=512,\n",
    "                      truncation_strategy=True,\n",
    "                      columns_list=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
    "                      test_columns_list=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "                      batch_size=1)\n",
    "\n",
    "sst_2_ds2 = dataset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349\n",
      "1\n",
      "872\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = sst_2_ds2['train']\n",
    "dev_dataset = sst_2_ds2['dev']\n",
    "print(train_dataset.get_dataset_size())\n",
    "print(train_dataset.get_batch_size())\n",
    "\n",
    "print(dev_dataset.get_dataset_size())\n",
    "print(dev_dataset.get_batch_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存mindrecord\n",
    "train_dataset.save(file_name=\"sst-2-512/sst_2_train_data.mindrecord\",\n",
    "                   num_files=1,\n",
    "                   file_type='mindrecord')\n",
    "dev_dataset.save(file_name=\"sst-2-512/sst_2_test_data.mindrecord\",\n",
    "                   num_files=1,\n",
    "                   file_type='mindrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mindspore.dataset.engine.datasets_standard_format.MindDataset object at 0x7fa77cf055b0> 1 67349\n",
      "<mindspore.dataset.engine.datasets_standard_format.MindDataset object at 0x7fa890610bb0> 1 872\n"
     ]
    }
   ],
   "source": [
    "from mindspore import dataset\n",
    "d1 = dataset.MindDataset(dataset_files=\"sst-2-512/sst_2_train_data.mindrecord\")\n",
    "d2 = dataset.MindDataset(dataset_files=\"sst-2-512/sst_2_test_data.mindrecord\")\n",
    "print(d1, d1.get_batch_size(),d1.get_dataset_size())\n",
    "print(d2, d2.get_batch_size(),d2.get_dataset_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mindspore.dataset.engine.datasets.BatchDataset object at 0x7fa773daa820> 16 4210\n",
      "<mindspore.dataset.engine.datasets.BatchDataset object at 0x7fa773daa340> 16 55\n"
     ]
    }
   ],
   "source": [
    "d1 = d1.batch(16)\n",
    "d2 = d2.batch(16)\n",
    "print(d1, d1.get_batch_size(),d1.get_dataset_size())\n",
    "print(d2, d2.get_batch_size(),d2.get_dataset_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask', 'label']\n",
      "(16, 512)\n",
      "(16, 1, 512)\n",
      "(16, 1, 512)\n",
      "(16, 1, 1)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# batch and tokenizer -> (input_ids ,token_type_ids, attention_mask, label)\n",
    "print(train_dataset.get_col_names())\n",
    "# for data in train_dataset_1.create_dict_iterator():\n",
    "#     print(data['input_ids'].shape)\n",
    "#     print(data['attention_mask'].shape)\n",
    "#     print(data['token_type_ids'].shape)\n",
    "#     print(data['label'].shape)\n",
    "#     print(len(data))\n",
    "#     break\n",
    "for data in d1.create_dict_iterator():\n",
    "    print(data['input_ids'].squeeze(-2).shape)\n",
    "    print(data['attention_mask'].shape)\n",
    "    print(data['token_type_ids'].shape)\n",
    "    print(data['label'].shape)\n",
    "    print(len(data))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test load ckpt\n",
    "from src.config import BertConfig\n",
    "ckpt_file = \"checkpoint/bert_L4_H128_step_308749_card_id_0.ckpt\"\n",
    "config_file = 'config/bert_L4_H128_config.json'\n",
    "config = BertConfig(config_file)\n",
    "print(config.vocab_size)\n",
    "model = BertForPretraining(config)\n",
    "\n",
    "before_model = BertForPretraining(config)\n",
    "dict_before = before_model.bert.parameters_dict()\n",
    "\n",
    "dict_after = mindspore.load_checkpoint(ckpt_file)\n",
    "mindspore.load_param_into_net(model, dict_after)\n",
    "model = model.bert\n",
    "dict_after = model.parameters_dict()\n",
    "import numpy as np\n",
    "list_before = list(dict_before.values())\n",
    "list_after = list(dict_after.values())\n",
    "\n",
    "print(np.allclose(list_before[2].asnumpy(), list_after[2].asnumpy(), rtol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sst_2 = BertBinaryClassificationModel(config, ckpt_file)\n",
    "# test output \n",
    "\n",
    "for input_ids, token_type_ids, attention_mask, label in train_dataset.create_tuple_iterator():\n",
    "    print(len(input_ids))\n",
    "    loss, logits = bert_sst_2(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, label=label)\n",
    "    break\n",
    "\n",
    "for item in bert_sst_2.get_parameters():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def getpwd():\n",
    "    pwd = sys.path[0]\n",
    "    if os.path.isfile(pwd):\n",
    "        pwd = os.path.dirname(pwd)\n",
    "    return pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, dataset, optimizer):\n",
    "    def forward_fn(input_ids, attention_mask, token_type_ids, label):\n",
    "        loss, logits = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        label=label)\n",
    "        return loss, logits\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "    def train_step(input_ids, attention_mask, token_type_ids, label):\n",
    "        (loss, _), grads = grad_fn(input_ids, attention_mask, token_type_ids, label)\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "\n",
    "    for batch, (input_ids, attention_mask, token_type_ids, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(input_ids, attention_mask, token_type_ids, label)\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, dataset, output_file, current_step, current_epoch):\n",
    "#def test_loop(model, dataset):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for input_ids, attention_mask, token_type_ids, label in dataset.create_tuple_iterator():\n",
    "        loss, logits = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        label=label)\n",
    "        total += len(input_ids)\n",
    "        test_loss += loss.asnumpy()\n",
    "        correct_loop = (logits.argmax(1) == label.view(-1)).asnumpy().sum()\n",
    "        correct += correct_loop\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    if correct*100 >= 82.84:\n",
    "        output_ckpt_file = os.path.join(output_file, \"{epoch}_epoch_{step}_step_acc_{acc}%.ckpt\".format(\n",
    "            epoch=current_epoch, step=current_step, acc=(correct*100)\n",
    "        ))\n",
    "        mindspore.save_checkpoint(model, output_ckpt_file)\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, train_dataset, dev_dataset, optimizer, current_epoch):\n",
    "    output_file = os.path.join(getpwd(), \"outputs\")\n",
    "    if not os.path.exists(output_file):\n",
    "        try:\n",
    "            os.mkdir(output_file)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    def forward_fn(input_ids, attention_mask, token_type_ids, label):\n",
    "        loss, logits = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        label=label)\n",
    "        return loss, logits\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "    def train_step(input_ids, attention_mask, token_type_ids, label):\n",
    "        (loss, _), grads = grad_fn(input_ids, attention_mask, token_type_ids, label)\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "\n",
    "    size = train_dataset.get_dataset_size()\n",
    "    model.set_train(True)\n",
    "\n",
    "    for batch, (input_ids, attention_mask, token_type_ids, label) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "        loss = train_step(input_ids, attention_mask, token_type_ids, label)\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")\n",
    "        # add test\n",
    "        if batch % 100 == 0:\n",
    "            print(\"In epoch {} _ batch {} testing\".format(current_epoch + 1, current))\n",
    "            test_loop(model, dev_dataset, output_file, batch, current_epoch)\n",
    "            # test_loop(model, dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "epoch_num = 10\n",
    "config_file = 'config/bert_L4_H128_config.json'\n",
    "ckpt_file = \"checkpoint/bert_L4_H128_step_308749_card_id_0.ckpt\"\n",
    "config = BertConfig(config_file)\n",
    "model = BertForPretraining(config)\n",
    "bert_sst_2 = BertBinaryClassificationModel(config, ckpt_file)\n",
    "\n",
    "# freeze_layer for classify task\n",
    "# freeze_params = list(filter(lambda x: 'bert.encoder.layer.0' in x.name \\\n",
    "#                                     or 'bert.encoder.layer.1' in x.name \\\n",
    "#                                     or 'bert.encoder.layer.2' in x.name \\\n",
    "#                                     or 'bert.embeddings.token_type_embeddings' in x.name \\\n",
    "#                                     or 'bert.embeddings.position_embeddings' in x.name \\\n",
    "#                                     or 'bert.embeddings.word_embeddings' in x.name, \\\n",
    "#                             bert_sst_2.bert.trainable_params()))\n",
    "# freeze_params = list(filter(lambda x: 'bert' in x.name, bert_sst_2.bert.trainable_params()))\n",
    "# for item in freeze_params:\n",
    "#     item.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = bert_sst_2.trainable_params()\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamWeightDecay\n",
    "from src.optimization import BertLearningRate\n",
    "# lr_schedule_for_backbone = BertLearningRate(learning_rate=5e-5,\n",
    "#                                    end_learning_rate=0.0,\n",
    "#                                    warmup_steps=int(train_dataset.get_dataset_size() * epoch_num * 0.1),\n",
    "#                                    decay_steps=train_dataset.get_dataset_size() * epoch_num,\n",
    "#                                    power=1.0)\n",
    "\n",
    "lr_schedule = BertLearningRate(learning_rate=5e-5,\n",
    "                                   end_learning_rate=0.0,\n",
    "                                   warmup_steps=int(train_dataset.get_dataset_size() * epoch_num * 0.1),\n",
    "                                   decay_steps=train_dataset.get_dataset_size() * epoch_num,\n",
    "                                   power=1.0)\n",
    "# decay_params = list(filter(lambda x: 'gamma' not in x.name \\\n",
    "#                                         and 'beta' not in x.name \\\n",
    "#                                         and 'bias' not in x.name, params))\n",
    "# other_params = list(filter(lambda x: 'gamma' in x.name \\\n",
    "#                                         or 'beta' in x.name \\\n",
    "#                                         or 'bias' in x.name, params))\n",
    "# group_params = [{'params': decay_params, 'weight_decay': 0.01},\n",
    "#                     {'params': other_params},\n",
    "#                     {'order_params': params}]\n",
    "\n",
    "# classify_params = list(filter(lambda x: 'classifier' in x.name, \\\n",
    "#                             bert_sst_2.trainable_params()))\n",
    "\n",
    "# other_params = list(filter(lambda x: 'classifier' not in x.name, \\\n",
    "#                             bert_sst_2.trainable_params()))\n",
    "\n",
    "# group_params = [{'params': classify_params, 'weight_decay': 0.01, 'lr': lr_schedule_for_finetuing},\n",
    "#                 {'params': other_params, 'lr': lr_schedule_for_finetuing}]\n",
    "\n",
    "optimizer = nn.AdamWeightDecay(params, lr_schedule, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_and_test(bert_sst_2, train_dataset, dev_dataset, optimizer, epoch)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
