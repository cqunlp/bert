# 1.This file shows the parsed IR info when graph evaluating failed to help find the problem.
# 2.You can search the last `------------------------>` to the node which is inferred failed.
# 3.Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.dat to get more instructions.
# ===============================================================================

# [No.1] _train_step_graph.1
# In file run_pretrain.py:20/def _train_step_graph(data):/
funcgraph fg_1(
        %para1 : List[Tensor(I32)*4,Tensor(F32),Tensor(I32)*2]ListShape[(16, 128), (16, 128), (16, 20), (16, 20), (16, 20), (16, 1), (16, 128)]    # data
        , %para2 : Ref[Tensor(F32)][30522, 256]    # bert.embeddings.word_embeddings.embedding_table
        , %para3 : Ref[Tensor(F32)][512, 256]    # bert.embeddings.position_embeddings.embedding_table
        , %para4 : Ref[Tensor(F32)][2, 256]    # bert.embeddings.token_type_embeddings.embedding_table
        , %para5 : Ref[Tensor(F32)][256]    # bert.embeddings.layer_norm.gamma
        , %para6 : Ref[Tensor(F32)][256]    # bert.embeddings.layer_norm.beta
        , %para7 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.0.attention.self_attn.query.weight
        , %para8 : Ref[Tensor(F32)][256]    # bert.encoder.layer.0.attention.self_attn.query.bias
        , %para9 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.0.attention.self_attn.key.weight
        , %para10 : Ref[Tensor(F32)][256]    # bert.encoder.layer.0.attention.self_attn.key.bias
        , %para11 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.0.attention.self_attn.value.weight
        , %para12 : Ref[Tensor(F32)][256]    # bert.encoder.layer.0.attention.self_attn.value.bias
        , %para13 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.0.attention.output.dense.weight
        , %para14 : Ref[Tensor(F32)][256]    # bert.encoder.layer.0.attention.output.dense.bias
        , %para15 : Ref[Tensor(F32)][256]    # bert.encoder.layer.0.attention.output.layer_norm.gamma
        , %para16 : Ref[Tensor(F32)][256]    # bert.encoder.layer.0.attention.output.layer_norm.beta
        , %para17 : Ref[Tensor(F32)][1024, 256]    # bert.encoder.layer.0.intermediate.dense.weight
        , %para18 : Ref[Tensor(F32)][1024]    # bert.encoder.layer.0.intermediate.dense.bias
        , %para19 : Ref[Tensor(F32)][256, 1024]    # bert.encoder.layer.0.output.dense.weight
        , %para20 : Ref[Tensor(F32)][256]    # bert.encoder.layer.0.output.dense.bias
        , %para21 : Ref[Tensor(F32)][256]    # bert.encoder.layer.0.output.layer_norm.gamma
        , %para22 : Ref[Tensor(F32)][256]    # bert.encoder.layer.0.output.layer_norm.beta
        , %para23 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.1.attention.self_attn.query.weight
        , %para24 : Ref[Tensor(F32)][256]    # bert.encoder.layer.1.attention.self_attn.query.bias
        , %para25 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.1.attention.self_attn.key.weight
        , %para26 : Ref[Tensor(F32)][256]    # bert.encoder.layer.1.attention.self_attn.key.bias
        , %para27 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.1.attention.self_attn.value.weight
        , %para28 : Ref[Tensor(F32)][256]    # bert.encoder.layer.1.attention.self_attn.value.bias
        , %para29 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.1.attention.output.dense.weight
        , %para30 : Ref[Tensor(F32)][256]    # bert.encoder.layer.1.attention.output.dense.bias
        , %para31 : Ref[Tensor(F32)][256]    # bert.encoder.layer.1.attention.output.layer_norm.gamma
        , %para32 : Ref[Tensor(F32)][256]    # bert.encoder.layer.1.attention.output.layer_norm.beta
        , %para33 : Ref[Tensor(F32)][1024, 256]    # bert.encoder.layer.1.intermediate.dense.weight
        , %para34 : Ref[Tensor(F32)][1024]    # bert.encoder.layer.1.intermediate.dense.bias
        , %para35 : Ref[Tensor(F32)][256, 1024]    # bert.encoder.layer.1.output.dense.weight
        , %para36 : Ref[Tensor(F32)][256]    # bert.encoder.layer.1.output.dense.bias
        , %para37 : Ref[Tensor(F32)][256]    # bert.encoder.layer.1.output.layer_norm.gamma
        , %para38 : Ref[Tensor(F32)][256]    # bert.encoder.layer.1.output.layer_norm.beta
        , %para39 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.2.attention.self_attn.query.weight
        , %para40 : Ref[Tensor(F32)][256]    # bert.encoder.layer.2.attention.self_attn.query.bias
        , %para41 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.2.attention.self_attn.key.weight
        , %para42 : Ref[Tensor(F32)][256]    # bert.encoder.layer.2.attention.self_attn.key.bias
        , %para43 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.2.attention.self_attn.value.weight
        , %para44 : Ref[Tensor(F32)][256]    # bert.encoder.layer.2.attention.self_attn.value.bias
        , %para45 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.2.attention.output.dense.weight
        , %para46 : Ref[Tensor(F32)][256]    # bert.encoder.layer.2.attention.output.dense.bias
        , %para47 : Ref[Tensor(F32)][256]    # bert.encoder.layer.2.attention.output.layer_norm.gamma
        , %para48 : Ref[Tensor(F32)][256]    # bert.encoder.layer.2.attention.output.layer_norm.beta
        , %para49 : Ref[Tensor(F32)][1024, 256]    # bert.encoder.layer.2.intermediate.dense.weight
        , %para50 : Ref[Tensor(F32)][1024]    # bert.encoder.layer.2.intermediate.dense.bias
        , %para51 : Ref[Tensor(F32)][256, 1024]    # bert.encoder.layer.2.output.dense.weight
        , %para52 : Ref[Tensor(F32)][256]    # bert.encoder.layer.2.output.dense.bias
        , %para53 : Ref[Tensor(F32)][256]    # bert.encoder.layer.2.output.layer_norm.gamma
        , %para54 : Ref[Tensor(F32)][256]    # bert.encoder.layer.2.output.layer_norm.beta
        , %para55 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.3.attention.self_attn.query.weight
        , %para56 : Ref[Tensor(F32)][256]    # bert.encoder.layer.3.attention.self_attn.query.bias
        , %para57 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.3.attention.self_attn.key.weight
        , %para58 : Ref[Tensor(F32)][256]    # bert.encoder.layer.3.attention.self_attn.key.bias
        , %para59 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.3.attention.self_attn.value.weight
        , %para60 : Ref[Tensor(F32)][256]    # bert.encoder.layer.3.attention.self_attn.value.bias
        , %para61 : Ref[Tensor(F32)][256, 256]    # bert.encoder.layer.3.attention.output.dense.weight
        , %para62 : Ref[Tensor(F32)][256]    # bert.encoder.layer.3.attention.output.dense.bias
        , %para63 : Ref[Tensor(F32)][256]    # bert.encoder.layer.3.attention.output.layer_norm.gamma
        , %para64 : Ref[Tensor(F32)][256]    # bert.encoder.layer.3.attention.output.layer_norm.beta
        , %para65 : Ref[Tensor(F32)][1024, 256]    # bert.encoder.layer.3.intermediate.dense.weight
        , %para66 : Ref[Tensor(F32)][1024]    # bert.encoder.layer.3.intermediate.dense.bias
        , %para67 : Ref[Tensor(F32)][256, 1024]    # bert.encoder.layer.3.output.dense.weight
        , %para68 : Ref[Tensor(F32)][256]    # bert.encoder.layer.3.output.dense.bias
        , %para69 : Ref[Tensor(F32)][256]    # bert.encoder.layer.3.output.layer_norm.gamma
        , %para70 : Ref[Tensor(F32)][256]    # bert.encoder.layer.3.output.layer_norm.beta
        , %para71 : Ref[Tensor(F32)][256, 256]    # bert.pooler.dense.weight
        , %para72 : Ref[Tensor(F32)][256]    # bert.pooler.dense.bias
        , %para73 : Ref[Tensor(F32)][30522]    # cls.predictions.bias
        , %para74 : Ref[Tensor(F32)][256, 256]    # cls.predictions.transform.dense.weight
        , %para75 : Ref[Tensor(F32)][256]    # cls.predictions.transform.dense.bias
        , %para76 : Ref[Tensor(F32)][256]    # cls.predictions.transform.layer_norm.gamma
        , %para77 : Ref[Tensor(F32)][256]    # cls.predictions.transform.layer_norm.beta
        , %para78 : Ref[Tensor(F32)][2, 256]    # cls.seq_relationship.weight
        , %para79 : Ref[Tensor(F32)][2]    # cls.seq_relationship.bias
        , %para80 : Ref[Tensor(I64)][]    # step
    ) {

#------------------------> 0
    %1 = DoSignaturePrimitive::S-Prim-getitem{prim_type=1}(%para1, "input_ids")    #(List[Tensor(I32)*4,Tensor(F32),Tensor(I32)*2]ListShape[(16, 128), (16, 128), (16, 20), (16, 20), (16, 20), (16, 1), (16, 128)], StringNoShape) #scope: Default
      # In file run_pretrain.py:21/    loss, grads = grad_fn(data['input_ids'], data['input_mask'], data['segment_ids'],/#[CNode]3
    %2 = DoSignaturePrimitive::S-Prim-getitem{prim_type=1}(%para1, "input_mask")    #(List[Tensor(I32)*4,Tensor(F32),Tensor(I32)*2]ListShape[(16, 128), (16, 128), (16, 20), (16, 20), (16, 20), (16, 1), (16, 128)], Undefined) #scope: Default
      # In file run_pretrain.py:21/    loss, grads = grad_fn(data['input_ids'], data['input_mask'], data['segment_ids'],/#[CNode]4
    %3 = DoSignaturePrimitive::S-Prim-getitem{prim_type=1}(%para1, "segment_ids")    #(List[Tensor(I32)*4,Tensor(F32),Tensor(I32)*2]ListShape[(16, 128), (16, 128), (16, 20), (16, 20), (16, 20), (16, 1), (16, 128)], Undefined) #scope: Default
      # In file run_pretrain.py:21/    loss, grads = grad_fn(data['input_ids'], data['input_mask'], data['segment_ids'],/#[CNode]5
    %4 = DoSignaturePrimitive::S-Prim-getitem{prim_type=1}(%para1, "input_ids")    #(List[Tensor(I32)*4,Tensor(F32),Tensor(I32)*2]ListShape[(16, 128), (16, 128), (16, 20), (16, 20), (16, 20), (16, 1), (16, 128)], Undefined) #scope: Default
      # In file run_pretrain.py:22/                          None, None, data['input_ids'], data['next_sentence_label'])/#[CNode]6
    %5 = DoSignaturePrimitive::S-Prim-getitem{prim_type=1}(%para1, "next_sentence_label")    #(List[Tensor(I32)*4,Tensor(F32),Tensor(I32)*2]ListShape[(16, 128), (16, 128), (16, 20), (16, 20), (16, 20), (16, 1), (16, 128)], Undefined) #scope: Default
      # In file run_pretrain.py:22/                          None, None, data['input_ids'], data['next_sentence_label'])/#[CNode]7
    %6 = FuncGraph::fg_8(%1, %2, %3, None, None, %4, %5)    #(Undefined, Undefined, Undefined, Undefined, Undefined, Undefined, Undefined)    # fg_8=after_grad.8 #scope: Default
      # In file run_pretrain.py:21/    loss, grads = grad_fn(data['input_ids'], data['input_mask'], data['segment_ids'],/#[CNode]9
    %7 = DoSignaturePrimitive::S-Prim-getitem{prim_type=1}(%6, I64(1))    #(Undefined, Undefined) #scope: Default
      # In file run_pretrain.py:21/    loss, grads = grad_fn(data['input_ids'], data['input_mask'], data['segment_ids'],/#grads
    %8 = FuncGraph::fg_10(%7)    #(Undefined)    # fg_10=Default.10 #scope: Default
      # In file run_pretrain.py:23/    optim(grads)/#[CNode]11
    %9 = Primitive::stop_gradient{prim_type=1}(%8)    #(Undefined) #scope: Default
#[CNode]12
    %10 = DoSignaturePrimitive::S-Prim-getitem{prim_type=1}(%6, I64(0))    #(Undefined, Undefined) #scope: Default
      # In file run_pretrain.py:21/    loss, grads = grad_fn(data['input_ids'], data['input_mask'], data['segment_ids'],/#loss
    %11 = Primitive::Depend{prim_type=1}[side_effect_propagate=I64(1)](%10, %9)    #(Undefined, Undefined) #scope: Default
#[CNode]13
    Primitive::Return{prim_type=1}(%11)    #(Undefined) #scope: Default
      # In file run_pretrain.py:24/    return loss/#[CNode]14
}
# order:
#   1: @_train_step_graph.1:[CNode]3{[0]: ValueNode<DoSignaturePrimitive> S-Prim-getitem, [1]: data, [2]: ValueNode<StringImm> input_ids}
#   2: @_train_step_graph.1:[CNode]4{[0]: ValueNode<DoSignaturePrimitive> S-Prim-getitem, [1]: data, [2]: ValueNode<StringImm> input_mask}
#   3: @_train_step_graph.1:[CNode]5{[0]: ValueNode<DoSignaturePrimitive> S-Prim-getitem, [1]: data, [2]: ValueNode<StringImm> segment_ids}
#   4: @_train_step_graph.1:[CNode]6{[0]: ValueNode<DoSignaturePrimitive> S-Prim-getitem, [1]: data, [2]: ValueNode<StringImm> input_ids}
#   5: @_train_step_graph.1:[CNode]7{[0]: ValueNode<DoSignaturePrimitive> S-Prim-getitem, [1]: data, [2]: ValueNode<StringImm> next_sentence_label}
#   6: @_train_step_graph.1:[CNode]9{[0]: ValueNode<FuncGraph> after_grad.8, [1]: [CNode]3, [2]: [CNode]4, [3]: [CNode]5, [4]: ValueNode<None> None, [5]: ValueNode<None> None, [6]: [CNode]6, [7]: [CNode]7}
#   7: @_train_step_graph.1:loss{[0]: ValueNode<DoSignaturePrimitive> S-Prim-getitem, [1]: [CNode]9, [2]: ValueNode<Int64Imm> 0}
#   8: @_train_step_graph.1:grads{[0]: ValueNode<DoSignaturePrimitive> S-Prim-getitem, [1]: [CNode]9, [2]: ValueNode<Int64Imm> 1}
#   9: @_train_step_graph.1:[CNode]11{[0]: ValueNode<FuncGraph> Default.10, [1]: grads}
#  10: @_train_step_graph.1:[CNode]14{[0]: ValueNode<Primitive> Return, [1]: [CNode]13}


#===============================================================================
# num of function graphs in stack: 1/2 (Ignored 1 internal frames).
